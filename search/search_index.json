{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Study Notes Blog # The aim of this blog is to document all the notes, exercises and research in Data Science In case you find any errors, please let me know in the respective comments section. Topic Notes Link Exercises Link Statistics Statistics Notes Statistics Exercises Probability Probability Notes","title":"Home"},{"location":"#study_notes_blog","text":"The aim of this blog is to document all the notes, exercises and research in Data Science In case you find any errors, please let me know in the respective comments section. Topic Notes Link Exercises Link Statistics Statistics Notes Statistics Exercises Probability Probability Notes","title":"Study Notes Blog"},{"location":"exercises/","text":"List of exercises and the solutions # Statistics Exercises # Course Class Exercise Number + Link Exercise Description Statistics 01 Exercise01 Statistics 01 Exercise02 Statistics 02 Exercise03 Statistics 02 Exercise04 Statistics 03 Exercise05 Normal Distribution Vs Probability Distribution Statistics 03 Exercise06 Confidence Interval","title":"Exercises Index"},{"location":"exercises/#list_of_exercises_and_the_solutions","text":"","title":"List of exercises and the solutions"},{"location":"exercises/#statistics_exercises","text":"Course Class Exercise Number + Link Exercise Description Statistics 01 Exercise01 Statistics 01 Exercise02 Statistics 02 Exercise03 Statistics 02 Exercise04 Statistics 03 Exercise05 Normal Distribution Vs Probability Distribution Statistics 03 Exercise06 Confidence Interval","title":"Statistics Exercises"},{"location":"exercises/exercise05/","text":"Normal Distribution is a probability distribution. # The topic in question is - Is normal distribution a probability distribution? Yes, it is and this writeup provides justification as to why it is so! Revisiting the Normal distribution # Let's revisit the normal distribution once - its characteristics and emprical rule Normal distribution is the distribution of data that is symmetric over the mean. 50% of the data lie to the left and 50% of the data lie to the right At the origin, mean = median = mode = 0 Emprical rule # The emprical rule states that - 68% of the data will be within one standard deviation of the mean - 95% of the data will be within two standard deviations from the mean - 99.7% of the data will be within three standard deviations from the mean Lets see that in the figure below Example Dataset # Let's plot a normal distribution graph of 300 people with a mean salary of 50,000 and standard deviation of 8700 (sigma) Descriptive Statistics Value Mean 50000 Median 50000 Std dev p 8700 Mode 50000 Then the following will be the ranges of the salary from the mean. Sigma Range Salary -1 sigma 41300 +1 sigma 58700 -2 sigma 32600 +2 sigma 67400 -3 sigma 23900 +3 sigma 76100 The number of people with the respective salary range and the normal distribution is mentioned below. Salary Range No of people Norm Dist 23000-33000 0 0 33001-41000 64 0.00118224 41001-50000 88 0.003468298 50001-59000 87 0.003404399 59001-67000 63 0.001154427 67001-76000 0 0 Normal distribution computed with NORMAL.DIST function in EXCEL So, we have all of our 300 people salaries mapped as a normal distribution. Random sample # Let's take a random sample from our range of salaries. NOTE: use the EXCEL RANDBETWEEN function to generate a random sample provided a range. Here our range is from -3 sigma to +3 sigma which is from 23900 to 76100 The random sample i.e. salary taken is 47243 Question? # What is the chance that this random sample (e.g. 47243 ) falls within -3 sigma to +3 sigma? i.e. the area under the curve (highlighted in red color)? Can we say that 99.7% of the time of the time the random sample taken will fall within the -3 sigma to +3 sigma (i.e. the area under the curve) This 99.7% chance is called as the probability of the sample falling within the range -3 sigma to +3 sigma. The range of Probability is from 0 to 1 . i.e 0 for no chance and 1 for 100% chance. One more sample # Let's take one more sample between -3 sigma and +3 sigma. What is the chance that it will fall under the area of the curve highlighted below (in red color) i.e. from -1 sigma to +1 sigma? Can we say that it will be 68% or 0.68 probability? Lets prove it! The area under the chart is from -1 sigma to +1 sigma. Correct? Let's revisit Z (zee) scores once z scores is the number of units of standard deviation from the mean. Here the range in consideration is -1 sigma to +1 sigma i.e we want to find out if a random sample (salary) falls within this range. So the z scores are -1 and +1 . To be precise we can say -1.0 and +1.0 Let's bring Z-Tables. Z-Tables provides us the area under the curve / probability. Using Z-tables lets find the area under the curve of -1.0Z Since -1 is a negative Z, we see the negative Z score table and see that the probability is 0.15866 (see figure below) i.e if we take a random sample between -3 sigma and +3 sigma, the chance of the random sample falling under the area of the curve of -1Z is 15.866% . Thus Z tables us gives the probability. For +1Z , let's refer to the positive Z score table. The area under the curve is 0.84134 . (See fig below). To summarize, to find out the area under the curve -1Z to +1Z , we need to subtract the area under +1Z with -1Z Note: Zee is 1 unit of standard deviation. So it is -1 sigma to +1 sigma i.e 0.84134 - 0.15866 = 0.68268 . Thus 68.268% is the probability that a given random sample will fall under the range of -1 sigma to +1 sigma. In this way, we can find out the probability of a random variable falling under the area of the curve of the normal distribution for any Zee values. i.e from -3 sigma to -2 sigma. or from -2 sigma to +3 sigma etc. Conclusion # Thus we have emprically concluded that the normal distribution is a probability distribution as it helps to find the probability of a random variable falling within the normal distribution range.","title":"Exercise05"},{"location":"exercises/exercise05/#normal_distribution_is_a_probability_distribution","text":"The topic in question is - Is normal distribution a probability distribution? Yes, it is and this writeup provides justification as to why it is so!","title":"Normal Distribution is a probability distribution."},{"location":"exercises/exercise05/#revisiting_the_normal_distribution","text":"Let's revisit the normal distribution once - its characteristics and emprical rule Normal distribution is the distribution of data that is symmetric over the mean. 50% of the data lie to the left and 50% of the data lie to the right At the origin, mean = median = mode = 0","title":"Revisiting the Normal distribution"},{"location":"exercises/exercise05/#emprical_rule","text":"The emprical rule states that - 68% of the data will be within one standard deviation of the mean - 95% of the data will be within two standard deviations from the mean - 99.7% of the data will be within three standard deviations from the mean Lets see that in the figure below","title":"Emprical rule"},{"location":"exercises/exercise05/#example_dataset","text":"Let's plot a normal distribution graph of 300 people with a mean salary of 50,000 and standard deviation of 8700 (sigma) Descriptive Statistics Value Mean 50000 Median 50000 Std dev p 8700 Mode 50000 Then the following will be the ranges of the salary from the mean. Sigma Range Salary -1 sigma 41300 +1 sigma 58700 -2 sigma 32600 +2 sigma 67400 -3 sigma 23900 +3 sigma 76100 The number of people with the respective salary range and the normal distribution is mentioned below. Salary Range No of people Norm Dist 23000-33000 0 0 33001-41000 64 0.00118224 41001-50000 88 0.003468298 50001-59000 87 0.003404399 59001-67000 63 0.001154427 67001-76000 0 0 Normal distribution computed with NORMAL.DIST function in EXCEL So, we have all of our 300 people salaries mapped as a normal distribution.","title":"Example Dataset"},{"location":"exercises/exercise05/#random_sample","text":"Let's take a random sample from our range of salaries. NOTE: use the EXCEL RANDBETWEEN function to generate a random sample provided a range. Here our range is from -3 sigma to +3 sigma which is from 23900 to 76100 The random sample i.e. salary taken is 47243","title":"Random sample"},{"location":"exercises/exercise05/#question","text":"What is the chance that this random sample (e.g. 47243 ) falls within -3 sigma to +3 sigma? i.e. the area under the curve (highlighted in red color)? Can we say that 99.7% of the time of the time the random sample taken will fall within the -3 sigma to +3 sigma (i.e. the area under the curve) This 99.7% chance is called as the probability of the sample falling within the range -3 sigma to +3 sigma. The range of Probability is from 0 to 1 . i.e 0 for no chance and 1 for 100% chance.","title":"Question?"},{"location":"exercises/exercise05/#one_more_sample","text":"Let's take one more sample between -3 sigma and +3 sigma. What is the chance that it will fall under the area of the curve highlighted below (in red color) i.e. from -1 sigma to +1 sigma? Can we say that it will be 68% or 0.68 probability?","title":"One more sample"},{"location":"exercises/exercise05/#conclusion","text":"Thus we have emprically concluded that the normal distribution is a probability distribution as it helps to find the probability of a random variable falling within the normal distribution range.","title":"Conclusion"},{"location":"exercises/exercise06/","text":"Confidence level Vs the accuracy of estimation of the population mean # Does the more the confidence level in the confidence interval mean more the accurate prediction of the estimation of the population mean? Confidence Interval # A confidence interval is the range of numbers that is believed to include an unknown population parameter. Confidence Level # It is the measure of confidence that the unknown parameter (of the population) lies within the confidence interval. A case study # Comcast, the computer services company, is planning to invest heavily in online television service. As part of the decision, the company wants to estimate the average number of online shows a family of four would watch per day. A random sample of 100 families is obtained, and in this sample the average number of shows viewed per day is 6.5 and the population standard deviation is known to be 3.2 . Construct a 95% confidence interval for the average number of online television shows watched by the entire population of families of four. Known measures sample size - $$ n = 100 $$ sample mean - $$ \\bar{x} = 6.5 $$ population standard deviation - $$ \\sigma = 3.2 $$ confidence interval $$ Z = 1.96 (for the value of 95 percent) $$ To be found: Confidence interval of within which the population mean i.e. the average number of online television shows are watched by the entire population Let's picturize it Let's understand the picture Let's compute the confidence interval Confidence interval = \\mu \\pm Z * (\\sigma / \\sqrt n) \\mu \\pm Z * (\\sigma / \\sqrt n) Since we don't know the population parameter $$ \\mu $$ here, we will replace it with the value we know about the sample i.e. the sample mean $$ \\bar{x} $$ So the confidence interval in this case is $$ \\bar{x} \\pm Z * (\\sigma / \\sqrt n) $$ When substituted with values above, we get $$ confidence interval = 6.5 \\pm 1.96 * (3.2 / \\sqrt(100) $$ Note: The Z value for 95% confidence is 1.96 We get the confidence interval as [5.8728, 7.1272] If we reduce the confidence level to 80%, what happens? Note: the Z score for 80% confidence is 1.282 $$ confidence interval = 6.5 \\pm 1.28 * (3.2 / \\sqrt(100) $$ The confidence interval becomes [6.0904, 6.9096] Inference So, when the confidence level decreases from 95% to 80%, the confidence interval becomes narrorwer - which would mean our chance of finding the population mean reduces by 15% So it is better to always have a high confidence level to get more accurate estimations of the population mean What if i need to have a good confidence level i.e. 95% but the confidence interval should be narrower to get closer to the population mean - more accuracy in predicting? Answer Increase the sample size i.e. increase the sample size n=100 provided above to n=200 . The confidence interval becomes [6.0565, 6.9434]","title":"Exercise06"},{"location":"exercises/exercise06/#confidence_level_vs_the_accuracy_of_estimation_of_the_population_mean","text":"Does the more the confidence level in the confidence interval mean more the accurate prediction of the estimation of the population mean?","title":"Confidence level Vs the accuracy of estimation of the population mean"},{"location":"exercises/exercise06/#confidence_interval","text":"A confidence interval is the range of numbers that is believed to include an unknown population parameter.","title":"Confidence Interval"},{"location":"exercises/exercise06/#confidence_level","text":"It is the measure of confidence that the unknown parameter (of the population) lies within the confidence interval.","title":"Confidence Level"},{"location":"exercises/exercise06/#a_case_study","text":"Comcast, the computer services company, is planning to invest heavily in online television service. As part of the decision, the company wants to estimate the average number of online shows a family of four would watch per day. A random sample of 100 families is obtained, and in this sample the average number of shows viewed per day is 6.5 and the population standard deviation is known to be 3.2 . Construct a 95% confidence interval for the average number of online television shows watched by the entire population of families of four.","title":"A case study"},{"location":"exercises/exercises-todo-class5/","text":"Exercises to do - Class 5 # The following are the four exercises that needs to be completed; given as part of Class 05 - Statistics If the population standard deviation is provided and the number of samples < 30, which test do we go for? i.e. Z test or t test - with justification Which test is to be used to measure the association between two categorical variables / qualitative variables F' test in excel using the data analysis plugin - for the same problem we saw in the class Different types of Discrete distributions - 2 liner summary","title":"Exercises to do - Class 5"},{"location":"exercises/exercises-todo-class5/#exercises_to_do_-_class_5","text":"The following are the four exercises that needs to be completed; given as part of Class 05 - Statistics If the population standard deviation is provided and the number of samples < 30, which test do we go for? i.e. Z test or t test - with justification Which test is to be used to measure the association between two categorical variables / qualitative variables F' test in excel using the data analysis plugin - for the same problem we saw in the class Different types of Discrete distributions - 2 liner summary","title":"Exercises to do - Class 5"},{"location":"probability/","text":"Probability Index #","title":"_Probability Home"},{"location":"probability/#probability_index","text":"","title":"Probability Index"},{"location":"probability/probability_theory/","text":"Probability # A probability is a quantitative measure of uncertainity . Basic Definitions # Set A set is a collection of elements Empty set A set containing no elements Universal set A set containing all elements. Represented by S Compliment of a set The complement of set A is the set containing all the elements in the universal set S that are not members of set A. Represented by \\bar{A} \\bar{A} Intersection of a set Denoted by A \\cap B A \\cap B , the intersection is the set containing all elements that are members of both A and B. Union of a set Denoted by A \\cup B A \\cup B , is the set containing all the elements that are members of both A and B Experiment and Outcome An experiment is a process that leads to one or several possible outcomes. An outcome of an experiment is some observation or measurement. Example Drawing a card out of a deck of 52 cards is an experiment. One outcome of the experiment may be that the queen of diamonds is drawn. Sample space Sample space is the set of all possible outcomes of an experiment. Example The sample space for the experiment of drawing a card out of a deck is the set of all cards in the deck. Event An event is a subset of a sample space. Example An ace is drawn out of a deck of cards. Probability of event A P(A) = \\frac{n(A)}{n(S)} P(A) = \\frac{n(A)}{n(S)} where n(A)= n(A)= the number of elements in the set of the event A n(S)= n(S)= the number of elements in the sample space S Example The probability of drawing an Ace is P(A) = \\frac{n(A)}{n(S)} = \\frac{4}{52} P(A) = \\frac{n(A)}{n(S)} = \\frac{4}{52} Note: There are 4 Ace's in a deck of cards - Spade, Club, Heart, and Diamond Practice Roulette is a popular casino game. As the game is played in Las Vegas or Atlantic City, the roulette wheel has 36 numbers, 1 through 36, and the number 0 as well as the number 00 (double zero). What is the probability of winning on a single number that you bet? Solution: total elements in the set = 36 + 2 (include 0 and 00) = 38 Probability of winning a single number = \\frac{1}{38} \\frac{1}{38} Basic rules of probability # The range of values For any event A, the probability P(A) P(A) lies between 0 and 1. i.e. 0 \\le P(A) \\le 1 0 \\le P(A) \\le 1 When a probability cannot occur, the probability is zero. Example What is the probability of drawing a green card from the deck of cards? It's 0 as the deck of cards has only red/black cards. When a probability is certain to occur, the probability is one. Rule of complements Probability of a complement is P(\\bar{A}) = 1 - P(A) P(\\bar{A}) = 1 - P(A) Example The probability of rain tomorrow is 0.3. This means the probability of no rain is 1 - 0.3 = 0.7 Rule of unions The rule of unions: P(A \\cup B) = P(A) + P(B) - P(A \\cap B) P(A \\cup B) = P(A) + P(B) - P(A \\cap B) Example The probability of an Ace in a deck of cards is \\frac{4}{52} \\frac{4}{52} and the probability of a heart in a deck of cards is \\frac{13}{52} \\frac{13}{52} . The probability of an ace with heart is \\frac{1}{52} \\frac{1}{52} So P(A \\cup \\heartsuit) = \\frac{4}{52} + \\frac{13}{52} - \\frac{1}{52} = \\frac{16}{52} P(A \\cup \\heartsuit) = \\frac{4}{52} + \\frac{13}{52} - \\frac{1}{52} = \\frac{16}{52} Mutually exclusive events # When the sets corresponding to two events are disjoint (i.e. have not an intersection), the two events are called mutually exclusive or in other words, they can't occur together. To simplify, events are mutually exclusive if they cannot occur simultaneously. Intersection: For mutually exclusive events A and B: P(A \\cap B) = 0 P(A \\cap B) = 0 Since there is no intersection, it is always 0 Union: For mutualy exclusive events A and B: P(A \\cup B) = P(A) + P(B) P(A \\cup B) = P(A) + P(B) Note Since there is no intersection, P(A \\cup B) = P(A) + P(B) - P(A \\cap B) P(A \\cup B) = P(A) + P(B) - P(A \\cap B) is reduced to P(A \\cup B) = P(A) + P(B) P(A \\cup B) = P(A) + P(B) Example What is the probability of drawing either a heart or a club? The probability of drawing a heart is \\frac{13}{52} \\frac{13}{52} . The probability of drawing a club is \\frac{13}{52} \\frac{13}{52} . Since they are mutually exclusive, P(\\heartsuit \\cup \\clubsuit) = P(\\heartsuit) + P(\\clubsuit) P(\\heartsuit \\cup \\clubsuit) = P(\\heartsuit) + P(\\clubsuit) which is \\frac{13}{52} + \\frac{13}{52} = \\frac{26}{52} = \\frac{1}{2} \\frac{13}{52} + \\frac{13}{52} = \\frac{26}{52} = \\frac{1}{2} Conditional probability # The probability depends on information. Say for example - what is the probability that the company Mannar & Co's stock price would go up? Well, it would depend on the information we have about the company like it's performance in recent times. So the probability of the stock price going up depends upon some information. This is called as conditional probability Formula The conditional probability of an event A given the occurrence of event B is P(A|B) = \\frac{P(A \\cap B)}{P(B)} P(A|B) = \\frac{P(A \\cap B)}{P(B)} The vertical line in P(A|B) is read given or conditional upon . Example A consulting firm is bidding for two jobs, one with each of two large multinational corporations. The company executives estimate that the probability of obtaining the consulting job with firm A, event A, is 0.45. The executives also feel that if the company should get the job with firm A, then there is a 0.90 probability that firm B will also give the company the consulting job. What are the company\u2019s chances of getting both jobs? Solution: P(A) = 0.45 \\\\ P(B|A) = 0.90 P(A) = 0.45 \\\\ P(B|A) = 0.90 We are looking for P(A \\cap B) P(A \\cap B) - i.e. chance of getting both jobs We know the formula is P(B|A) = \\frac{P(B \\cap A)}{P(A)} P(B|A) = \\frac{P(B \\cap A)}{P(A)} So the formula becomes, P(B \\cap A) = P(B|A).P(A) P(B \\cap A) = P(B|A).P(A) which is 0.90 * 0.45 = 0.405 0.90 * 0.45 = 0.405 Note Conditional probability is not symmetrical P(A|B) \\ne P(B|A) P(A|B) \\ne P(B|A) Independence of events # As an example of independent events, consider the following: Suppose I roll a single die. What is the probability that the number 6 will turn up? The answer is \\frac{1}{6} \\frac{1}{6} . Now suppose that I told you that I just tossed a coin and it turned up heads. What is now the probability that the die will show the number 6? The answer is unchanged, \\frac{1}{6} \\frac{1}{6} , because events of the die and the coin are independent of each other. We see that P(6 | H) = P(6) P(6 | H) = P(6) So when do we say that the events are independent of each other? Conditions for the independence of two events A and B The events are said to be independent when they meet the below conditions: P(A|B) = P(A) \\\\ P(B|A) = P(B) \\\\ and \\\\ P(A \\cap B) = P(A)P(B) P(A|B) = P(A) \\\\ P(B|A) = P(B) \\\\ and \\\\ P(A \\cap B) = P(A)P(B) The first equation P(A|B)=P(A) P(A|B)=P(A) implies that the probability of A would not be affected by the probability of B as they are both independent. So the probability of A would remain the same. Similarly, the second equation P(B|A)=P(B) P(B|A)=P(B) implies that the probability of B would remain the same as B is not affected by the probability of A since both are independent. The third equation P(A \\cap B)= P(A)P(B) P(A \\cap B)= P(A)P(B) is called the intersection rule for independent events . Intersection rule for independent events P(A \\cap B)= P(A)P(B) P(A \\cap B)= P(A)P(B) The probability of the intersection of several independent events is just the product of the separate probabilities. Let's understand this rule with an example. Example The rate of defects in corks of wine bottles is very high say 75%. Assuming independence, if four bottles are opened, what is the probability that all four corks are defective? Solution: P (all 4 are defective) = P (first cork is defective) * P (second cork is defective) * P (third cork is defective) * P (fourth cork is defective) which is equal to P (all 4 are defective) = 0.75 * 0.75 * 0.75 * 0.75 = 0.316 Tip Whenever a random sampling is done, it would mean independence - as the outcome of one event will not be dependent on the outcome of another event. Question What is the difference between a mutually exclusive event and an independent event? mutually exclusive - Two events cannot occur at the same time. e.g., the head & tail of a coin cannot occur at the same time. They're dependent in fact. independent - Two events can occur at the same time and they are not dependent on each other. e.g., events of tossing a coin & events of rolling a dice at the same time are independent. Bayes Theorem # Bayes theorem is a technique for calculating a conditional probability. Derivation of Bayes Theorem # We have already seen the conditional probability which is Equation 1: P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\space provided \\space that \\space P(B) > 0 P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\space provided \\space that \\space P(B) > 0 Equation 2: This also means that P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\space provided \\space that \\space P(A) > 0 P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\space provided \\space that \\space P(A) > 0 So from equation 2, we can say that (Equation 3) P(B \\cap A) = P(B|A)P(A) P(B \\cap A) = P(B|A)P(A) We also know that P(A \\cap B) = P(B \\cap A) P(A \\cap B) = P(B \\cap A) So substituting the Equation 3 in Equation 1, we get the Bayes theorem. P(A|B) = \\frac{P(B|A).P(A)}{P(B)} \\space provided \\space that \\space P(A), P(B) > 0 P(A|B) = \\frac{P(B|A).P(A)}{P(B)} \\space provided \\space that \\space P(A), P(B) > 0 Example # Refer to Example 1 (Elderly fall and death) in this article for how to apply the Bayes theorem Summary # In this post, we had seen what probability is, definition of terms used in probability, mutually-exclusive and independent events, conditional probability and then the Bayes theorem.","title":"Probability theory"},{"location":"probability/probability_theory/#probability","text":"A probability is a quantitative measure of uncertainity .","title":"Probability"},{"location":"probability/probability_theory/#basic_definitions","text":"","title":"Basic Definitions"},{"location":"probability/probability_theory/#basic_rules_of_probability","text":"","title":"Basic rules of probability"},{"location":"probability/probability_theory/#mutually_exclusive_events","text":"When the sets corresponding to two events are disjoint (i.e. have not an intersection), the two events are called mutually exclusive or in other words, they can't occur together. To simplify, events are mutually exclusive if they cannot occur simultaneously. Intersection: For mutually exclusive events A and B: P(A \\cap B) = 0 P(A \\cap B) = 0 Since there is no intersection, it is always 0 Union: For mutualy exclusive events A and B: P(A \\cup B) = P(A) + P(B) P(A \\cup B) = P(A) + P(B) Note Since there is no intersection, P(A \\cup B) = P(A) + P(B) - P(A \\cap B) P(A \\cup B) = P(A) + P(B) - P(A \\cap B) is reduced to P(A \\cup B) = P(A) + P(B) P(A \\cup B) = P(A) + P(B) Example What is the probability of drawing either a heart or a club? The probability of drawing a heart is \\frac{13}{52} \\frac{13}{52} . The probability of drawing a club is \\frac{13}{52} \\frac{13}{52} . Since they are mutually exclusive, P(\\heartsuit \\cup \\clubsuit) = P(\\heartsuit) + P(\\clubsuit) P(\\heartsuit \\cup \\clubsuit) = P(\\heartsuit) + P(\\clubsuit) which is \\frac{13}{52} + \\frac{13}{52} = \\frac{26}{52} = \\frac{1}{2} \\frac{13}{52} + \\frac{13}{52} = \\frac{26}{52} = \\frac{1}{2}","title":"Mutually exclusive events"},{"location":"probability/probability_theory/#conditional_probability","text":"The probability depends on information. Say for example - what is the probability that the company Mannar & Co's stock price would go up? Well, it would depend on the information we have about the company like it's performance in recent times. So the probability of the stock price going up depends upon some information. This is called as conditional probability","title":"Conditional probability"},{"location":"probability/probability_theory/#independence_of_events","text":"As an example of independent events, consider the following: Suppose I roll a single die. What is the probability that the number 6 will turn up? The answer is \\frac{1}{6} \\frac{1}{6} . Now suppose that I told you that I just tossed a coin and it turned up heads. What is now the probability that the die will show the number 6? The answer is unchanged, \\frac{1}{6} \\frac{1}{6} , because events of the die and the coin are independent of each other. We see that P(6 | H) = P(6) P(6 | H) = P(6) So when do we say that the events are independent of each other?","title":"Independence of events"},{"location":"probability/probability_theory/#bayes_theorem","text":"Bayes theorem is a technique for calculating a conditional probability.","title":"Bayes Theorem"},{"location":"probability/probability_theory/#derivation_of_bayes_theorem","text":"We have already seen the conditional probability which is Equation 1: P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\space provided \\space that \\space P(B) > 0 P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\space provided \\space that \\space P(B) > 0 Equation 2: This also means that P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\space provided \\space that \\space P(A) > 0 P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\space provided \\space that \\space P(A) > 0 So from equation 2, we can say that (Equation 3) P(B \\cap A) = P(B|A)P(A) P(B \\cap A) = P(B|A)P(A) We also know that P(A \\cap B) = P(B \\cap A) P(A \\cap B) = P(B \\cap A) So substituting the Equation 3 in Equation 1, we get the Bayes theorem. P(A|B) = \\frac{P(B|A).P(A)}{P(B)} \\space provided \\space that \\space P(A), P(B) > 0 P(A|B) = \\frac{P(B|A).P(A)}{P(B)} \\space provided \\space that \\space P(A), P(B) > 0","title":"Derivation of Bayes Theorem"},{"location":"probability/probability_theory/#example","text":"Refer to Example 1 (Elderly fall and death) in this article for how to apply the Bayes theorem","title":"Example"},{"location":"probability/probability_theory/#summary","text":"In this post, we had seen what probability is, definition of terms used in probability, mutually-exclusive and independent events, conditional probability and then the Bayes theorem.","title":"Summary"},{"location":"statistics/","text":"Statistics Index #","title":"_Stats Home"},{"location":"statistics/#statistics_index","text":"","title":"Statistics Index"},{"location":"statistics/Inferential_stats_basics/","text":"Inferential Statistics # In real-life situations, we may not be able to know the entire population in which case, we use the known random sample to extract information about the unknown population from which the sample is drawn. To put it simply, using the sample to come up with an estimation of the population is called as Inferential statistics Parameter and Statistic # A numerical measure of a population is called a population parameter or more simply, a parameter . Examples of parameters are population mean, population variance, population standard deviation, etc. A numerical measure of the sample is called a sample statistic , or simply a statistic . Examples of sample statistics are the sample mean, sample variance, sample standard deviation, etc. In inferential statistics, population parameters are estimated by sample statistics. When a simple statistic is used to estimate the population parameter, the statistic is called an estimator of the parameter. Notations of parameter and statistic # Population Symbol Sample Symbol Mean \\mu \\mu Mean x x Variance \\sigma^2 \\sigma^2 Variance s^2 s^2 Std deviation \\sigma \\sigma Std deviation s s To summarize, we have the following relationships between a sample statistic and a population parameter \\bar{x} \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\mu \\\\ \\\\ S^2 \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\sigma^2 \\bar{x} \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\mu \\\\ \\\\ S^2 \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\sigma^2 Confidence levels and confidence intervals # To understand confidence level and confidence intervals, we will need to understand what a population distribution and a sampling distribution is. Population distribution # The population is the entire set of values/data points that we are interested in. For example, if we know the age of all Indian residents, it is called the population. The population characteristics are mean \\mu \\mu , standard deviation \\sigma \\sigma , median, percentiles, etc. Sampling distribution # The sample is a subset of the population which is used to estimate the population characteristic. This sample has a few characteristics like mean \\bar{x} \\bar{x} , standard deviation s s , etc. Samples are used to draw inferences about the population. This is done by drawing samples and computing the sample statistics. When the sample statistics are plotted, the distribution (represented as a histogram) is called the sampling distribution. Note The sampling distribution is also called as theoretical distribution How is a sampling distribution plotted? From the population, we take N N samples of n n size and compute the mean. Then we compute the frequency of these means and plot a histogram that would give the sampling distribution. Note If the population is normally distributed, then the sampling distribution will also be normally distributed. Standard error # The standard error SE SE is the standard deviation of the sampling distribution Standard error of the mean # The standard error (or deviation) of the mean can be expressed as SE = \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} SE = \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} where \\sigma \\sigma is the standard deviation of the population n n is the size of the sample Since the standard deviation of the population is rarely known, the standard error of the mean is usually estimated as the sample standard deviation divided by the square root of the sample size i.e. SE = \\sigma_{\\bar{x}} \\approx \\frac{s}{\\sqrt n} SE = \\sigma_{\\bar{x}} \\approx \\frac{s}{\\sqrt n} where s s is the sample standard deviation n n is the size of the sample. Confidence interval # When we use samples to project population estimates, we cannot be CERTAIN that it will be accurate. There is an amount of uncertainty that needs to be factored in/calculated. So we come up with a range and state that the population parameter (e.g. population means \\mu \\mu ) would fall within this range. This is called the confidence interval Definition A confidence interval is the range of numbers that is believed to include an unknown population parameter. Confidence level Let's recall the empirical split. Recall Empirical rule/split The empirical rules state that for a normal distribution, nearly all of the data will fall within three standard deviations of the mean. 68% data will fall within 1 standard deviation 95% data will fall within 2 standard deviation 99.7% data will fall within 3 standard deviation This rule is called as the 3 sigma rule or 68-95-99.7 rule We know that if a population is normally distributed, then 68% of the data will fall under 1 standard deviation. This also means that we are 68% confident that the population mean is within the 1 standard deviation range. Similarly, a 95% confidence level means that the population mean will be within the 2 standard deviation range. So Confidence levels are expressed as a percentage (for example, a 95% confidence level). It means that should you repeat an experiment or survey over and over again, 95 percent of the time your results will match the results you get from a population. Formula The formula for computing the confidence interval is CI = \\mu \\pm z *SE CI = \\mu \\pm z *SE where \\mu \\mu is the mean, z z is the quantile, SE SE is the standard error Most of the time, the population mean is not known. In that case, the formula becomes CI = \\bar{x} \\pm z *SE CI = \\bar{x} \\pm z *SE where \\bar{x} \\bar{x} is the mean of the sampling distribution. Substituting the standard error formula (mentioned above), we get CI = \\bar{x} \\pm z * \\frac{\\sigma}{\\sqrt n} CI = \\bar{x} \\pm z * \\frac{\\sigma}{\\sqrt n} This would give the confidence interval - the range within which the population parameter would fall. Margin of error # The margin of error is the range of the expected variation for a given survey result. If we keep repeating the survey using the same methodology, the results of the survey would fall within that range of variation. In short, half of the confidence interval is called the margin of error. The margin of error (ME) is denoted by the formula z * SE z * SE which is ME = z * \\frac{\\sigma}{\\sqrt n} ME = z * \\frac{\\sigma}{\\sqrt n} The following image represents the confidence interval and the margin of error in a sampling distribution. Example # A survey was taken of US companies that do business with firms in India. One of the survey questions was: Approximately how many years has your company been trading with firms in India? A random sample of 44 responses to this question yielded a mean of 10.455 years . Suppose the population standard deviation for this question is 7.7 years, construct a 90% confidence interval of the mean number of years that a company has been trading in India for the population of US companies trading with firms in India. Known parameters n = 44 \\\\ \\bar{x} = 10.455 \\\\ \\sigma = 7.7 \\\\ confidence \\space level = 90% n = 44 \\\\ \\bar{x} = 10.455 \\\\ \\sigma = 7.7 \\\\ confidence \\space level = 90% z value for 90% confidence interval is 1.64 To find: confidence interval that has the population mean Solution approach We know the formula for the confidence interval CI = \\bar{x} \\pm z * \\frac{\\sigma}{\\sqrt n} CI = \\bar{x} \\pm z * \\frac{\\sigma}{\\sqrt n} substituting, we get CI = 10.455 \\pm 1.64 * \\frac{7.7}{\\sqrt 44} CI = 10.455 \\pm 1.64 * \\frac{7.7}{\\sqrt 44} which is CI = [8.545 , 12.365] CI = [8.545 , 12.365] The analyst is 90% confident that the actual population mean number of trading years of firms would be between 8.545 and 12.365. Example 2 # The lung function in 57 people is tested using FEVI (Forced Expiratory Volume in 1 Second) measurements. The mean FEVI value for this sample is 4.062 litres and standard deviation , s is 0.67 litres. Construct the 95% Confidence Interval. Data 2.85 3.42 3.7 4.14 4.47 4.9 2.85 3.48 3.75 4.16 4.5 5 2.98 3.5 3.78 4.2 4.5 5.1 3.04 3.54 3.83 4.2 4.56 5.1 3.1 3.54 3.9 4.3 4.68 5.2 3.1 3.57 3.96 4.3 4.7 5.3 3.19 3.6 4.05 4.32 4.71 5.43 3.2 3.6 4.08 4.44 4.78 3.3 3.69 4.1 4.47 4.8 3.39 3.7 4.14 4.47 4.8 Known parameters \\bar{x} = 4.062 \\\\ s = 0.67 \\\\ n = 57 \\bar{x} = 4.062 \\\\ s = 0.67 \\\\ n = 57 The z value for a 95% confidence level is 1.96 Solution approach We know the formula for the confidence interval CI = \\bar{x} \\pm z * \\frac{\\sigma}{\\sqrt n} CI = \\bar{x} \\pm z * \\frac{\\sigma}{\\sqrt n} substituting, we get CI = 4.062 \\pm 1.96 * \\frac{0.67}{\\sqrt 57} CI = 4.062 \\pm 1.96 * \\frac{0.67}{\\sqrt 57} CI = [3.89, 4.23] CI = [3.89, 4.23] Additional material # What happens to confidence interval as confidence level changes? Refer to this article for the answer. Z-Score # Z-score denotes the number of units of standard deviation from the mean the data point is. A Z-score will also help in finding the probability i.e. area under the curve of where the data point would fall in the normal distribution More info here Formula Z-score is derived using the formula z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma} where x is the data point we are interested in, \\mu \\mu is the mean and \\sigma \\sigma is the standard deviation. Example # Take a look at the weight of newborn babies. Suppose that the mean weight of newborns is 7.5 pounds and the standard deviation is 1.25 pounds . Say you\u2019re interested in determining the probability that a newborn weighs less than 6 pounds. How do you do that? Known parameters \\mu = 7.5 \\\\ \\sigma = 1.25 \\\\ data \\space point \\space x = 6 \\mu = 7.5 \\\\ \\sigma = 1.25 \\\\ data \\space point \\space x = 6 To find the probability of the new-born weighing less than 6 pounds Solution approach We know the formula of the z-score. z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma} substituting the values, we get z = \\frac{6 - 7.5}{1.25} = -1.20 z = \\frac{6 - 7.5}{1.25} = -1.20 The graph would look like the below To find the probability, the z-table is used. Note There are two Z-tables - one for a positive value of Z and other for a negative value of Z. Here, the value of Z is -1.20 which is on the negative side and hence, we will be using the negative z table. -1.20 is read in the table as -1.2 in the rows and 0 in the columns - highlighted below. The probability that the baby is born less than 6 pounds is 0.1150 or 11.5%. What is the probability that the new-born baby might weigh more than 10 pounds? Let's visualize it. Solution approach Let's compute the Z-score We know the formula of the z-score. z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma} substituting the values, we get z = \\frac{10 - 7.5}{1.25} = 2.0 z = \\frac{10 - 7.5}{1.25} = 2.0 The graph would look like the below - the shaded area is the probability of newborn weighing less than 10 pounds Looking into the z-table for positive scores, we get the probability value as 0.97725 i.e 97.7% of the newborn is less than or equal to 10 pounds. The question is about the probability of babies weighing more than 10 pounds To find that, we have to subtract the babies weighing less than or equal to 10 pounds from 1 i.e. babies weighing more than 10 pounds = 1 - babies weighing less than 10 pounds Note We are subtracting from 1 because the total probability is equal to 1 So we get Babies weighing more than 10 pounds = 1 - 0.9772 = 0.0228 = 1 - 0.9772 = 0.0228 or 2.2\\% 2.2\\% Summary # To summarize, we started with what is inferential statistics - using the samples to estimate the population, what is a parameter and a statistic, confidence interval and confidence level, standard error and the margin of error, Z- scores and a few examples in each of these. Next post Hypothesis Testing","title":"Inferential Stats - Basics"},{"location":"statistics/Inferential_stats_basics/#inferential_statistics","text":"In real-life situations, we may not be able to know the entire population in which case, we use the known random sample to extract information about the unknown population from which the sample is drawn. To put it simply, using the sample to come up with an estimation of the population is called as Inferential statistics","title":"Inferential Statistics"},{"location":"statistics/Inferential_stats_basics/#parameter_and_statistic","text":"A numerical measure of a population is called a population parameter or more simply, a parameter . Examples of parameters are population mean, population variance, population standard deviation, etc. A numerical measure of the sample is called a sample statistic , or simply a statistic . Examples of sample statistics are the sample mean, sample variance, sample standard deviation, etc. In inferential statistics, population parameters are estimated by sample statistics. When a simple statistic is used to estimate the population parameter, the statistic is called an estimator of the parameter.","title":"Parameter and Statistic"},{"location":"statistics/Inferential_stats_basics/#notations_of_parameter_and_statistic","text":"Population Symbol Sample Symbol Mean \\mu \\mu Mean x x Variance \\sigma^2 \\sigma^2 Variance s^2 s^2 Std deviation \\sigma \\sigma Std deviation s s To summarize, we have the following relationships between a sample statistic and a population parameter \\bar{x} \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\mu \\\\ \\\\ S^2 \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\sigma^2 \\bar{x} \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\mu \\\\ \\\\ S^2 \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\sigma^2","title":"Notations of parameter and statistic"},{"location":"statistics/Inferential_stats_basics/#confidence_levels_and_confidence_intervals","text":"To understand confidence level and confidence intervals, we will need to understand what a population distribution and a sampling distribution is.","title":"Confidence levels and confidence intervals"},{"location":"statistics/Inferential_stats_basics/#population_distribution","text":"The population is the entire set of values/data points that we are interested in. For example, if we know the age of all Indian residents, it is called the population. The population characteristics are mean \\mu \\mu , standard deviation \\sigma \\sigma , median, percentiles, etc.","title":"Population distribution"},{"location":"statistics/Inferential_stats_basics/#sampling_distribution","text":"The sample is a subset of the population which is used to estimate the population characteristic. This sample has a few characteristics like mean \\bar{x} \\bar{x} , standard deviation s s , etc. Samples are used to draw inferences about the population. This is done by drawing samples and computing the sample statistics. When the sample statistics are plotted, the distribution (represented as a histogram) is called the sampling distribution. Note The sampling distribution is also called as theoretical distribution","title":"Sampling distribution"},{"location":"statistics/Inferential_stats_basics/#standard_error","text":"The standard error SE SE is the standard deviation of the sampling distribution","title":"Standard error"},{"location":"statistics/Inferential_stats_basics/#standard_error_of_the_mean","text":"The standard error (or deviation) of the mean can be expressed as SE = \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} SE = \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} where \\sigma \\sigma is the standard deviation of the population n n is the size of the sample Since the standard deviation of the population is rarely known, the standard error of the mean is usually estimated as the sample standard deviation divided by the square root of the sample size i.e. SE = \\sigma_{\\bar{x}} \\approx \\frac{s}{\\sqrt n} SE = \\sigma_{\\bar{x}} \\approx \\frac{s}{\\sqrt n} where s s is the sample standard deviation n n is the size of the sample.","title":"Standard error of the mean"},{"location":"statistics/Inferential_stats_basics/#confidence_interval","text":"When we use samples to project population estimates, we cannot be CERTAIN that it will be accurate. There is an amount of uncertainty that needs to be factored in/calculated. So we come up with a range and state that the population parameter (e.g. population means \\mu \\mu ) would fall within this range. This is called the confidence interval","title":"Confidence interval"},{"location":"statistics/Inferential_stats_basics/#margin_of_error","text":"The margin of error is the range of the expected variation for a given survey result. If we keep repeating the survey using the same methodology, the results of the survey would fall within that range of variation. In short, half of the confidence interval is called the margin of error. The margin of error (ME) is denoted by the formula z * SE z * SE which is ME = z * \\frac{\\sigma}{\\sqrt n} ME = z * \\frac{\\sigma}{\\sqrt n} The following image represents the confidence interval and the margin of error in a sampling distribution.","title":"Margin of error"},{"location":"statistics/Inferential_stats_basics/#example","text":"A survey was taken of US companies that do business with firms in India. One of the survey questions was: Approximately how many years has your company been trading with firms in India? A random sample of 44 responses to this question yielded a mean of 10.455 years . Suppose the population standard deviation for this question is 7.7 years, construct a 90% confidence interval of the mean number of years that a company has been trading in India for the population of US companies trading with firms in India.","title":"Example"},{"location":"statistics/Inferential_stats_basics/#example_2","text":"The lung function in 57 people is tested using FEVI (Forced Expiratory Volume in 1 Second) measurements. The mean FEVI value for this sample is 4.062 litres and standard deviation , s is 0.67 litres. Construct the 95% Confidence Interval.","title":"Example 2"},{"location":"statistics/Inferential_stats_basics/#additional_material","text":"What happens to confidence interval as confidence level changes? Refer to this article for the answer.","title":"Additional material"},{"location":"statistics/Inferential_stats_basics/#z-score","text":"Z-score denotes the number of units of standard deviation from the mean the data point is. A Z-score will also help in finding the probability i.e. area under the curve of where the data point would fall in the normal distribution More info here","title":"Z-Score"},{"location":"statistics/Inferential_stats_basics/#example_1","text":"Take a look at the weight of newborn babies. Suppose that the mean weight of newborns is 7.5 pounds and the standard deviation is 1.25 pounds . Say you\u2019re interested in determining the probability that a newborn weighs less than 6 pounds. How do you do that?","title":"Example"},{"location":"statistics/Inferential_stats_basics/#summary","text":"To summarize, we started with what is inferential statistics - using the samples to estimate the population, what is a parameter and a statistic, confidence interval and confidence level, standard error and the margin of error, Z- scores and a few examples in each of these.","title":"Summary"},{"location":"statistics/descriptive_stats_basics/","text":"Descriptive Statistics # Statistics Basics # What is statistics? # A branch of mathematics taking and transforming numbers into useful information for decision-makers. It originated from the word Stata which means state - i.e. understanding about the state of data. It is a way to get information from data. Complex data can be easily translated into meaningful information using statistics. Example # A college in the US has students from the following countries. Which country is in the majority? US Canada China US India England Mexico Japan China Germany China China Germany US Japan India US China India Japan US Japan India US England China Canada US India China Sweden Mexico India China India Mexico Pakistan Japan China US China US Japan China Japan US India Germany China Japan It is very difficult to get that information from this data when presented this way. However, when the same data is converted into meaningful information, conclusions can arrive very easily. Translating the above table into the below table gives us the answer much faster. This is the use of statistics. Count Frequency Canada 2 China 12 England 2 Germany 3 India 8 Japan 8 Mexico 3 Pakistan 1 Sweden 1 US 10 Example 2 # Data can be misleading. A parent changes the school of their Son who is studying in 11^{th} 11^{th} standard since his academic results are not good in 10^{th} 10^{th} Standard in his current School. They change Student A from ABC school to XYZ school The result was different Ranked 15^{th} 15^{th} in ABC school Ranked 2^{nd} 2^{nd} in XYZ school What's the conclusion? Has the student improved? Well, it depends on the number of students studying school. It seems that in the XYZ school, only 2 students were studying and hence he came 2^{nd} 2^{nd} . Such clarity of information can be obtained via statistics. The knowledge of statistics allows us to make better sense of the use of numbers. Statistics in a nutshell The branch of statistics can be summed up as Collecting data Analyzing data Interpreting data Presenting data Statistics classification # Statistics can be classified into two types Descriptive statistics Presenting, organizing and summarizing data is called descriptive statistics Inferential statistics Drawing conclusions about a population based on data observed in a sample is called as inferential statistics We talked about the population and sample in the definition of the inferential statistics above. What does it mean? Population is the entire set of data points available e.g. elections, 10-year census. Sample is a subset of the data points taken from the population. e.g. opinion polls Parameter and Statistic A parameter is a descriptive measure of the population e.g. population mean, population variance, population standard deviation, etc A statistic is a descriptive measure of a sample e.g. sample mean, sample variance, sample standard deviation, etc. Types of variables # Variables are the fancy name of data in statistics. Variables can be classified as Qualitative or Quantitative 1 2 3 4 5 6 7 graph TD A [ Variables ] --> B(Qualitative) A [ Variables ] --> C(Quantitative) B [ Qualitative ] --> D(Nominal) B [ Qualitative ] --> E(Ordinal) C [ Quantitative ] --> F(Discrete) C [ Quantitative ] --> G(Continuous) Nominal variables Variables that have a name is called as nominal variables E.g. Gender (male/female), Ethnicity (Indian, American, Russian), etc Ordinal variables Variables that are based on order/rank E.g. Movie ratings (1 star to 5 stars), Fortune 50 rankings, etc. Discrete variables Anything that can be counted is a discrete variable e.g number of cats in a room, number of tires in a car Anything that can be measured is a continuous variable e.g. Weight of sugar, time, age, etc Check your understanding # Numerical or categorical variable? Age Gender Major Units Housing GPA 18 Male Psychology 16 Dorm 3.6 21 Male Nursing 15 Parents 3.1 20 Female Business 16 Apartment 2.8 Age, Units, GPA are numerical variable Gender, Major, Housing are categorical variable Dependent variables The variable that is altered based on the input variables. Ideally, there is a relationship between the input variable that makes the dependent variable change. E.g how much sleep you had before you took the test? Studies is a factor that contributes to less sleep. Independent variables The variable that stands alone and is unaffected by other variables E.g. What one eats with the age of the person? Both are un-related and hence are independent variables. Characteristics of frequency distribution # There are four characteristics of a frequency distribution. They are Modality Symmetry Central tendency Variability Modality # The modality is determined by the number of peaks. A unimodal distribution has one peak, a bimodal distribution has two peaks. Symmetry # A distribution can be symmetric or asymmetric . Symmetric distribution A symmetric distribution is a type of distribution where the left side of the distribution mirrors the right side. Asymmetric distribution An asymmetric distribution is a type of distribution where the left side and the right side are NOT mirrors of each other. It can be either negatively skewed or positively skewed. Negatively skewed In a negatively skewed distribution, the long tail is towards the smaller values Positively skewed In a positively skewed distribution, the long tail is towards the larger values Measure of central tendency # A measure of Central Tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data . In other words, the Central Tendency computes the \"center\" around which the data is distributed. There are three measures of central tendency Median Mode Mean Median Median is the central value is a set of data. To find the median , we arrange the observations in order from smallest to largest value. If there is an odd number of observations, the median is the middle value. If there is an even number of observations, the median is the average of the two middle values. The formula for computing the median is Median = \\space \\frac{n + 1}{2} Median = \\space \\frac{n + 1}{2} Note Median is less influenced by outliers. Mode The mode of a set of data values is the value that appears most often. Mean The mean (average) of a data set is found by adding all numbers in the data set and then dividing by the number of values in the set The formula for computing the mean is Mean \\space \\mu = \\frac{\\sum(x)}{n} Mean \\space \\mu = \\frac{\\sum(x)}{n} Measures of central tendency is not sufficient Let's take an example. The following table shows the scores of two players across a series of matches. We need to select one player out of these two. Who would we choose? Note The mean, median and mode are the same. Match Player A Player B 1 40 40 2 40 35 3 7 45 4 40 52 5 0 30 6 90 40 7 3 29 8 11 43 9 120 37 Sum 351 351 Mean 39 39 Median 40 40 In cases like these, where we are not able to conclude based on the mean, median and mode, we go with the measures of dispersion Measures of dispersion # Measures of Dispersion describe the data spread or how far the measurements are from the center. The measures of dispersion are Range Variance Standard deviation Range The range is the difference between the maximum value and the minimum value is a data set. Variance Variance is the difference between each data point and the mean of the data set. It measures how far a set of numbers are spread out from their average value. Variance is given by the formula Variance \\space = \\frac{\\sum(x - \\mu)^2}{n} Variance \\space = \\frac{\\sum(x - \\mu)^2}{n} Standard deviation The Standard Deviation is a measure of how spread out the numbers are. It is denoted by the letter \\sigma \\sigma A large standard deviation indicates that the data points can spread far from the mean and a small standard deviation indicates that they are clustered closely around the mean. The formula for computing the standard deviation (of a population) is \\sigma = \\sqrt \\frac{(x - \\bar{x})^2}{n} \\sigma = \\sqrt \\frac{(x - \\bar{x})^2}{n} or standard deviation \\sigma = \\sqrt {variance} \\sigma = \\sqrt {variance} Standard deviation for the sample The formula for computing the standard deviation (of a population) is \\sigma = \\sqrt \\frac{(x - \\bar{x})^2}{n -1} \\sigma = \\sqrt \\frac{(x - \\bar{x})^2}{n -1} where, n - 1 n - 1 is the degree of freedom. With the measures of dispersion, let's revisit the previous example and compute the standard deviation Match Player A Player B 1 40 40 2 40 35 3 7 45 4 40 52 5 0 30 6 90 40 7 3 29 8 11 43 9 120 37 Sum 351 351 Mean 39 39 Median 40 40 Std Dev 41.518 7.280 We will select player B as the standard deviation is less which means he is more consistent. Percentile / Quartile # Percentile # A percentile (or a centile) is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations falls. For example, the 20 th percentile is the value (or score) below which 20% of the observations may be found. Nth percentile states that there are at least N% of values less than or equal to this value and (100-N) values are greater or equal to this value The formula for finding the number in the N'th percentile is i = \\frac{N}{100}* n i = \\frac{N}{100}* n where N N is the percentile we are interested in and n n is the number of values Key If i is decimal then round off to next value If i is an integer then take the average of i and i+1 value Computing percentile in Excel Data 3310 3355 3450 3480 3480 3490 3520 3540 3550 3650 3730 3925 Solution approach First, populate the data in excel and then sort it in ascending order Use the PERCENTILE.INC function to get the percentile. Use the PERCENTRANK.INC function to find the percentile for each value in a range Quartile # Quartile means dividing data % into 4 parts QI - First Quartile - 25 th percentile Q2 - Second Quartile - 50 th percentile (Median) Q3 - Third Quartile - 75 th percentile Inter Quartile Range (IQR) IQR is the difference between the third quartile and the first quartile IQR = Q3 - Q1 IQR = Q3 - Q1 Let's take the same data and compute the IQR for it Use the excel function QUARTILE.INC to compute the corresponding quartile. Coefficient of Variation # The coefficient of variation shows the extent of variability of data in a sample about the mean of the population. The lesser the coefficient of variation, the better. It is the ratio of the standard deviation to the mean. The formula for computing the coefficient of variation (CV) is CV= \\frac{\\sigma}{\\mu} CV= \\frac{\\sigma}{\\mu} where: \\sigma \\sigma = standard deviation \\mu \\mu = mean\u200b Example # Let's take an example and see why the coefficient of variation plays an important part. Data In an Under 19 World Cup selection squad for 2018, the BCCI needs to select 1 player based on the current performance in 2017 - 2018 Ranji Trophy. There are 2 players with similar stats and the board is not sure whom to select. Can you help the board members with your analysis? Player X Player Y 40 35 20 40 5 7 20 23 10 20 75 26 100 12 25 30 15 27 15 102 20 18 17 17 11 14 5 7 Solution approach Let's compute the mean and the standard deviation Player X Player Y 40 35 20 40 5 7 20 23 10 20 75 26 100 12 25 30 15 27 15 102 20 18 17 17 11 14 5 7 Mean 27 27 Std Dev 27.5317998 23.70978376 The mean is both the same and the standard deviation is almost the same. How do we decide which player to choose? Let's compute the coefficient of variation using the formula CV= \\frac{\\sigma}{\\mu} CV= \\frac{\\sigma}{\\mu} Player X Player Y 40 35 20 40 5 7 20 23 10 20 75 26 100 12 25 30 15 27 15 102 20 18 17 17 11 14 5 7 Mean 27 27 Std Dev 27.5317998 23.70978376 CV 1.019696289 0.878140139 We see that the CV for the Player Y is much lesser than Player X. Hence, player Y is more consistent and should be selected. Correlation and Covariance # What if we need to measure the association between two variables? So far, we have seen only one variable. We have two measures that will help us determine the association between two variables Covariance Correlation coefficient Covariance # Covariance is a measure of how much two random variables vary together. It's similar to variance, but where variance tells you how a single variable varies, covariance tells you how two variables vary together. Since covariance is the variance for two variables, the formula is Cov(X,Y)= \\frac{\\sum{(X_i - \\bar{X})*(Y_i - \\bar{Y})}}{n} Cov(X,Y)= \\frac{\\sum{(X_i - \\bar{X})*(Y_i - \\bar{Y})}}{n} X_i X_i = Observation point of variable \\bar{X} \\bar{X} = Mean of all observations(X) Y_i Y_i = Observation point of variable Y \\bar{Y} \\bar{Y} = Mean of all observations(Y) n n = Number of observations Interpretation Covariance explains causation between two variables. Higher the value of covariance, the stronger the relationship between two variables. Covariance can range from -\\infty \\space to +\\infty -\\infty \\space to +\\infty Example # Data Let us take two variables - temperature and the number of customers and try to understand the covariance between these two variables Temperature No of customers 97 14 86 11 89 9 84 9 94 15 74 7 Solution approach Let's do the covariance computation in excel Go to Data -> Data Analysis and Select \"Covariance\" Provide the input range and the output range and click \"OK\" Excel does the computation and shows the covariance between temperature and the number of customers which is 18.72 18.72 . A positive covariance suggests that these two variables are positively associated. But, there is one problem. We don't know to how extent these variables are related or in other words, how strong is their relationship? This is where Correlation comes into the picture. Correlation # Correlation determines the degree or the extent to which the two variables are associated. The correlation coefficient ranges from -1 \\space to +1 -1 \\space to +1 Interpretation A coefficient greater than +0.5 +0.5 indicates a positive correlation. A coefficient lesser than -0.5 -0.5 indicates a negative correlation. Formula r = \\frac{Cov(x,y)}{\\sigma_x * \\sigma_y} r = \\frac{Cov(x,y)}{\\sigma_x * \\sigma_y} Where r r is the correlation coefficient \\sigma_x \\sigma_x is the standard deviation of x x \\sigma_y \\sigma_y is the standard deviation of y y Solution approach Let's take the same data and compute the correlation coefficient. Go to Data -> Data analysis -> Correlation and Click \"OK\" Provide the input range and the output range and click \"OK\" We get the correlation coefficient as 0.88 This means the variables temperature and number of customers are highly correlated. Thus correlation provides the extent of correlation between the two variables. Correlation types Positive correlation is a relationship between two variables in which both variables move in the same direction. i.e as x increases, y also increases. E.g. housing prices over time Negative correlation is a relationship between two variables in which both variables move in the opposite direction. i.e. as x increases, y decreases. E.g Health over time When there is no correlation between variables, the graph would look like below Central Limit Theorem (CLT) # The central limit theorem states that if you have a population with mean \u03bc and standard deviation \u03c3 and take sufficiently large random samples ( n \\ge 30 n \\ge 30 ) from the population, then the distribution of the sample means will be approximately normally distributed. The following three properties hold in the central limit theorem \\mu_{\\bar{x}} = \\mu \\mu_{\\bar{x}} = \\mu \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} z = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} z = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} Summary # In this post, we saw what is statistics and how the data is described using descriptive statistics. We have also seen the measures of central tendency, and the measures of variation. We touched upon the coefficient of variation and how to measure the association of two variables using covariance and correlation. We finally concluded with Central Limit Theorem which is crucial when we proceed with inferential statistics. Next post Inferential statistics","title":"Descriptive Statistics"},{"location":"statistics/descriptive_stats_basics/#descriptive_statistics","text":"","title":"Descriptive Statistics"},{"location":"statistics/descriptive_stats_basics/#statistics_basics","text":"","title":"Statistics Basics"},{"location":"statistics/descriptive_stats_basics/#what_is_statistics","text":"A branch of mathematics taking and transforming numbers into useful information for decision-makers. It originated from the word Stata which means state - i.e. understanding about the state of data. It is a way to get information from data. Complex data can be easily translated into meaningful information using statistics.","title":"What is statistics?"},{"location":"statistics/descriptive_stats_basics/#example","text":"A college in the US has students from the following countries. Which country is in the majority? US Canada China US India England Mexico Japan China Germany China China Germany US Japan India US China India Japan US Japan India US England China Canada US India China Sweden Mexico India China India Mexico Pakistan Japan China US China US Japan China Japan US India Germany China Japan It is very difficult to get that information from this data when presented this way. However, when the same data is converted into meaningful information, conclusions can arrive very easily. Translating the above table into the below table gives us the answer much faster. This is the use of statistics. Count Frequency Canada 2 China 12 England 2 Germany 3 India 8 Japan 8 Mexico 3 Pakistan 1 Sweden 1 US 10","title":"Example"},{"location":"statistics/descriptive_stats_basics/#example_2","text":"Data can be misleading. A parent changes the school of their Son who is studying in 11^{th} 11^{th} standard since his academic results are not good in 10^{th} 10^{th} Standard in his current School. They change Student A from ABC school to XYZ school The result was different Ranked 15^{th} 15^{th} in ABC school Ranked 2^{nd} 2^{nd} in XYZ school What's the conclusion? Has the student improved? Well, it depends on the number of students studying school. It seems that in the XYZ school, only 2 students were studying and hence he came 2^{nd} 2^{nd} . Such clarity of information can be obtained via statistics. The knowledge of statistics allows us to make better sense of the use of numbers.","title":"Example 2"},{"location":"statistics/descriptive_stats_basics/#statistics_classification","text":"Statistics can be classified into two types","title":"Statistics classification"},{"location":"statistics/descriptive_stats_basics/#types_of_variables","text":"Variables are the fancy name of data in statistics. Variables can be classified as Qualitative or Quantitative 1 2 3 4 5 6 7 graph TD A [ Variables ] --> B(Qualitative) A [ Variables ] --> C(Quantitative) B [ Qualitative ] --> D(Nominal) B [ Qualitative ] --> E(Ordinal) C [ Quantitative ] --> F(Discrete) C [ Quantitative ] --> G(Continuous)","title":"Types of variables"},{"location":"statistics/descriptive_stats_basics/#check_your_understanding","text":"","title":"Check your understanding"},{"location":"statistics/descriptive_stats_basics/#characteristics_of_frequency_distribution","text":"There are four characteristics of a frequency distribution. They are Modality Symmetry Central tendency Variability","title":"Characteristics of frequency distribution"},{"location":"statistics/descriptive_stats_basics/#modality","text":"The modality is determined by the number of peaks. A unimodal distribution has one peak, a bimodal distribution has two peaks.","title":"Modality"},{"location":"statistics/descriptive_stats_basics/#symmetry","text":"A distribution can be symmetric or asymmetric .","title":"Symmetry"},{"location":"statistics/descriptive_stats_basics/#measure_of_central_tendency","text":"A measure of Central Tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data . In other words, the Central Tendency computes the \"center\" around which the data is distributed. There are three measures of central tendency Median Mode Mean","title":"Measure of central tendency"},{"location":"statistics/descriptive_stats_basics/#measures_of_dispersion","text":"Measures of Dispersion describe the data spread or how far the measurements are from the center. The measures of dispersion are Range Variance Standard deviation","title":"Measures of dispersion"},{"location":"statistics/descriptive_stats_basics/#percentile_quartile","text":"","title":"Percentile / Quartile"},{"location":"statistics/descriptive_stats_basics/#percentile","text":"A percentile (or a centile) is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations falls. For example, the 20 th percentile is the value (or score) below which 20% of the observations may be found. Nth percentile states that there are at least N% of values less than or equal to this value and (100-N) values are greater or equal to this value The formula for finding the number in the N'th percentile is i = \\frac{N}{100}* n i = \\frac{N}{100}* n where N N is the percentile we are interested in and n n is the number of values Key If i is decimal then round off to next value If i is an integer then take the average of i and i+1 value","title":"Percentile"},{"location":"statistics/descriptive_stats_basics/#quartile","text":"Quartile means dividing data % into 4 parts QI - First Quartile - 25 th percentile Q2 - Second Quartile - 50 th percentile (Median) Q3 - Third Quartile - 75 th percentile","title":"Quartile"},{"location":"statistics/descriptive_stats_basics/#coefficient_of_variation","text":"The coefficient of variation shows the extent of variability of data in a sample about the mean of the population. The lesser the coefficient of variation, the better. It is the ratio of the standard deviation to the mean. The formula for computing the coefficient of variation (CV) is CV= \\frac{\\sigma}{\\mu} CV= \\frac{\\sigma}{\\mu} where: \\sigma \\sigma = standard deviation \\mu \\mu = mean\u200b","title":"Coefficient of Variation"},{"location":"statistics/descriptive_stats_basics/#example_1","text":"Let's take an example and see why the coefficient of variation plays an important part.","title":"Example"},{"location":"statistics/descriptive_stats_basics/#correlation_and_covariance","text":"What if we need to measure the association between two variables? So far, we have seen only one variable. We have two measures that will help us determine the association between two variables Covariance Correlation coefficient","title":"Correlation and Covariance"},{"location":"statistics/descriptive_stats_basics/#covariance","text":"Covariance is a measure of how much two random variables vary together. It's similar to variance, but where variance tells you how a single variable varies, covariance tells you how two variables vary together. Since covariance is the variance for two variables, the formula is Cov(X,Y)= \\frac{\\sum{(X_i - \\bar{X})*(Y_i - \\bar{Y})}}{n} Cov(X,Y)= \\frac{\\sum{(X_i - \\bar{X})*(Y_i - \\bar{Y})}}{n} X_i X_i = Observation point of variable \\bar{X} \\bar{X} = Mean of all observations(X) Y_i Y_i = Observation point of variable Y \\bar{Y} \\bar{Y} = Mean of all observations(Y) n n = Number of observations","title":"Covariance"},{"location":"statistics/descriptive_stats_basics/#example_3","text":"","title":"Example"},{"location":"statistics/descriptive_stats_basics/#correlation","text":"Correlation determines the degree or the extent to which the two variables are associated. The correlation coefficient ranges from -1 \\space to +1 -1 \\space to +1","title":"Correlation"},{"location":"statistics/descriptive_stats_basics/#central_limit_theorem_clt","text":"The central limit theorem states that if you have a population with mean \u03bc and standard deviation \u03c3 and take sufficiently large random samples ( n \\ge 30 n \\ge 30 ) from the population, then the distribution of the sample means will be approximately normally distributed. The following three properties hold in the central limit theorem \\mu_{\\bar{x}} = \\mu \\mu_{\\bar{x}} = \\mu \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} z = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} z = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}}","title":"Central Limit Theorem (CLT)"},{"location":"statistics/descriptive_stats_basics/#summary","text":"In this post, we saw what is statistics and how the data is described using descriptive statistics. We have also seen the measures of central tendency, and the measures of variation. We touched upon the coefficient of variation and how to measure the association of two variables using covariance and correlation. We finally concluded with Central Limit Theorem which is crucial when we proceed with inferential statistics.","title":"Summary"},{"location":"statistics/hypothesis_testing_01/","text":"Hypothesis Testing # What is Hypothesis Testing? # First let us understand what does the word hypothesis mean. Let us break the word into two parts hypo + thesis . What does thesis mean? thesis means something that has already been proven to be true. e.g Atleast 60% of the adult human body is made up of water. Sounds fine! So what does hypothesis mean? Hypothesis is something that is not yet been proven to be true. Let's come back to the original question! What is hypothesis testing? Can we say that it is the testing of hypothesis or more precisely the process of determining whether a given hypothesis is true or not To sum up, we take the hypothesis, we perform some statistical computations and prove if the hypothesis holds true or not. Null Hypothesis & Alternate Hypothesis # Null hypothesis is the assertion or belief that we hold as true unless we have sufficient evidence to prove otherwise. In statistical terms, we say it as the belief we hold about the value of a population parameter Remember Parameter is for the population and Statistic is for the sample Let's take an example and see what is the null hypothesis and how it is written. We believe that the mean of the population is 500 . Unless we obtain sufficient evidence that it is not 500 , our belief holds true i.e we accept that the mean is 500 So we can write it as null hypothesis: mean = 500 To write it more compactly, we can represent the same thing as \\mathbf{H}_\\mathbf{0} : \\mu = 500 \\mathbf{H}_\\mathbf{0} : \\mu = 500 where the symbol \\space\\mathbf{H}_\\mathbf{0} \\space\\mathbf{H}_\\mathbf{0} denotes Null hypothesis. The opposite of Null hypothesis is called as Alternate Hypothesis . That is, the negation of the null hypothesis. If there is a way to represent the null hypothesis, then there should be a way to represent the alternate hypothesis. Agreed? Ah, I see I have quoted a null hypothesis there! \\mathbf{H}_\\mathbf{1}: \\mu \\ne 500 \\mathbf{H}_\\mathbf{1}: \\mu \\ne 500 where the symbol \\space\\mathbf{H}_\\mathbf{1} \\space\\mathbf{H}_\\mathbf{1} denotes alternate hypothesis Note Since the null hypothesis and the alternate hypothesis are exactly opposite statements, only one can be true. Rejecting one is accepting the other. Let's check our understanding with a few examples Example 1 My broadband company claims that the minimum internet speed they are providing is 150 Mbps. However, I suspect that they are not providing the promised internet speed. Let's write the null and alternate hypothesis for this! \\mathbf{H}_\\mathbf{0}: speed \\ge 150 Mbps \\\\ \\mathbf{H}_\\mathbf{1}: speed < 150 Mbps \\mathbf{H}_\\mathbf{0}: speed \\ge 150 Mbps \\\\ \\mathbf{H}_\\mathbf{1}: speed < 150 Mbps Example 2 The project manager claims that the software defects raised are resolved within 5 business days. I being the quality department manager think it is taking longer to fix the bugs. Can we write the null and alternate hypothesis for this? \\mathbf{H}_\\mathbf{0}: Resolution\\space period \\le 5 days \\\\ \\mathbf{H}_\\mathbf{1}: Resolution\\space period > 5 days \\mathbf{H}_\\mathbf{0}: Resolution\\space period \\le 5 days \\\\ \\mathbf{H}_\\mathbf{1}: Resolution\\space period > 5 days Let's see how we can prove the alternate hypothesis Example 1 To check the internet speed, I visit Speed Test from my home computer and measure the speed. I do this 'n' number of times and find that the mean speed is around 100 Mbps. So we reject the null hypothesis and accept the alternate hypothesis - which is the internet speed is less than 150 Mbps Example 2 To verify the claims of the project manager, I take all of the 5 projects managed by him. He is relatively new to the project manager role and has only managed 5 projects so far. I get the bug details from the bug tracking system - bugzilla / jira for example. I compute the mean of all the bug resolution time by doing some computations ( calculate the difference between the bug start date and bug end date and so on). I find that the resolution time is indeed less than 5 days. So we accept the null hypothesis here i.e. resolution time is less than or equal to 5 business days. Wow, Hypothesis testing is very easy! Level of significance # Life is not easy always, Isn't it? In the above two cases, taking the samples was easy and our agreement / rejection of the null hypothesis was indeed accurate. In real life scenarios, we need to estimate a population parameter based on the sample and things might go wrong. That is to say, we might reject the null hypothesis when it is actually true or we might accept the null hypothesis when it is otherwise. This depends on the sample we take and if we don't have enough samples (or worse picked up wrong samples), things might go wrong. Say for example, the null hypothesis states that the mean of the population is greater than or equal to 500 i.e. \\mathbf{H}_\\mathbf{0}: \\mu \\ge 500 \\mathbf{H}_\\mathbf{0}: \\mu \\ge 500 and the alternate hypothesis is mean less than 500 \\mathbf{H}_\\mathbf{1}: \\mu < 500 \\mathbf{H}_\\mathbf{1}: \\mu < 500 Let's say, we took a sample size of 30 i.e n=30 and found the mean is 499 . Ah! now comes the dilemma - whether we need to accept or reject the null hypothesis? The sample mean is just falling short by 1 from the population mean. We go into self-doubt if we have taken the correct sample size or what happens if the sample size is increased or worst case, should I have to repeat the experiment once again and my manager fires me for wasting the time and effort? So, what we are contemplating here is the probability of the evidence (samples picked) being unfavorable to the null hypothesis. This probablility is called as the p-value If we say the p_value or probability is 2% , it means that our sample has 2% chances of going wrong in rejecting the null hypothesis. If we say the p_value or probability is 30% , it means that our sample has 30% chances of going wrong in rejecting the null hypothesis and our chances of getting a promotion will be adversely impacted. But, we are humans and we need to have a leeway (a threshold) for making some mistakes / error with the samples. This threshold is called as the level of significance In other words, level of significance is the maximum level of risk (maximum acceptable probability - in statistical terms) that we may take in rejecting the null hypothesis while the null hypothesis is actually true. Level of significance is denoted by the letter \\alpha \\alpha (alpha) alpha is normally represented as 1% or 5% or 10% etc. Let's consolidate our understanding! if the p-value is less than the level of significance , we reject the null hypothesis. if the p-value is greater than the level of significance , we accept the null hypothesis. Confused? Yeah, it happened with me. If not, I salute you. Proof for the 1 st statement Let's say the p-value is 2% and the level of significance is 5% . What does this mean? It means the probability of getting our samples wrong is 2% but the maximum risk that we can take is 5% . Correct? Remember our quest is always to prove the null hypothesis is wrong . In this case, our samples are within the permissible limits of going wrong and hence we reject the null hypothesis. Proof for the 2 nd statement Let's say the p-value is 10% and the level of significance is 5% . What does this mean? It means the probability of getting our samples wrong is 10% but the maximum risk that we can take is 5% . Correct? In this case, our samples are above the permissible limits of going wrong and hence we cannot reject the null hypothesis and have to accept the null hypothesis. Confidence level # When the null hypothesis is rejected with a level of significance of 5% , we may be questioned of how confident we are in rejecting the null hypothesis In other words, it is to say that what is our confidence level in rejecting the null hypothesis. The relationship between confidence level and the level of significance is given by the relation confidence \\space level = (1 - \\alpha) \\\\ confidence \\space level = (1 - \\alpha) \\\\ which in this case is confidence \\space level = (1 - 0.05) \\space = 0.95 confidence \\space level = (1 - 0.05) \\space = 0.95 which means, we need to have atleast 95% confidence level to reject the null hypothesis. Summary # To summarize, we have seen what is hypothesis, what is a null hypothesis, what is an alternate hypothesis and when hypothesis testing can go wrong. In the class, we have not seen the level of significance in detail and what important role it has in hypothesis testing. The last part of the article was included to make this grey area more clearer. Next post Hypothesis Test Process","title":"Hypothesis Testing"},{"location":"statistics/hypothesis_testing_01/#hypothesis_testing","text":"","title":"Hypothesis Testing"},{"location":"statistics/hypothesis_testing_01/#what_is_hypothesis_testing","text":"First let us understand what does the word hypothesis mean. Let us break the word into two parts hypo + thesis . What does thesis mean? thesis means something that has already been proven to be true. e.g Atleast 60% of the adult human body is made up of water. Sounds fine! So what does hypothesis mean? Hypothesis is something that is not yet been proven to be true. Let's come back to the original question! What is hypothesis testing? Can we say that it is the testing of hypothesis or more precisely the process of determining whether a given hypothesis is true or not To sum up, we take the hypothesis, we perform some statistical computations and prove if the hypothesis holds true or not.","title":"What is Hypothesis Testing?"},{"location":"statistics/hypothesis_testing_01/#null_hypothesis_alternate_hypothesis","text":"Null hypothesis is the assertion or belief that we hold as true unless we have sufficient evidence to prove otherwise. In statistical terms, we say it as the belief we hold about the value of a population parameter Remember Parameter is for the population and Statistic is for the sample Let's take an example and see what is the null hypothesis and how it is written. We believe that the mean of the population is 500 . Unless we obtain sufficient evidence that it is not 500 , our belief holds true i.e we accept that the mean is 500 So we can write it as null hypothesis: mean = 500 To write it more compactly, we can represent the same thing as \\mathbf{H}_\\mathbf{0} : \\mu = 500 \\mathbf{H}_\\mathbf{0} : \\mu = 500 where the symbol \\space\\mathbf{H}_\\mathbf{0} \\space\\mathbf{H}_\\mathbf{0} denotes Null hypothesis. The opposite of Null hypothesis is called as Alternate Hypothesis . That is, the negation of the null hypothesis. If there is a way to represent the null hypothesis, then there should be a way to represent the alternate hypothesis. Agreed? Ah, I see I have quoted a null hypothesis there! \\mathbf{H}_\\mathbf{1}: \\mu \\ne 500 \\mathbf{H}_\\mathbf{1}: \\mu \\ne 500 where the symbol \\space\\mathbf{H}_\\mathbf{1} \\space\\mathbf{H}_\\mathbf{1} denotes alternate hypothesis Note Since the null hypothesis and the alternate hypothesis are exactly opposite statements, only one can be true. Rejecting one is accepting the other.","title":"Null Hypothesis &amp; Alternate Hypothesis"},{"location":"statistics/hypothesis_testing_01/#level_of_significance","text":"Life is not easy always, Isn't it? In the above two cases, taking the samples was easy and our agreement / rejection of the null hypothesis was indeed accurate. In real life scenarios, we need to estimate a population parameter based on the sample and things might go wrong. That is to say, we might reject the null hypothesis when it is actually true or we might accept the null hypothesis when it is otherwise. This depends on the sample we take and if we don't have enough samples (or worse picked up wrong samples), things might go wrong. Say for example, the null hypothesis states that the mean of the population is greater than or equal to 500 i.e. \\mathbf{H}_\\mathbf{0}: \\mu \\ge 500 \\mathbf{H}_\\mathbf{0}: \\mu \\ge 500 and the alternate hypothesis is mean less than 500 \\mathbf{H}_\\mathbf{1}: \\mu < 500 \\mathbf{H}_\\mathbf{1}: \\mu < 500 Let's say, we took a sample size of 30 i.e n=30 and found the mean is 499 . Ah! now comes the dilemma - whether we need to accept or reject the null hypothesis? The sample mean is just falling short by 1 from the population mean. We go into self-doubt if we have taken the correct sample size or what happens if the sample size is increased or worst case, should I have to repeat the experiment once again and my manager fires me for wasting the time and effort? So, what we are contemplating here is the probability of the evidence (samples picked) being unfavorable to the null hypothesis. This probablility is called as the p-value If we say the p_value or probability is 2% , it means that our sample has 2% chances of going wrong in rejecting the null hypothesis. If we say the p_value or probability is 30% , it means that our sample has 30% chances of going wrong in rejecting the null hypothesis and our chances of getting a promotion will be adversely impacted. But, we are humans and we need to have a leeway (a threshold) for making some mistakes / error with the samples. This threshold is called as the level of significance In other words, level of significance is the maximum level of risk (maximum acceptable probability - in statistical terms) that we may take in rejecting the null hypothesis while the null hypothesis is actually true. Level of significance is denoted by the letter \\alpha \\alpha (alpha) alpha is normally represented as 1% or 5% or 10% etc.","title":"Level of significance"},{"location":"statistics/hypothesis_testing_01/#confidence_level","text":"When the null hypothesis is rejected with a level of significance of 5% , we may be questioned of how confident we are in rejecting the null hypothesis In other words, it is to say that what is our confidence level in rejecting the null hypothesis. The relationship between confidence level and the level of significance is given by the relation confidence \\space level = (1 - \\alpha) \\\\ confidence \\space level = (1 - \\alpha) \\\\ which in this case is confidence \\space level = (1 - 0.05) \\space = 0.95 confidence \\space level = (1 - 0.05) \\space = 0.95 which means, we need to have atleast 95% confidence level to reject the null hypothesis.","title":"Confidence level"},{"location":"statistics/hypothesis_testing_01/#summary","text":"To summarize, we have seen what is hypothesis, what is a null hypothesis, what is an alternate hypothesis and when hypothesis testing can go wrong. In the class, we have not seen the level of significance in detail and what important role it has in hypothesis testing. The last part of the article was included to make this grey area more clearer.","title":"Summary"},{"location":"statistics/hypothesis_testing_02/","text":"Hypothesis Tests # In the previous article , we got introduced to the concept of hypothesis - null hypothesis and the alternate hypothesis. We concluded with how the p-value is compared with the level of significance to either accept or reject the null hypothesis. We will keep p-value aside at the moment and will later see how it is calculated; less we know - less we are confused. Hypothesis test steps # Just to recap, hypothesis testing is the process of determining whether a given hypothesis is true or not The highlight is on the word process . If it is called a process, then there have to be some steps associated with it. The steps for testing the hypothesis are mentioned below State the null hypothesis \\mathbf{H}_\\mathbf{0} \\mathbf{H}_\\mathbf{0} and the alternate hypothesis \\mathbf{H}_\\mathbf{1} \\mathbf{H}_\\mathbf{1} Choose the level of significance Find critical values Find test statistic Draw your conclusion Ok. Sounds good! Let us take stock of what knowledge we have already about the above-mentioned steps. We know what the null and the alternate hypothesis are, level of significance, and probably presume what draw your conclusion means. What we don't know about at this point is Step 4 - Find critical values and Step 5 - find the test statistic. Good! Let's see what a critical value is! To understand the critical value and critical region, we need to first understand what one-tailed and two-tailed tests are. One-Tailed and Two-Tailed Tests # We have already seen tails in statistics - when there is a long tail to the right of a frequency distribution, the data is positively skewed and where is a long tail to the left, the data is negatively skewed. So it has to do something with the distribution curve? Yes, you are right! Let's dive into detail! What are tails? # In the day-to-day context, we know a tail is that extra portion that is attached to the body of an animal. Similarly, in the context of statistics, tails are that portion that is attached to the side of distribution - see figure below - the grey area Note Distribution does not have to have two tails always. There can be only one tail as well. Now that we have seen the tails visually, we understand things better. But, there are two tails here - do we have names that distinguish between these tails? Yes! It's upper tail and lower tail. The following image shows the lower tail and upper tail respectively. Upper tail The upper tail is towards the upper side i.e. the positive side of the graph (see image above). Remember positive values lie on the right of the graph (1, 2, 3, etc). So it is called as \"right-tail\" also. Lower tail The lower tail is towards the lower side i.e the negative side of the graph. Hence it is referred to as \"left-tail\" as well. So if the distribution has just one tail, it can be either an upper-tail or lower-tail and if the distribution has two-tails, it will have both upper-tail and lower-tail . Critical Region / Rejection Region The region that is shaded in grey i.e. the tail area is called as the Critical Region or Rejection Region The following image shows the rejection region for the lower-tailed graph Similarly, the image below depicts the rejection region for the upper tailed graph The rejection region for the two-tailed graph is shown below So far, so good!? Now in real-life problems, we will not be given the graph to understand if the tail is towards the left or right or it is two-tailed. The problem statement will be provided. We need to figure out the hypothesis and then decide if its right-tailed or left-tailed or both! One-Tailed tests # One-tailed tests can be either an upper tailed test or a lower tailed test. How can we spot or understand which tailed-tests the problem we have at hand belong to? Upper tail or lower tail? Remember in the previous article , we had stated the hypothesis for two examples. Let's revisit those examples. Example 1 My broadband company claims that the minimum internet speed they are providing is 150 Mbps. However, I suspect that they are not providing the promised internet speed. Let's write the null and alternate hypotheses for this! \\mathbf{H}_\\mathbf{0}: speed \\ge 150 Mbps \\\\ \\mathbf{H}_\\mathbf{1}: speed < 150 Mbps \\mathbf{H}_\\mathbf{0}: speed \\ge 150 Mbps \\\\ \\mathbf{H}_\\mathbf{1}: speed < 150 Mbps What we are trying to prove is our alternate hypothesis with evidence that the speed is less than 150 Mbps. So we see the symbol of alternate hypothesis i.e - if it is less than < 150 Mbps, it is called a lower tail test. Example 2 The project manager claims that the software defects raised are resolved within 5 business days. I being the quality department manager think it is taking longer to fix the bugs. The null and alternate hypothesis is mentioned below \\mathbf{H}_\\mathbf{0}: Resolution\\space period \\le 5 days \\\\ \\mathbf{H}_\\mathbf{1}: Resolution\\space period > 5 days \\mathbf{H}_\\mathbf{0}: Resolution\\space period \\le 5 days \\\\ \\mathbf{H}_\\mathbf{1}: Resolution\\space period > 5 days We see that the alternate hypothesis is greater than > 5 days. Hence, it is an upper-tail test. Important The upper-tailed test or lower-tailed test is determined based on the alternative hypothesis only and NOT the null hypothesis. To conclude, one-tailed tests can be either an upper-tail test or a lower-tail test and the test is determined by the symbol of the alternate hypothesis \\mathbf{H}_\\mathbf{1} \\mathbf{H}_\\mathbf{1} Two-Tailed tests # So how does the hypothesis of a two-tailed test look like? A residential school claims that the average time the students of the school get to do extra-curricular activities is 200 hours per year but, the parents doubt that the students spend so much time. The null and the alternate hypothesis looks like the below \\mathbf{H}_\\mathbf{0}: \\mu = 200 \\space hours \\\\ \\mathbf{H}_\\mathbf{1}: \\mu \\ne 200 \\space hours \\mathbf{H}_\\mathbf{0}: \\mu = 200 \\space hours \\\\ \\mathbf{H}_\\mathbf{1}: \\mu \\ne 200 \\space hours Here we see the symbol of the alternate hypothesis is \\ne \\ne which means the test is a two-tailed test. We don't check for increase or decrease i.e. \\ge \\ge or \\le \\le but, we check for a change in the parameter. So the critical region / tails are split over both the ends. Both the ends contain \\alpha/2 \\alpha/2 , making a total of \\alpha \\alpha - which is the level of significance. Refer to the previous article . Milestone reached: So from the hypothesis statement, we are understanding if the test is either a one-tailed (upper or lower tail) or two-tailed. We are still in step 3 - Finding the critical value Let's understand what a critical value is and then we will see how to compute the critical value. Critical value # We know what a critical region or a rejection region is! So critical value should be near or in that critical region. To take an analogy, the critical values of water - i.e. the boiling point is 100 deg Celcius and the freezing point is 0 deg Celcius. It is an important measure that helps us make important decisions. Definition of critical value A critical value is a line on a graph that splits the graph into sections. If our test value falls into that region, then you reject the null hypothesis - which means the samples and the evidence we had taken supports the alternate hypothesis Critical value depicted in the graph below - for lower-tailed graph Similarly, the critical value for the upper-tailed graph is below Critical values for a two-tailed graph is below In the case of a two-tailed graph, the value that we are computing should be either below or above the critical value for rejecting the null hypothesis. Ok - now we have visualized what a critical value is. This critical value is nothing but the 'Z' score We already know some of the common 'Z' scores. Example of a 'Z' score Let's say we have a sample normal distribution. We know that as per the empirical split (or rule), 68% lie within the 1 standard deviation from the mean, 95% lie within 2 standard deviation from the mean and 99.7% of the values lie within 3 standard deviations from the mean. We know that 'Z' score for +2.0 standard deviation is 1.96 and for -2.0 standard deviation is -1.96 . We have used this in several computations in the class. We will see how to compute 'Z' scores in another post. Let us see how the level of significance ( \\alpha \\alpha ) is used to find the critical value (Z score). Level of significance is covered in this post Critical value and level of significance # We use the level of significance ( \\alpha \\alpha ) to determine the critical value. How? Example: Determine the critical value at 5% level of significance. Presume its a right-tailed / upper-tailed test . We know \\alpha \\alpha = 5% (see image below to find where level of significance \\alpha \\alpha of 5% falls) and 1 - \\alpha \\alpha is 95% We know that 95% of the values fall within the 2 standard deviation mark and the corresponding the 'Z' score is 1.96 . So the critical value ( Z ) is 1.96 . So how will we determine if we have to accept or reject the null hypothesis? I am quoting a new word test statistic here. We will see what it is and how it is computed in the next post. At the moment, just think of test statistic as a number computed from our samples. If the test statistic ( test Z ) is less than the critical value ( Z ), we will accept the null hypothesis. In other words, we have not stepped into the rejection region and hence will accept. test\\space Z \\le Z : \\space accept \\space \\mathbf{H}_\\mathbf{0} \\\\ test\\space Z > Z : \\space reject \\space \\mathbf{H}_\\mathbf{0} test\\space Z \\le Z : \\space accept \\space \\mathbf{H}_\\mathbf{0} \\\\ test\\space Z > Z : \\space reject \\space \\mathbf{H}_\\mathbf{0} Let's visualize the above scenario with an example graph! The below are the numbers Critical value Z = 1.96 Test statistic ( test Z ) = 0.5 Since the test statistic is less than the critical value, we will accept the null hypothesis \\mathbf{H}_\\mathbf{0} \\mathbf{H}_\\mathbf{0} Summary # In this article, we focussed on the 3 rd step of the hypothesis testing process - Find the critical value We started with what are tails, upper tail and lower tail, one-tailed tests and two-tailed tests, critical region / rejection region, critical values and when to accept or reject the null hypothesis. In the next post, we will explore more on finding the test statistic which is the 4 th step of the hypothesis testing process. Next post Test statistic - Z and t","title":"Hypothesis Test Process"},{"location":"statistics/hypothesis_testing_02/#hypothesis_tests","text":"In the previous article , we got introduced to the concept of hypothesis - null hypothesis and the alternate hypothesis. We concluded with how the p-value is compared with the level of significance to either accept or reject the null hypothesis. We will keep p-value aside at the moment and will later see how it is calculated; less we know - less we are confused.","title":"Hypothesis Tests"},{"location":"statistics/hypothesis_testing_02/#hypothesis_test_steps","text":"Just to recap, hypothesis testing is the process of determining whether a given hypothesis is true or not The highlight is on the word process . If it is called a process, then there have to be some steps associated with it. The steps for testing the hypothesis are mentioned below State the null hypothesis \\mathbf{H}_\\mathbf{0} \\mathbf{H}_\\mathbf{0} and the alternate hypothesis \\mathbf{H}_\\mathbf{1} \\mathbf{H}_\\mathbf{1} Choose the level of significance Find critical values Find test statistic Draw your conclusion Ok. Sounds good! Let us take stock of what knowledge we have already about the above-mentioned steps. We know what the null and the alternate hypothesis are, level of significance, and probably presume what draw your conclusion means. What we don't know about at this point is Step 4 - Find critical values and Step 5 - find the test statistic. Good! Let's see what a critical value is! To understand the critical value and critical region, we need to first understand what one-tailed and two-tailed tests are.","title":"Hypothesis test steps"},{"location":"statistics/hypothesis_testing_02/#one-tailed_and_two-tailed_tests","text":"We have already seen tails in statistics - when there is a long tail to the right of a frequency distribution, the data is positively skewed and where is a long tail to the left, the data is negatively skewed. So it has to do something with the distribution curve? Yes, you are right! Let's dive into detail!","title":"One-Tailed and Two-Tailed Tests"},{"location":"statistics/hypothesis_testing_02/#what_are_tails","text":"In the day-to-day context, we know a tail is that extra portion that is attached to the body of an animal. Similarly, in the context of statistics, tails are that portion that is attached to the side of distribution - see figure below - the grey area Note Distribution does not have to have two tails always. There can be only one tail as well. Now that we have seen the tails visually, we understand things better. But, there are two tails here - do we have names that distinguish between these tails? Yes! It's upper tail and lower tail. The following image shows the lower tail and upper tail respectively.","title":"What are tails?"},{"location":"statistics/hypothesis_testing_02/#one-tailed_tests","text":"One-tailed tests can be either an upper tailed test or a lower tailed test. How can we spot or understand which tailed-tests the problem we have at hand belong to? Upper tail or lower tail? Remember in the previous article , we had stated the hypothesis for two examples. Let's revisit those examples. Example 1 My broadband company claims that the minimum internet speed they are providing is 150 Mbps. However, I suspect that they are not providing the promised internet speed. Let's write the null and alternate hypotheses for this! \\mathbf{H}_\\mathbf{0}: speed \\ge 150 Mbps \\\\ \\mathbf{H}_\\mathbf{1}: speed < 150 Mbps \\mathbf{H}_\\mathbf{0}: speed \\ge 150 Mbps \\\\ \\mathbf{H}_\\mathbf{1}: speed < 150 Mbps What we are trying to prove is our alternate hypothesis with evidence that the speed is less than 150 Mbps. So we see the symbol of alternate hypothesis i.e - if it is less than < 150 Mbps, it is called a lower tail test. Example 2 The project manager claims that the software defects raised are resolved within 5 business days. I being the quality department manager think it is taking longer to fix the bugs. The null and alternate hypothesis is mentioned below \\mathbf{H}_\\mathbf{0}: Resolution\\space period \\le 5 days \\\\ \\mathbf{H}_\\mathbf{1}: Resolution\\space period > 5 days \\mathbf{H}_\\mathbf{0}: Resolution\\space period \\le 5 days \\\\ \\mathbf{H}_\\mathbf{1}: Resolution\\space period > 5 days We see that the alternate hypothesis is greater than > 5 days. Hence, it is an upper-tail test. Important The upper-tailed test or lower-tailed test is determined based on the alternative hypothesis only and NOT the null hypothesis. To conclude, one-tailed tests can be either an upper-tail test or a lower-tail test and the test is determined by the symbol of the alternate hypothesis \\mathbf{H}_\\mathbf{1} \\mathbf{H}_\\mathbf{1}","title":"One-Tailed tests"},{"location":"statistics/hypothesis_testing_02/#two-tailed_tests","text":"","title":"Two-Tailed tests"},{"location":"statistics/hypothesis_testing_02/#critical_value","text":"We know what a critical region or a rejection region is! So critical value should be near or in that critical region. To take an analogy, the critical values of water - i.e. the boiling point is 100 deg Celcius and the freezing point is 0 deg Celcius. It is an important measure that helps us make important decisions.","title":"Critical value"},{"location":"statistics/hypothesis_testing_02/#critical_value_and_level_of_significance","text":"We use the level of significance ( \\alpha \\alpha ) to determine the critical value. How? Example: Determine the critical value at 5% level of significance. Presume its a right-tailed / upper-tailed test . We know \\alpha \\alpha = 5% (see image below to find where level of significance \\alpha \\alpha of 5% falls) and 1 - \\alpha \\alpha is 95% We know that 95% of the values fall within the 2 standard deviation mark and the corresponding the 'Z' score is 1.96 . So the critical value ( Z ) is 1.96 .","title":"Critical value and level of significance"},{"location":"statistics/hypothesis_testing_02/#summary","text":"In this article, we focussed on the 3 rd step of the hypothesis testing process - Find the critical value We started with what are tails, upper tail and lower tail, one-tailed tests and two-tailed tests, critical region / rejection region, critical values and when to accept or reject the null hypothesis. In the next post, we will explore more on finding the test statistic which is the 4 th step of the hypothesis testing process.","title":"Summary"},{"location":"statistics/hypothesis_testing_03/","text":"Test statistic - Z & t # In the previous article , we used test statistic as a value to compare against the critical value. This helps us to accept or reject our null hypothesis. In this article, we will see what is a test statistic and how to compute it. Note We are in the fourth step of the hypothesis testing process - Find the test statistic What is a test statistic? # A test statistic is used in a hypothesis test to decide to support or reject a null hypothesis. A test statistic is a number that is calculated from a sample and is compared with the null hypothesis. So what does this mean? We know that for the population we have parameters and for sample, we have statistics. i.e. any value that represents the population is called as parameter and any value that represents sample is called as a statistic We use this sample and come up with a value (statistic). This is called the test statistic. Methods of finding the test statistic # There are various methods/tests for finding the test statistic. Usually, the following four methods are used. Z test t test Chi-squared test ( \\chi^2 \\chi^2 ) F test In this article, we will cover the Z test and t test. We have these tests - how are they different? When the null hypothesis is about the mean of the population, the Z test or the t test is used. When the null hypothesis is about the variance of the population, the \\chi^2 \\chi^2 (chi-square) test and the F test is used. Let us see the Z test first. Z test # Assumptions of a Z test # Z test is used when the following conditions are met. The sample size n should be greater than 30 i.e. n > 30 n > 30 The population standard deviation should be known The variable should be continuous (remember, this is a continuous sampling distribution) Formula for Z test # The test statistic for Z test is represented as test Z The formula for finding the test statistic in a Z test is \\textrm {test}\\space Z = \\frac{\\bar{x} - \\mu} {\\sigma / \\sqrt n} \\textrm {test}\\space Z = \\frac{\\bar{x} - \\mu} {\\sigma / \\sqrt n} where \\bar{x} \\bar{x} is the sample mean, \\mu \\mu is the population mean, \\sigma \\sigma is the population standard deviation and n n is the number of samples. Example # Let's say the null and the alternate hypothesis are \\mathbf{H}_\\mathbf{0} : \\mu \\ge 1000 \\\\ \\mathbf{H}_\\mathbf{1} : \\mu < 1000 \\mathbf{H}_\\mathbf{0} : \\mu \\ge 1000 \\\\ \\mathbf{H}_\\mathbf{1} : \\mu < 1000 and the population standard deviation \\sigma \\sigma is known and a random sample of size n n = 30 is taken, then the test statistic of Z Z would be test\\space Z = \\frac{\\bar{x} - 1000}{\\sigma / \\sqrt 30} test\\space Z = \\frac{\\bar{x} - 1000}{\\sigma / \\sqrt 30} Let's check our understanding with a detailed example Let's highlight the keywords along with the problem statement An automatic bottling machine fills cola into 2-liter ( 2,000 - cubic cm) bottles. A consumer advocate wants to test the null hypothesis that the average amount filled by the machine into a bottle is at least 2,000 cubic cm . A random sample of 40 bottles coming out of the machine was selected and the exact contents of the selected bottles are recorded. The sample mean was 1,999.6 cubic cm . The population standard deviation is known from past experience to be 1.30 cubic cm. Test the null hypothesis at an \\alpha \\alpha of 5%. Let's write the values that have been provided \\mu = 2000; \\space n = 40; \\space \\bar{x} = 1999.6;\\space\\sigma=1.30; \\space \\alpha = 5% \\mu = 2000; \\space n = 40; \\space \\bar{x} = 1999.6;\\space\\sigma=1.30; \\space \\alpha = 5% Let's try to follow the hypothesis test process State the null and the alternate hypothesis Choose the level of significance Find the critical value Find the test statistic Draw the conclusion 1. Stating the null and the alternate hypothesis \\mathbf{H}_\\mathbf{0}: \\mu \\ge 2000 \\\\ \\mathbf{H}_\\mathbf{1}: \\mu < 2000 \\mathbf{H}_\\mathbf{0}: \\mu \\ge 2000 \\\\ \\mathbf{H}_\\mathbf{1}: \\mu < 2000 We see the sign of the alternate hypothesis and it is < (less than symbol) which means the test is a one-tailed lower-tail test. 2. Choose the level of significance The level of significance provided is \\alpha = 5 \\alpha = 5 %. So the confidence level is , 1 - \\alpha\\space = 95 \\alpha\\space = 95 % 3. Find the critical value The corresponding Z score (critical value) is z_c = -1.64 z_c = -1.64 Here the - (negative) sign implies that it is a lower tail test. Note The Z score for 95% is 1.64 but, since this is a lower-tailed test, we take the Z score as -1.64 The rejection region is highlighted in the graph below 4. Find the test statistic Here the sample is n = 40 n = 40 which means it confirms to the first condition of the Z Z test mentioned above Second, the population standard deviation is known - which means we can use the Z Z test to find the test statistic The formula for computing the Z Z score is \\textrm {test}\\space Z = \\frac{\\bar{x} - \\mu} {\\sigma / \\sqrt n} \\textrm {test}\\space Z = \\frac{\\bar{x} - \\mu} {\\sigma / \\sqrt n} When we substitute the values, we get \\textrm {test}\\space Z = \\frac{1999.6 - 2000} {1.30 / \\sqrt 40} \\textrm {test}\\space Z = \\frac{1999.6 - 2000} {1.30 / \\sqrt 40} which yeilds the result \\textrm {test} \\space z \\textrm {test} \\space z = -1.95 When we plot it in the graph, the graph would look like the below image The test statistic \\textrm{test} \\space z \\textrm{test} \\space z falls within the rejection range. Hence the null hypothesis will be rejected. 5. Drawing the conclusion The hypothesis that the average amount filled by the cola machine into a cola bottle is less than 2,000 cubic cm and hence, it is rejected. To sum up, the Z test is used to calculate the test statistic when the null hypothesis is about the means of the population and it satisfies the assumption mentioned above. 't' test or Student 't' test # We saw that the 'Z' test is applicable when the sample size is greater than 30 i.e. n>30 . There arises a logical question as to what test is to be done when the sample size is less than 30 i.e. n<30 . We use the 't' test for computing the test statistic for null hypothesis testing. Assumptions of a 't' test # t t test is used when the following conditions are met. The sample size n should be less than 30 i.e. n < 30 n < 30 The population standard deviation is not known irrespective of the sample size The population is normally distributed Formula for 't' test # t \\space statistic \\space (or \\space t\\space score), t= \\frac{\\bar{x} - \\mu}{s /\\sqrt n} t \\space statistic \\space (or \\space t\\space score), t= \\frac{\\bar{x} - \\mu}{s /\\sqrt n} where \\bar{x} \\bar{x} is the sample mean, \\mu \\mu is the population mean, s s is the sample standard deviation[^1] and n n is the number of samples. [^1] Since we don't know the population standard deviation in a 't' test, we use the sample standard deviation Characteristics of the 't' distribution it has degrees-of-freedom parameter df df it is symmetric and bell-shaped has wider tails than the Z distribution Extra_research Why does a t t distribution has wider tails than the Z distribution? In a t t distribution, the population standard deviation ( \\sigma \\sigma ) is not known and only the sample standard deviation ( s s ) is known. We know that by using the sample standard deviation, we cannot accurately project the population variance. Hence, to accommodate the level of uncertainity in computing the population variance, it has wider tails. As the degree of freedom ( df df ) increases, the t-distribution approaches the 'Z' distribution The 't' distribution with infinite degrees-of-freedom is called as the standard normal distribution. Confidence interval to estimate the population mean # As the population standard deviation, \\sigma \\sigma is not known in a t t , we will not able to estimate the population mean \\mu \\mu - but, we can find the range (confidence interval) of the population mean \\mu \\mu The formula for computing the confidence interval to estimate \\mu \\mu is \\bar{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\le \\mu \\le \\bar{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\bar{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\le \\mu \\le \\bar{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} where \\bar{x} \\bar{x} is the sample mean, t t is the test statistic of the t t distribution, n-1 n-1 is the degree of freedom, \\alpha \\alpha is the level of significance, S S is the sample standard deviation, n n is the number of samples. Why \\frac{\\alpha}{2} \\frac{\\alpha}{2} ? In a confidence interval, the area is symmetrically distributed between the two tails. Example # A stock market analyst wants to estimate the average return on a certain stock. A random sample of 15 days yields an average (annualized) return of \\bar{x} \\bar{x} 10.37% and a standard deviation of s = = 3.5%. Assuming a normal population of returns, give a 95% confidence interval for the average return on this stock. What we know here? Sample of 15 days i.e. s = 15 s = 15 (since sample is less than 30, t test is to be used) We need to use t distribution with n-1 n-1 degrees of freedom. So, df = 14 df = 14 . Sample standard deviation s = 3.5 s = 3.5 Sample mean \\bar{x} = 10.37 \\bar{x} = 10.37 Confidence level is 95%. So the level of significance deduced is 5%. Remember confidence level = 1 - \\alpha \\alpha which means \\alpha \\alpha = 100 - confidence level Solution We know the formula for the confidence interval of the t distribution is \\bar{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\le \\mu \\le \\bar{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\bar{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\le \\mu \\le \\bar{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} If we substitute the values, we get 10.37 - t_{14,0.025} \\frac{3.5}{\\sqrt 15} \\le \\mu \\le 10.37 + t_{14,0.025} \\frac{3.5}{\\sqrt 15} 10.37 - t_{14,0.025} \\frac{3.5}{\\sqrt 15} \\le \\mu \\le 10.37 + t_{14,0.025} \\frac{3.5}{\\sqrt 15} From the t tables, we can find the t statistic value for df = 14 df = 14 and \\alpha /2 = 0.025 \\alpha /2 = 0.025 is t_{14,0.025} = 2.145 t_{14,0.025} = 2.145 See below image for finding the t value for df = 14 df = 14 and \\alpha = 0.025 \\alpha = 0.025 Substituting in the formula, we get 10.37 - 2.145 \\frac{3.5}{\\sqrt 15} \\le \\mu \\le 10.37 + 2.145 \\frac{3.5}{\\sqrt 15} 10.37 - 2.145 \\frac{3.5}{\\sqrt 15} \\le \\mu \\le 10.37 + 2.145 \\frac{3.5}{\\sqrt 15} the confidence interval range is [8.43, 12.31] [8.43, 12.31] . This is represented as CI(0.05) = [8.43, 12.31] CI(0.05) = [8.43, 12.31] Thus, the analyst may be 95% sure that the average annualized return on the stock is anywhere from 8.43% to 12.31%. Degree of Freedom # Degrees of freedom refers to the maximum number of logically independent values in a data sample which have the freedom to vary within. What does that mean? We will take an example and try to understand. Example Let us consider that we have a sample of 3 values {5, x , 15} {5, x , 15} and the mean of all the values is 10. Note x x is unknown here It is easy to deduce that the value of x x would be 10 as the mean of these 3 values has to equate to 10. But, let's say 2 values from this sample are not known, {5, x, y} {5, x, y} Note ( x, y x, y are unknown) with the same mean 10, then we cannot be sure about the exact values of x x and y y . It could be any values of these values - (10, 15), (15, 10), (5, 20), (20, 5) or even (1, 24). So we cannot determine the exact value of these variables x x and y y . These 2 values have the freedom to vary. So, the degree of freedom ( df df ) of this sample data of size 3 is 2. Formula df = n \u2013 1 df = n \u2013 1 where df df is the degree of freedom and n n is the sample size Application of t-test # Your company wants to improve sales. Past sales data indicate that the average sale was $100 per transaction. After training your sales force, recent sales data (taken from a sample of 25 salesmen) indicates an average sale of $130, with a standard deviation of $15. Did the training work? Test your hypothesis at a 5% alpha level. What is given? From the problem, first, we need to understand if the given problem belongs to which category of tests - Z or t. The average sale is provided \\mu = 100 \\mu = 100 . Sample is n = 25 n = 25 . Recent sales data (sample) is given - ie. average sale \\bar{x} = 130 \\bar{x} = 130 . Sample standard deviation s = 15 s = 15 . Level of significance = 5% or 0.05 Since the sample size is less than 30 and we don't know the population standard deviation, we will go with the t test Solution Approach Did the training work? In other words, is there an increase in sales after the training is provided. Hypothesis testing steps We know the hypothesis testing steps State the null and the alternate hypothesis \\mathbf{H}_\\mathbf{0}: \\mu = 100 \\\\ \\mathbf{H}_\\mathbf{1}: \\mu > 100 \\mathbf{H}_\\mathbf{0}: \\mu = 100 \\\\ \\mathbf{H}_\\mathbf{1}: \\mu > 100 Since the symbol of the alternate hypothesis is greater than, it is a right-tailed test. Find the level of significance. Here the level of significance is provided - which is \\alpha = 0.05 \\alpha = 0.05 Find the critical value Since this is a t test, we need to use the t-table to find the critical value. The t-table has the degree of freedom and corresponding level of significance to provide the critical value. Here the degree of freedom df = n - 1 df = n - 1 which is 25 - 1 = 24 25 - 1 = 24 Looking into the critical table for df = 24 df = 24 and \\alpha = 0.05 \\alpha = 0.05 , we get the critical value t_c = 1.711 t_c = 1.711 (see figure below) Find the test statistic We know the formula for the t test is t_{score} = \\frac{\\bar{x} - \\mu}{s / \\sqrt n } t_{score} = \\frac{\\bar{x} - \\mu}{s / \\sqrt n } Substituting the above values, we get t_{score} = \\frac{130 - 100}{15 / \\sqrt 25} t_{score} = \\frac{130 - 100}{15 / \\sqrt 25} which gives us the t_{score} = 10 t_{score} = 10 Drawing the conclusions We see that the t_{score} t_{score} falls beyond the rejection region and hence the null hypothesis is rejected. This means to say that indeed the average sales increased after the training. Summary # In this article, we saw what a test statistic is!, the different types of tests, when to use Z test and t test and their respective formulas. In the end, we saw what is the degree of freedom along with an application of t-test using an example. We also got more familiar with the hypothesis testing process and following it would help us accept or reject the null hypothesis in a logical manner. Next post Test statistic - chi-square and F","title":"Test statistic - Z & t"},{"location":"statistics/hypothesis_testing_03/#test_statistic_-_z_t","text":"In the previous article , we used test statistic as a value to compare against the critical value. This helps us to accept or reject our null hypothesis. In this article, we will see what is a test statistic and how to compute it. Note We are in the fourth step of the hypothesis testing process - Find the test statistic","title":"Test statistic - Z &amp; t"},{"location":"statistics/hypothesis_testing_03/#what_is_a_test_statistic","text":"A test statistic is used in a hypothesis test to decide to support or reject a null hypothesis. A test statistic is a number that is calculated from a sample and is compared with the null hypothesis. So what does this mean? We know that for the population we have parameters and for sample, we have statistics. i.e. any value that represents the population is called as parameter and any value that represents sample is called as a statistic We use this sample and come up with a value (statistic). This is called the test statistic.","title":"What is a test statistic?"},{"location":"statistics/hypothesis_testing_03/#methods_of_finding_the_test_statistic","text":"There are various methods/tests for finding the test statistic. Usually, the following four methods are used. Z test t test Chi-squared test ( \\chi^2 \\chi^2 ) F test In this article, we will cover the Z test and t test. We have these tests - how are they different? When the null hypothesis is about the mean of the population, the Z test or the t test is used. When the null hypothesis is about the variance of the population, the \\chi^2 \\chi^2 (chi-square) test and the F test is used. Let us see the Z test first.","title":"Methods of finding the test statistic"},{"location":"statistics/hypothesis_testing_03/#z_test","text":"","title":"Z test"},{"location":"statistics/hypothesis_testing_03/#assumptions_of_a_z_test","text":"Z test is used when the following conditions are met. The sample size n should be greater than 30 i.e. n > 30 n > 30 The population standard deviation should be known The variable should be continuous (remember, this is a continuous sampling distribution)","title":"Assumptions of a Z test"},{"location":"statistics/hypothesis_testing_03/#formula_for_z_test","text":"The test statistic for Z test is represented as test Z The formula for finding the test statistic in a Z test is \\textrm {test}\\space Z = \\frac{\\bar{x} - \\mu} {\\sigma / \\sqrt n} \\textrm {test}\\space Z = \\frac{\\bar{x} - \\mu} {\\sigma / \\sqrt n} where \\bar{x} \\bar{x} is the sample mean, \\mu \\mu is the population mean, \\sigma \\sigma is the population standard deviation and n n is the number of samples.","title":"Formula for Z test"},{"location":"statistics/hypothesis_testing_03/#example","text":"Let's say the null and the alternate hypothesis are \\mathbf{H}_\\mathbf{0} : \\mu \\ge 1000 \\\\ \\mathbf{H}_\\mathbf{1} : \\mu < 1000 \\mathbf{H}_\\mathbf{0} : \\mu \\ge 1000 \\\\ \\mathbf{H}_\\mathbf{1} : \\mu < 1000 and the population standard deviation \\sigma \\sigma is known and a random sample of size n n = 30 is taken, then the test statistic of Z Z would be test\\space Z = \\frac{\\bar{x} - 1000}{\\sigma / \\sqrt 30} test\\space Z = \\frac{\\bar{x} - 1000}{\\sigma / \\sqrt 30}","title":"Example"},{"location":"statistics/hypothesis_testing_03/#t_test_or_student_t_test","text":"We saw that the 'Z' test is applicable when the sample size is greater than 30 i.e. n>30 . There arises a logical question as to what test is to be done when the sample size is less than 30 i.e. n<30 . We use the 't' test for computing the test statistic for null hypothesis testing.","title":"'t' test or Student 't' test"},{"location":"statistics/hypothesis_testing_03/#assumptions_of_a_t_test","text":"t t test is used when the following conditions are met. The sample size n should be less than 30 i.e. n < 30 n < 30 The population standard deviation is not known irrespective of the sample size The population is normally distributed","title":"Assumptions of a 't' test"},{"location":"statistics/hypothesis_testing_03/#formula_for_t_test","text":"t \\space statistic \\space (or \\space t\\space score), t= \\frac{\\bar{x} - \\mu}{s /\\sqrt n} t \\space statistic \\space (or \\space t\\space score), t= \\frac{\\bar{x} - \\mu}{s /\\sqrt n} where \\bar{x} \\bar{x} is the sample mean, \\mu \\mu is the population mean, s s is the sample standard deviation[^1] and n n is the number of samples. [^1] Since we don't know the population standard deviation in a 't' test, we use the sample standard deviation","title":"Formula for 't' test"},{"location":"statistics/hypothesis_testing_03/#confidence_interval_to_estimate_the_population_mean","text":"As the population standard deviation, \\sigma \\sigma is not known in a t t , we will not able to estimate the population mean \\mu \\mu - but, we can find the range (confidence interval) of the population mean \\mu \\mu The formula for computing the confidence interval to estimate \\mu \\mu is \\bar{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\le \\mu \\le \\bar{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\bar{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\le \\mu \\le \\bar{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} where \\bar{x} \\bar{x} is the sample mean, t t is the test statistic of the t t distribution, n-1 n-1 is the degree of freedom, \\alpha \\alpha is the level of significance, S S is the sample standard deviation, n n is the number of samples.","title":"Confidence interval to estimate the population mean"},{"location":"statistics/hypothesis_testing_03/#example_1","text":"A stock market analyst wants to estimate the average return on a certain stock. A random sample of 15 days yields an average (annualized) return of \\bar{x} \\bar{x} 10.37% and a standard deviation of s = = 3.5%. Assuming a normal population of returns, give a 95% confidence interval for the average return on this stock.","title":"Example"},{"location":"statistics/hypothesis_testing_03/#degree_of_freedom","text":"Degrees of freedom refers to the maximum number of logically independent values in a data sample which have the freedom to vary within. What does that mean? We will take an example and try to understand.","title":"Degree of Freedom"},{"location":"statistics/hypothesis_testing_03/#application_of_t-test","text":"Your company wants to improve sales. Past sales data indicate that the average sale was $100 per transaction. After training your sales force, recent sales data (taken from a sample of 25 salesmen) indicates an average sale of $130, with a standard deviation of $15. Did the training work? Test your hypothesis at a 5% alpha level.","title":"Application of t-test"},{"location":"statistics/hypothesis_testing_03/#summary","text":"In this article, we saw what a test statistic is!, the different types of tests, when to use Z test and t test and their respective formulas. In the end, we saw what is the degree of freedom along with an application of t-test using an example. We also got more familiar with the hypothesis testing process and following it would help us accept or reject the null hypothesis in a logical manner.","title":"Summary"},{"location":"statistics/hypothesis_testing_04/","text":"Test statistic - \\chi^2 \\chi^2 & F # In the previous article , we saw what a Z test and t-test are! Z test and t-test are used when the hypothesis test is about the means of the population. In this article, let us see two tests - the chi-squared ( \\chi^2 \\chi^2 ) and F test which tests the hypothesis about the variance of the population. Recall The sample estimate of the population variance is given by $$ s^2 = \\frac{\\sum(x_i-\\bar{x})^2}{n-1} $$ where s s is the sample variance, \\bar{x} \\bar{x} is the sample mean, and n n is the number of samples. Chi-squared statistic # \\chi^2 \\chi^2 is used to test the hypothesis about a single population variance. Formula # The formula for computing \\chi ^2 \\chi ^2 is \\chi ^2 = \\frac{(n-1)s ^2}{\\sigma ^2} \\chi ^2 = \\frac{(n-1)s ^2}{\\sigma ^2} where df df is the degree of freedom, s s is the sample standard deviation and \\sigma \\sigma is the population standard deviation and n n is the number of samples. Interpretation of \\chi ^2 \\chi ^2 If the chi-square value is more than the critical value, then there is a significant difference in the variance between the sample and the population. Tip The Chi-square statistic can only be used on numbers . They can\u2019t be used for percentages, proportions, means or similar statistical value. For example, if you have 20 percent of 100 people, you would need to convert that to a number (20) before you can run a test statistic. Application of Chi-square # A manufacturing company produces bearings of 2.65 cm in diameter. A major customer requires that the variance in diameter be no more than 0.001 cm ^2 cm ^2 . The manufacturer tests 20 bearings using a precise instrument and gets the below values. Assuming the diameters are normally distributed, can the population of these bearings be rejected due to high variance at 1% significance level. Data 2.69, 2.66, 2.64, 2.59, 2.62, 2.63, 2.69, 2.66, 2.63, 2.65, 2.57, 2.63, 2.70, 2.71, 2.64, 2.65, 2.59, 2.66, 2.62, 2.57 Solution approach # The problem talks about a single population variance. Hence, a \\chi ^2 \\chi ^2 test can be used. Known parameters sample \\space size \\space n = 20 sample \\space size \\space n = 20 level \\space of \\space significance \\space \\alpha = 1\\% \\space or \\space 0.01 level \\space of \\space significance \\space \\alpha = 1\\% \\space or \\space 0.01 Degree \\space of \\space freedom \\space = n - 1 \\space which \\space is \\space 20 - 1 \\space = 19 Degree \\space of \\space freedom \\space = n - 1 \\space which \\space is \\space 20 - 1 \\space = 19 population \\space variance = \\sigma ^2 = 0.001 population \\space variance = \\sigma ^2 = 0.001 Let's follow the hypothesis testing process State the null and the alternate hypothesis. The variance in diameter to be no more than 0.001 cm ^2 cm ^2 . So the null hypothesis is \\mathbf{H}_\\mathbf{0}: diameter \\le 0.001 \\mathbf{H}_\\mathbf{0}: diameter \\le 0.001 and hence the alternate hypothesis is \\mathbf{H}_\\mathbf{1} : diameter > 0.001 \\mathbf{H}_\\mathbf{1} : diameter > 0.001 Since the alternate hypothesis has the greater than symbol > > , it is a chi-square right-tailed test. Find the level of significance The level of significance is already provided \\alpha = 0.05 \\alpha = 0.05 Tip If the level of significance is not provided, take the default \\alpha = 0.05 \\alpha = 0.05 Find the critical value The critical value is found out in the chi-squared table for df = 19 df = 19 and \\alpha = 0.01 \\alpha = 0.01 . The critical value is 36.191. i.e \\chi ^2_c = 36.191 \\chi ^2_c = 36.191 Find the test statistic To find the test statistic, we will use the formula. \\chi ^2 = \\frac{(n-1)s ^2}{\\sigma ^2} \\chi ^2 = \\frac{(n-1)s ^2}{\\sigma ^2} We don't know the sample variance s ^2 s ^2 but, it can be computed by using the data provided. Copy the data in excel and use the formula VAR.S and get the value which in turn is 0.001621 0.001621 . Substituting the values, we get \\chi ^2 = \\frac{19 *0.001621}{0.001} \\chi ^2 = \\frac{19 *0.001621}{0.001} which gives \\chi ^2 = 30.8 \\chi ^2 = 30.8 Draw the conclusion - to accept/reject the null hypothesis Since the \\chi ^2 < \\chi^2_{critical} \\chi ^2 < \\chi^2_{critical} i.e. 30.8 < 36.191 30.8 < 36.191 , we accept the null hypothesis. The bearings produced are within the specified limits required by the customer. To conclude, a \\chi ^2 \\chi ^2 test (chi-squared) is used to test the hypothesis about a single population variance. F distribution # \\chi ^2 \\chi ^2 is useful when testing hypothesis about a single population. What if we want to test the hypothesis about the difference in variances of two populations? Example Do parts manufactured on 2 different machines have the same variance or not? Formula # Since F-test is a comparison of variances of two different populations using samples collected from each population, we can say that it is the ratio of two sample variances i.e. F = \\frac{s_1 ^2}{s_2 ^2} = \\frac{est.\\sigma ^2_1}{est.\\sigma ^2_2} F = \\frac{s_1 ^2}{s_2 ^2} = \\frac{est.\\sigma ^2_1}{est.\\sigma ^2_2} What does this formula mean? We know that s_1 s_1 is the standard deviation of sample 1 and s_2 s_2 is the standard deviation of sample 2. Since the F test is a comparison between two variances , we need to square the standard deviation. (Remember: variance = standard deviation ^2 ^2 ) Interpretation Ideally, this F ratio should be about 1 if the 2 samples come from the same population or the 2 samples come from a different population with the same variance So if we compute the F ratio and see if the value is near to 1, it means two samples have the same variance thereby the population variance is also the same. Bigger the F ratio - bigger the variance (or the two population is not related to each other) Important Since the F test/distribution deals with two samples, there will be two degrees of freedom - one for sample 1 and one for sample 2. Facts The curve is not symmetrical but skewed to the right. There is a different curve for each set of df . The F statistic is greater than or equal to zero. As the degrees of freedom for the numerator and the denominator get larger, the curve approximates the normal. Application of the 'F' test # A machine produces metal sheets with 22mm thickness. There is a variability in thickness due to machines, operators, manufacturing environment, raw material, etc. The company wants to know the consistency of two machines and randomly samples 10 sheets from machine 1 and 12 sheets from machine 2. Thickness measurements are taken. Assume sheet thickness is normally distributed in the population. The company wants to know if the variance for each sample comes from the same population variance (i.e. population variances are equal) or from different population variances (population variances are unequal). Data provided Machine 1 Machine 2 22.3 22.0 21.8 22.1 22.3 21.8 21.6 21.9 21.8 22.2 21.9 22.0 22.4 21.7 22.5 21.9 22.2 22.0 21.6 22.1 21.9 22.1 Solution Approach # Understanding what type of test it is Here the problem statement is about knowing the consistency of two machines in terms of variability . For variances test, we have two tests - \\chi ^2 \\chi ^2 and F test. But since we have to compare two variances, we have to go with the F test. Known parameters Samples \\space from \\space machine \\space one \\space n_1 = 10 \\\\ Samples \\space from \\space machine \\space two \\space n_2 = 12 Samples \\space from \\space machine \\space one \\space n_1 = 10 \\\\ Samples \\space from \\space machine \\space two \\space n_2 = 12 We have the data for machine 1 and machine 2 - from which we can find the variance - from excel Compute the rest of the parameters We compute the variance and the count of samples. The degree of freedom for machine 1 is 9 ( df_1 = n_1 - 1 df_1 = n_1 - 1 ). The degree of freedom for machine 1 is 11 ( df_2 = n_2 - 1 df_2 = n_2 - 1 ). Following the hypothesis testing process There are 5 steps to the hypothesis testing process. Let's follow that one by one. State the null and the alternate hypothesis Since this problem talks about variance in the two machines, the null hypothesis will be that there is no variance. \\mathbf{H}_\\mathbf{0}: \\sigma_1 ^2 = \\sigma_2 ^2 \\mathbf{H}_\\mathbf{0}: \\sigma_1 ^2 = \\sigma_2 ^2 The alternate hypothesis will be \\mathbf{H}_\\mathbf{0}: \\sigma_1 ^2 \\ne \\sigma_2 ^2 \\mathbf{H}_\\mathbf{0}: \\sigma_1 ^2 \\ne \\sigma_2 ^2 So based on the symbol of the alternate hypothesis which is \\ne \\ne , we conclude that this is a two-tailed test. Tip If there is any difficulty in stating the null hypothesis, start with the alternate hypothesis and then draft the null hypothesis. Find the level of significance The level of significance \\alpha \\alpha is not given and hence a \\alpha \\alpha of 5% or 0.05 is presumed. Since this is a two-tailed test, we have to divide \\alpha \\alpha by 2 which gives the value as 0.025. Find the critical value Similar to tables for other tests, F tests also have a corresponding table called the F-table. The F-table has a degree of freedom one ( df_1 df_1 ) on the 'x' axis and degree of freedom two ( df_2 df_2 ) on the 'y' axis. Let's see the F table for df_1 = 9 df_1 = 9 and df_2 = 11 df_2 = 11 to find the critical value for F test. The critical value is F_{0.025,9,11} = 3.5879 F_{0.025,9,11} = 3.5879 . This number is for the right-tail. Remember, we have a two-tails for this hypothesis testing. We also need to compute the left-tail critical value which can be either computed like the F-table above with a level of significance \\alpha = 0.975 (1-0.025 = 0.975) \\alpha = 0.975 (1-0.025 = 0.975) and the df_1 = 9 df_1 = 9 and df_2 = 11 df_2 = 11 . The other method of doing this is to divide 1 by F_{0.025,9,11} F_{0.025,9,11} F_{0.975,9,11} = \\frac{1}{F_{0.025,9,11}} F_{0.975,9,11} = \\frac{1}{F_{0.025,9,11}} F_{0.975,9,11} = \\frac{1}{3.5879} = 0.2787 F_{0.975,9,11} = \\frac{1}{3.5879} = 0.2787 Find the test statistic The test statistic is found by the F distribution formula - which is F = \\frac{s_1 ^2}{s_2 ^2} F = \\frac{s_1 ^2}{s_2 ^2} So the F score is 5.62 $$ F_{score} = \\frac{0.11378}{0.02023} = 5.62 $$ Draw the conclusion From the above steps, we know that the F_{critical} = 3.5879 F_{critical} = 3.5879 and the F_{score} = 5.62 F_{score} = 5.62 Since F_{score} F_{score} > F_{critical} F_{critical} or in other words, falls into the rejection region, we reject the null hypothesis. That is, the variance in Machine 1 and Machine 2 are not equal. Machine 1 has a higher variance and hence needs to be inspected for issues. The graph below illustrates the F score and the F critical value and why the null hypothesis is rejected. ANOVA # ANOVA - Analysis of Variance ANOVA is used to analyze the means of the samples. In the \\chi^2 \\chi^2 test, we saw how to compare variance within a single population and in the F test, we saw how to compare the variance between two samples of a single population / two population. What if there are three or more samples? Can we find if they are from the same population? ANOVA test is used for this purpose. When performing ANOVA test, we try to determine if the difference between the averages reflects a real difference between the groups, or is due to the random noise inside each group. The groups here mean samples - say out of a Population p, we are taking three groups of samples - n_1, n_2, n_3 n_1, n_2, n_3 . Assumptions in the ANOVA test # The samples taken are independent; (taking samples in one group does not affect the probability of the samples taken in other groups) The population must be normally distributed. Null hypothesis and computation # Since we are comparing three groups or more, the null hypothesis of ANOVA would look like below \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C We compute the F value and compare it with the critical value determined by the degrees of freedom. Here, the degrees of freedom are to be calculated for the groups and the number of items in each group. Example # Let's say we have three groups - A, B and C which have the below samples picked. Group A Group B Group C 37 62 50 60 27 63 52 69 58 43 64 54 40 43 49 52 54 52 55 44 53 39 31 43 39 49 65 23 57 43 These three groups have an equal number of samples. The degree of freedom of the groups = df_g = 3 - 1 = 2 df_g = 3 - 1 = 2 . The degree of freedom for the samples within each group is df_s = 9 + 9 + 9 = 27 df_s = 9 + 9 + 9 = 27 i.e. n-1 n-1 for each group which is 10 - 1 = 9 10 - 1 = 9 Let us calculate the mean of each of the groups and the total mean (which is total of all the means of the three groups divided by 3) Group A Group B Group C 37 62 50 60 27 63 52 69 58 43 64 54 40 43 49 52 54 52 55 44 53 39 31 43 39 49 65 23 57 43 Mean 44 50 53 Mean Total 49 Important ANOVA considers two types of variance Between groups How far each group mean vary from the total mean (i.e. in this case, how 44, 50, 53 44, 50, 53 vary from the total mean 49 49 Within groups How far individual values vary from their respective group mean. Note We will compute the F score both manually as well as using excel. If you want to move on with excel computation only, you may skip the below section and can proceed with computation with excel (using data-analysis addin) only. Formula # We compute F value for the groups which is the ratio between the two variances - i.e. variance between groups and variance within groups F = \\frac{Variance\\space Between\\space Groups}{Variance\\space Within\\space Groups} F = \\frac{Variance\\space Between\\space Groups}{Variance\\space Within\\space Groups} Recall We know that the formula for variance is $$ s ^2 = \\frac{\\sum(x - \\bar{x}) ^2}{n -1} = \\frac{SS}{df} $$ where \\sum(x - \\bar{x}) ^2 \\sum(x - \\bar{x}) ^2 is the sum of squares SS SS and the n -1 n -1 is the degree of freedom . So the formula for the f value becomes F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} where SSG SSG = Sum of squares groups, df_{groups} = df_{groups} = degrees of freedom (groups) and SSE SSE = Sum of squares error and df_{error} df_{error} = degrees of freedom (error) Solution approach # As with any hypothesis testing, let us perform the calculation using the hypothesis testing steps. State the null and the alternate hypothesis For ANOVA, the hypothesis will always be - the means across different groups will be equal. In this case, the means of the groups will be equal is the null hypothesis. \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C The alternate hypothesis will be \\mathbf{H}_\\mathbf{1}: \\mu_A \\ne \\mu_B \\ne \\mu_C \\mathbf{H}_\\mathbf{1}: \\mu_A \\ne \\mu_B \\ne \\mu_C Important In ANOVA, there will be NO two-tailed test. ANOVA will ALWAYS be one-tailed and that will be upper-tailed only . Why? We know that in the formula, we are dividing the sum of squares between groups and within groups which would always yield a positive value and hence it will always be upper tailed . Find the level of significance Here level of significance is not provided; we will take the default \\alpha = 0.05 \\alpha = 0.05 Find the critical value We know that the two degrees of freedom - for the groups and within the groups are 2 and 27 respectively. So, df_1 = 2 df_1 = 2 and df_2 = 27 df_2 = 27 Looking into the F table (for ANOVA) at 0.05 significance level, we get the F_{critical} F_{critical} value as 3.35 3.35 (see image below) Find the test statistic To compute it manually, we will be using excel. The following screenshot shows how it is done. We know the formula is F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} Computing the sum of squares within groups and between groups manually. substituting in the formula, we get F_{test} = \\frac{\\frac{420}{2}}{\\frac{3300}{27}} = 1.718 F_{test} = \\frac{\\frac{420}{2}}{\\frac{3300}{27}} = 1.718 Computing the sum of squares within groups and between groups via excel data-analysis addin. Enter the data in excel Go to Data -> Data Analysis and select \"Anova - Single factor\" and click \"OK\" Give the input range, check \"labels in first row\" as we have the group headers in the first row and input the level of significance. Click \"OK\" We get the analysis in a new sheet. Draw your conclusion Here we see that the F_{score} F_{score} is less than the F_{critical} F_{critical} ( 1.718 < 3.354 1.718 < 3.354 ) and hence we will accept the null hypothesis - which means that there is no difference between the means of any group. Summary # To summarize, we have seen two tests - the chi-square and the F test to test the variances in the population. We have also seen ANOVA - which is used to test the hypothesis when more than two groups of samples are picked from the population.","title":"Test statistic - Chi-square & F"},{"location":"statistics/hypothesis_testing_04/#test_statistic_-_chi2chi2_f","text":"In the previous article , we saw what a Z test and t-test are! Z test and t-test are used when the hypothesis test is about the means of the population. In this article, let us see two tests - the chi-squared ( \\chi^2 \\chi^2 ) and F test which tests the hypothesis about the variance of the population. Recall The sample estimate of the population variance is given by $$ s^2 = \\frac{\\sum(x_i-\\bar{x})^2}{n-1} $$ where s s is the sample variance, \\bar{x} \\bar{x} is the sample mean, and n n is the number of samples.","title":"Test statistic - \\chi^2\\chi^2 &amp; F"},{"location":"statistics/hypothesis_testing_04/#chi-squared_statistic","text":"\\chi^2 \\chi^2 is used to test the hypothesis about a single population variance.","title":"Chi-squared statistic"},{"location":"statistics/hypothesis_testing_04/#formula","text":"The formula for computing \\chi ^2 \\chi ^2 is \\chi ^2 = \\frac{(n-1)s ^2}{\\sigma ^2} \\chi ^2 = \\frac{(n-1)s ^2}{\\sigma ^2} where df df is the degree of freedom, s s is the sample standard deviation and \\sigma \\sigma is the population standard deviation and n n is the number of samples.","title":"Formula"},{"location":"statistics/hypothesis_testing_04/#application_of_chi-square","text":"A manufacturing company produces bearings of 2.65 cm in diameter. A major customer requires that the variance in diameter be no more than 0.001 cm ^2 cm ^2 . The manufacturer tests 20 bearings using a precise instrument and gets the below values. Assuming the diameters are normally distributed, can the population of these bearings be rejected due to high variance at 1% significance level.","title":"Application of Chi-square"},{"location":"statistics/hypothesis_testing_04/#solution_approach","text":"The problem talks about a single population variance. Hence, a \\chi ^2 \\chi ^2 test can be used.","title":"Solution approach"},{"location":"statistics/hypothesis_testing_04/#f_distribution","text":"\\chi ^2 \\chi ^2 is useful when testing hypothesis about a single population. What if we want to test the hypothesis about the difference in variances of two populations? Example Do parts manufactured on 2 different machines have the same variance or not?","title":"F distribution"},{"location":"statistics/hypothesis_testing_04/#formula_1","text":"Since F-test is a comparison of variances of two different populations using samples collected from each population, we can say that it is the ratio of two sample variances i.e. F = \\frac{s_1 ^2}{s_2 ^2} = \\frac{est.\\sigma ^2_1}{est.\\sigma ^2_2} F = \\frac{s_1 ^2}{s_2 ^2} = \\frac{est.\\sigma ^2_1}{est.\\sigma ^2_2} What does this formula mean? We know that s_1 s_1 is the standard deviation of sample 1 and s_2 s_2 is the standard deviation of sample 2. Since the F test is a comparison between two variances , we need to square the standard deviation. (Remember: variance = standard deviation ^2 ^2 )","title":"Formula"},{"location":"statistics/hypothesis_testing_04/#application_of_the_f_test","text":"A machine produces metal sheets with 22mm thickness. There is a variability in thickness due to machines, operators, manufacturing environment, raw material, etc. The company wants to know the consistency of two machines and randomly samples 10 sheets from machine 1 and 12 sheets from machine 2. Thickness measurements are taken. Assume sheet thickness is normally distributed in the population. The company wants to know if the variance for each sample comes from the same population variance (i.e. population variances are equal) or from different population variances (population variances are unequal).","title":"Application of the 'F' test"},{"location":"statistics/hypothesis_testing_04/#solution_approach_1","text":"","title":"Solution Approach"},{"location":"statistics/hypothesis_testing_04/#anova","text":"ANOVA - Analysis of Variance ANOVA is used to analyze the means of the samples. In the \\chi^2 \\chi^2 test, we saw how to compare variance within a single population and in the F test, we saw how to compare the variance between two samples of a single population / two population. What if there are three or more samples? Can we find if they are from the same population? ANOVA test is used for this purpose. When performing ANOVA test, we try to determine if the difference between the averages reflects a real difference between the groups, or is due to the random noise inside each group. The groups here mean samples - say out of a Population p, we are taking three groups of samples - n_1, n_2, n_3 n_1, n_2, n_3 .","title":"ANOVA"},{"location":"statistics/hypothesis_testing_04/#assumptions_in_the_anova_test","text":"The samples taken are independent; (taking samples in one group does not affect the probability of the samples taken in other groups) The population must be normally distributed.","title":"Assumptions in the ANOVA test"},{"location":"statistics/hypothesis_testing_04/#null_hypothesis_and_computation","text":"Since we are comparing three groups or more, the null hypothesis of ANOVA would look like below \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C We compute the F value and compare it with the critical value determined by the degrees of freedom. Here, the degrees of freedom are to be calculated for the groups and the number of items in each group.","title":"Null hypothesis and computation"},{"location":"statistics/hypothesis_testing_04/#example","text":"Let's say we have three groups - A, B and C which have the below samples picked. Group A Group B Group C 37 62 50 60 27 63 52 69 58 43 64 54 40 43 49 52 54 52 55 44 53 39 31 43 39 49 65 23 57 43 These three groups have an equal number of samples. The degree of freedom of the groups = df_g = 3 - 1 = 2 df_g = 3 - 1 = 2 . The degree of freedom for the samples within each group is df_s = 9 + 9 + 9 = 27 df_s = 9 + 9 + 9 = 27 i.e. n-1 n-1 for each group which is 10 - 1 = 9 10 - 1 = 9 Let us calculate the mean of each of the groups and the total mean (which is total of all the means of the three groups divided by 3) Group A Group B Group C 37 62 50 60 27 63 52 69 58 43 64 54 40 43 49 52 54 52 55 44 53 39 31 43 39 49 65 23 57 43 Mean 44 50 53 Mean Total 49 Important ANOVA considers two types of variance Between groups How far each group mean vary from the total mean (i.e. in this case, how 44, 50, 53 44, 50, 53 vary from the total mean 49 49 Within groups How far individual values vary from their respective group mean. Note We will compute the F score both manually as well as using excel. If you want to move on with excel computation only, you may skip the below section and can proceed with computation with excel (using data-analysis addin) only.","title":"Example"},{"location":"statistics/hypothesis_testing_04/#formula_2","text":"We compute F value for the groups which is the ratio between the two variances - i.e. variance between groups and variance within groups F = \\frac{Variance\\space Between\\space Groups}{Variance\\space Within\\space Groups} F = \\frac{Variance\\space Between\\space Groups}{Variance\\space Within\\space Groups} Recall We know that the formula for variance is $$ s ^2 = \\frac{\\sum(x - \\bar{x}) ^2}{n -1} = \\frac{SS}{df} $$ where \\sum(x - \\bar{x}) ^2 \\sum(x - \\bar{x}) ^2 is the sum of squares SS SS and the n -1 n -1 is the degree of freedom . So the formula for the f value becomes F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} where SSG SSG = Sum of squares groups, df_{groups} = df_{groups} = degrees of freedom (groups) and SSE SSE = Sum of squares error and df_{error} df_{error} = degrees of freedom (error)","title":"Formula"},{"location":"statistics/hypothesis_testing_04/#solution_approach_2","text":"As with any hypothesis testing, let us perform the calculation using the hypothesis testing steps. State the null and the alternate hypothesis For ANOVA, the hypothesis will always be - the means across different groups will be equal. In this case, the means of the groups will be equal is the null hypothesis. \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C The alternate hypothesis will be \\mathbf{H}_\\mathbf{1}: \\mu_A \\ne \\mu_B \\ne \\mu_C \\mathbf{H}_\\mathbf{1}: \\mu_A \\ne \\mu_B \\ne \\mu_C Important In ANOVA, there will be NO two-tailed test. ANOVA will ALWAYS be one-tailed and that will be upper-tailed only . Why? We know that in the formula, we are dividing the sum of squares between groups and within groups which would always yield a positive value and hence it will always be upper tailed . Find the level of significance Here level of significance is not provided; we will take the default \\alpha = 0.05 \\alpha = 0.05 Find the critical value We know that the two degrees of freedom - for the groups and within the groups are 2 and 27 respectively. So, df_1 = 2 df_1 = 2 and df_2 = 27 df_2 = 27 Looking into the F table (for ANOVA) at 0.05 significance level, we get the F_{critical} F_{critical} value as 3.35 3.35 (see image below) Find the test statistic To compute it manually, we will be using excel. The following screenshot shows how it is done. We know the formula is F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}}","title":"Solution approach"},{"location":"statistics/hypothesis_testing_04/#summary","text":"To summarize, we have seen two tests - the chi-square and the F test to test the variances in the population. We have also seen ANOVA - which is used to test the hypothesis when more than two groups of samples are picked from the population.","title":"Summary"}]}