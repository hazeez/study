{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Study Notes Blog # The aim of this blog is to document all the notes, exercises and research in Data Science In case you find any errors, please let me know in the respective comments section. Topic Notes Link Exercises Link Statistics Statistics Notes Statistics Exercises Probability Probability Notes Python Python Notes Python Exercises","title":"Home"},{"location":"#study_notes_blog","text":"The aim of this blog is to document all the notes, exercises and research in Data Science In case you find any errors, please let me know in the respective comments section. Topic Notes Link Exercises Link Statistics Statistics Notes Statistics Exercises Probability Probability Notes Python Python Notes Python Exercises","title":"Study Notes Blog"},{"location":"blog/","text":"Blog Index #","title":"_Blog Home"},{"location":"blog/#blog_index","text":"","title":"Blog Index"},{"location":"blog/covid19-dataset-analysis/","text":"Analysis of COVID-19 dataset # A personal account of my experience - the thought process, challenges and solution approach is documented in this post. The idea is to express that data analysis / science can be tricky and not everyone gets it right the first time. It takes multiple iterations to come up with something decent while navigating the challenges faced. Objective # The objective of this exercise is to analyze the COVID-19 cases and report the findings. The data for the same can be downloaded from Tableau Website . The data source has approximately 114,000 rows of COVID-19 cases all over the world. The following data is provided - the date, country, region, the case status (active, confirmed, recovered, deaths) information and the location details. Note This is a point in time exercise. The dataset is of Mar 21, 2020 and the analysis is based on this data. Any interpretations are based on this date and might not reflect the current trend. Tool to be used # The data will be analyzed via Microsoft Excel and insights will be provided. Why Excel? Because, at this point, I knew only Excel (40% I presume). But, I thought that it was enough to do this exercise. Actually speaking, the choice of the tool doesn't matter. What matters is the analysis! I took stock of what all Excel stuff I knew and whether it would be sufficient to do my analysis!? I didn't know yet. The following features of Excel may be used to make analysis easier was my guess. pivot table basic functions graphs In the retrospective, I was grossly wrong on my Excel skills and I had to do some learning along the way to achieve what I wanted. Metrics # At the start of the analysis, I was not having a definitive list of what metrics I would be reporting on - as I was just having a raw dataset. The possible things that came up rationally is to report on certain key metrics that are easy to report like Top 10 countries with most confirmed cases Top 10 countries with most deaths Top 10 countries with most recovered cases Time series analysis of the growth pattern across countries Geological location of cases concentrated Time series data is the complex one here. Why complex? Anything with dates' - my understanding is that it can be complex. Geological location - well, I don't have much experience in plotting data on maps. But, apart from that, the rest is easy to report. Will the analysis be interesting? Will I capture the essence of the dataset? Not sure yet! My understanding is that data analysis does not have a definitive ending. What metrics to report on depends on the amount of time we have at hand and what metrics the business will be interested in. If we think that way, the top three in the above list does not add much business value plus point 5 is just cosmetic. Time series analysis is the one the business will be much interested in as they can see the trend and can take decisions if such diseases spread next time. I presume China did this as they played along with the SARS playbook which they drafted when it hit China in 2003. I didn't overthink at this point as to what metrics to report unless I look into the data and understand it. I had to get the ball rolling as overthinking can lead to a thought-loop and I will end up where I have started. So, I started with the first step - the data clean-up and understanding process. Data challenges # Data clean-ups # The first thing to notice was that the data needed a clean-up. The first column date had two different formats when imported into Excel. I don't know why this happens in Excel. Not a good start! Ugh! Upon Googling, I found that this is a known problem in Excel (for large datasets?). The first challenge - I have to bring it to a consistent format to present a meaningful analysis. I had to further see which of the two formats needed a clean-up. Upon applying the filter, I got to see that mm/dd/yyyy was not properly picked up and it needs clean-up. Date problem in Excel Though the data in the Tableau website is correct, when opened locally in Excel, Excel might change the day to month and month to day. This would change the date value e.g. march 11, 2020 will change to November 03, 2020, as the day and month might get swapped. We have to correct the data before we start working on the data. The solution to the problem is documented here Understanding the data # Understanding the data is critical - what data is captured in the spreadsheet and how to interpret it!? First of all, the date column was not sorted. I had to sort on the date - Column A to set the date in ascending order. This would give me the ability to see how the total cases were captured. Is it incremental or cumulative cases reported per day? Then, I did a quick pivot table analysis to check the cases of a few countries; one being Afghanistan. Afghanistan - as per my analysis with a quick pivot table suggested that there were a total of 418 cases. To understand if the above numbers were correct, I cross-verified with the coronavirus numbers published by John Hopkins University's Corona Dashboard What I found was that the total cases for Afghanistan was only 40! (see image below - as of date Mar 23, 2020) The data-set which I downloaded was also accurate but, my interpretation of the data was wrong. The cases were captured by date and the latest date across any country was the cumulative total on that date. When I did a pivot table and computed by \"Sum of total cases\", I got a grossly wrong number. Instead, the total cases have to be set to \"Max of total cases\" in the pivot table to get the correct number. I did that and I could see that the data was in-line with the data provided in the John Hopkins University website. But there was one more problem! Manual tweaks # When I took the \"Max of total cases\", it would not work for countries like China, US, France, etc as they have multiple districts and province numbers reported separately in each row. What do I mean? Well, for countries like America, it had multiple rows - one for each State in the US like Arizona, Washington, New York, etc. But, for countries like India, Japan, etc there was just one row for the entire country. So, if I take the China data and compute the data based on \"Max of total cases\", it would only report numbers from the Wuhan province as it had the most confirmed cases. For most of the countries like India, Japan etc, I had to go with the \"Max of total cases\" route and for other countries like China, the US etc, I had to compute based on \"Sum of total cases\". This data pre-processing took hours as I didn't expect the pattern of data to change based on countries. At the end, I found it and manually tweaked the data. It was a boring process but I had to do and correct it. Lesson learned: Always double check the data. Additional cleanups # Concerning data, one more clean up was necessary. The total number of cases were broken down into categories like \"Confirmed\", \"Active\", \"Recovered\" and \"Deaths\" and all these numbers were reported in \"Rows\". [See image below] What I need was - the data to be represented in columns (like the image below) so that aggregating the numbers would be easier. To get it in this format, I had to do a pivot table on the raw data and for each country, I had to translate data in this format. I had to break this task into two days as the dataset had 168 countries and each country had approximately 80 rows of data. Once I had this data cleaned and prepared, I was ready to launch the analysis. Solution Approach # Give me six hours to cut a tree and I will spend the first four to sharpen the axe # \u2015 Abraham Lincoln Anybody can cut a tree when enough time is given. But, the challenge is to cut a tree in six hours which not everyone can do. The approach has to be different. My thought process at this point was - how can I efficiently analyze the data!? The dataset is huge; 110K rows which I had aggregated to 9700+ rows for 160+ countries. In short, I thought how can I sharpen the axe so that I can cut down the tree in minimal time. I have a week to complete the analysis. I can take the traditional route and make one worksheet for each country and each sheet can have n n number of pivot tables and graphs which can be analyzed further. I started with this approach and whenever I saved the file, it took almost 10 seconds to save. The reason behind is the master data, cleaned up data was all in one file. I had to move the master data to another sheet to save some time plus size of the file itself. But, that didn't help much. Since I had multiple sheets and multiple graphs, I had to scramble across sheets to find the required data and in the process, I got mad. Yeah! too much data can make you mad - at times. There has to be a better way! What if I can create one master sheet and have it as a base template? When I change a country from a drop-down list, the dataset has to change and the graph should refresh automatically. Sounded better! Well, there was a problem! I didn't know how to do it. I had never worked with Excel dynamic ranges or drop-down lists. I began my research and learned a few topics and watched some videos. (References at the end of this article.) And, when you want something, all the universe conspires in helping you to achieve it. # \u2015 Paulo Coelho One thing leads to another, that thing leads to another and finally, I learned the following functions and features in Excel that helped me to get it done. Functions learnt MATCH INDEX OFFSET ADDRESS SMALL Features learnt Using F9 key in formula bar Name manager Array Formulas Maps in Excel Controls like text box, list box. Graph features like Markers Tweaks like Highlighting a specific bar in a bar chart Adding vertical lines to any chart to highlight a certain date Extra stuff learnt Dashboards in Excel Preparing layouts in Powerpoint (Yes! Powerpoint) and bring that to Excel Design principles All these topics were learnt as and when it required in preparing the holy grail - the Excel dashboard. First iteration # I prepared a time-series chart in a separate sheet to show the confirmed cases, active and recovered cases over time, plotted the map for the specific country (Excel makes it very easy), another time series to show the recovery % and death %, Next 10 countries in the list to show how the selected country is doing with respect to its counterparts, list countries based on their recovery and death percentage - who is doing good? and who is doing bad? And yes! the final thing - Add a list of countries and make all the above charts dynamic. Woohoo! The first iteration is the one below! Clicking each country refreshed the graph, highlighted the specific country in the Recovery % graph and the Death % graph (highlighted in Cyan and Red respectively in the above image). I played around with it and saw what analysis I could get from it. Well! the basic analysis were there - what I needed! In three words, I can sum up everything I have learned about life: it goes on! # \u2015 Robert Frost Like the quote said, life moved on and a curious question popped up. I wanted to narrow down to a particular period to see how many people were affected in a particular period. This would give me the ability to see how pre-lockdown and post-lockdown trends vary and answer whether the lockdown was effective or not!? Such questions cannot be answered by the current dashboard as it didn't have a provision to narrow down a time-period say e.g. From Feb 21 st to Mar 7 th . This sort of ability can help me breakdown the data into different time-periods like weeks, months and analyze. Second iteration # I knew the solution would be to give some sort of date range as input - a start date and an end date. I had to introduce a couple of combo boxes and based on the from date and to date , the results should also filter and give me numbers for that week. I chose to keep the current graph but added one more graph to the bottom to show the trend and numbers for the period. With some combo boxes, INDEX , MATCH , OFFSET functions and the Name Manager feature, I was able to do it. The second iteration of the dashboard looked like the one below. I changed the layout to accommodate more graphs. 1 is the From date , 2 is the To date . Clicking each of these will change the graph 4 and 5 accordingly. Countries list 3 moved to the top. 6 is the numbers on the From date and To date respectively. 7 and 8 represents Death % and Recovery %. Not bad at all! I took it for a test and everything seems to be in place. Third iteration # To improve is to change. To be perfect is to change often. # \u2015 Sir Winston Churchill Again, I stumbled upon a thought. I am not doing this for myself. I am doing this for everyone who does not have the time to clean the data and do the analysis. Can I add a visual-cue to the graph to indicate the From date and To date so that the users know which window they are looking into? So, I decided to introduce a couple of vertical lines into the graph to show the dates. Also, I wanted to add a line to indicate when the first infected case was reported? This would give me an indication as to when the infection started and how much time the countries had to make decisions before the infections grew exponentially. I tested it and it worked fine. Now, I can find out when the infection started and can narrow down to the period when the first death occurred. I can then find (from Google) when the lockdown period was announced and can describe the trends before and after - and how much lead-time the countries had before going on lockdown and what they could have done differently to minimize the infection. Final result # This is how the Excel based COVID19 Dashboard looks like. Here is a gif that tells you how it looks like in action. What next? # So, I was happy with the result - not only because of what I could churn out but also I learnt so much in the process. It was time-consuming but, worth it! Now you may ask where is my analysis!? Here is the generic analysis - the one that businesses would expect - one-liners mostly. Detailed analysis would make a separate post. Generic observation # Every country had enough lead time for them from the day of the first infection to when the growth in infection started exponentially. From the samples studied like the US, Italy, Spain, France, etc, the death rates (not the number of deaths) have gone up - which is worrying. As of Mar 22, 2020, India's death rate was 2% but, it has gone upto 2.2% now (Mar 27, 2020) Western countries # European countries like Italy, France, Spain, Germany all had infections within a 1 week window. Germany has managed better than the US and the United Kingdom despite having a large number of confirmed cases. Germany's death rate is less than 1%. Switzerland - a neighbor sharing borders with Italy was infected almost a month later than Italy. On 25 th Feb 2020, when Italy had almost 300+ confirmed cases, Switzerland had the first confirmed case reported. Still, It didn't introduce the lockdown till March 2 nd week. It costed Switzerland with 6575 confirmed cases. The US is one of the worst mis-managed countries with 307 deaths and 17 recoveries despite having a well-established health care system. The death rate to the recovery rate is too high. This is due to bad decisions like undermining the seriousness of the situation, not practicing social-distancing, not enforcing lockdowns etc. Developing countries # Most Middle-east countries have managed the virus much better than European countries despite having lesser resources. They have lesser death rate and good recovery rates when compared to European countries. Iran was affected most because of economic sanctions. With 8% of confirmed cases dead, it has closed the gap between the active cases and recovered cases with 37% recoveries - only next to China and Bahrain. Most developing countries have death rates above 5%. E.g Algeria at 11%, Indonesia, Bangladesh, Iran, Iraq at 8%, Mauritius, Ukraine, Jamaica, etc above 6%. This might be largely due to poor health-care infrastructure. African countries # African countries are outliers (less number of cases so far) but, if they are affected, the death rates could be higher than other countries to as close as 20% due to poor health care infrastructure. At the time of analysis, the infection count is less but, once it starts, the fatality rate would be higher than other nations. Summary # This exercise was time consuming yet, rewarding. On the learning front, some key take-aways Data analysis comprises a lot of data-preparation activity (read that as 'nasty' work) which is about 60% to 80% but, by the time it is over, we may already get a feel of the data. It is boring work we have to live with. Useful data (cleaned data) will give a lot of insights and there is no end to how much value we can churn from it. It is only a matter of time - we need to decide what is most important and what is not. We have to prioritize based on business objectives. Sharpening the axe (building right tools - in this case, a dashboard) is an important skill and if we do it faster, cutting the tree (analysis) will be a breeze. Excel is a versatile tool. It may trick with its simplicity but, it provides so much for people who look into it deeply enough. Data science - in other words, means you have to keep learning new tools, techniques and assimilate whatever is thrown-upon on you faster. What does it have for others? # Well, you can download the COVID19 Excel dashboard (link below) and can come up with your analysis. That way, this is open-ended and gives you the opportunity to explore more and come up with analysis. In short, this is my version of the sharpened axe you can take and go for cutting the tree! If you have any feedback on how this can be improved, let me know. Thanks for reading! Be socially responsible, practice social-distancing and stay safe! References John Hopkins University - Corona Website COVID India Tracker Website Excel Dynamic Dropdown Video Excel Dashboard Tutorial Download Covid19 Dashboard # The Excel dashboard discussed in this post can be downloaded from the following link. Excel - COVID19 Dashboard","title":"Covid19 Data Analysis"},{"location":"blog/covid19-dataset-analysis/#analysis_of_covid-19_dataset","text":"A personal account of my experience - the thought process, challenges and solution approach is documented in this post. The idea is to express that data analysis / science can be tricky and not everyone gets it right the first time. It takes multiple iterations to come up with something decent while navigating the challenges faced.","title":"Analysis of COVID-19 dataset"},{"location":"blog/covid19-dataset-analysis/#objective","text":"The objective of this exercise is to analyze the COVID-19 cases and report the findings. The data for the same can be downloaded from Tableau Website . The data source has approximately 114,000 rows of COVID-19 cases all over the world. The following data is provided - the date, country, region, the case status (active, confirmed, recovered, deaths) information and the location details. Note This is a point in time exercise. The dataset is of Mar 21, 2020 and the analysis is based on this data. Any interpretations are based on this date and might not reflect the current trend.","title":"Objective"},{"location":"blog/covid19-dataset-analysis/#tool_to_be_used","text":"The data will be analyzed via Microsoft Excel and insights will be provided. Why Excel? Because, at this point, I knew only Excel (40% I presume). But, I thought that it was enough to do this exercise. Actually speaking, the choice of the tool doesn't matter. What matters is the analysis! I took stock of what all Excel stuff I knew and whether it would be sufficient to do my analysis!? I didn't know yet. The following features of Excel may be used to make analysis easier was my guess. pivot table basic functions graphs In the retrospective, I was grossly wrong on my Excel skills and I had to do some learning along the way to achieve what I wanted.","title":"Tool to be used"},{"location":"blog/covid19-dataset-analysis/#metrics","text":"At the start of the analysis, I was not having a definitive list of what metrics I would be reporting on - as I was just having a raw dataset. The possible things that came up rationally is to report on certain key metrics that are easy to report like Top 10 countries with most confirmed cases Top 10 countries with most deaths Top 10 countries with most recovered cases Time series analysis of the growth pattern across countries Geological location of cases concentrated Time series data is the complex one here. Why complex? Anything with dates' - my understanding is that it can be complex. Geological location - well, I don't have much experience in plotting data on maps. But, apart from that, the rest is easy to report. Will the analysis be interesting? Will I capture the essence of the dataset? Not sure yet! My understanding is that data analysis does not have a definitive ending. What metrics to report on depends on the amount of time we have at hand and what metrics the business will be interested in. If we think that way, the top three in the above list does not add much business value plus point 5 is just cosmetic. Time series analysis is the one the business will be much interested in as they can see the trend and can take decisions if such diseases spread next time. I presume China did this as they played along with the SARS playbook which they drafted when it hit China in 2003. I didn't overthink at this point as to what metrics to report unless I look into the data and understand it. I had to get the ball rolling as overthinking can lead to a thought-loop and I will end up where I have started. So, I started with the first step - the data clean-up and understanding process.","title":"Metrics"},{"location":"blog/covid19-dataset-analysis/#data_challenges","text":"","title":"Data challenges"},{"location":"blog/covid19-dataset-analysis/#data_clean-ups","text":"The first thing to notice was that the data needed a clean-up. The first column date had two different formats when imported into Excel. I don't know why this happens in Excel. Not a good start! Ugh! Upon Googling, I found that this is a known problem in Excel (for large datasets?). The first challenge - I have to bring it to a consistent format to present a meaningful analysis. I had to further see which of the two formats needed a clean-up. Upon applying the filter, I got to see that mm/dd/yyyy was not properly picked up and it needs clean-up. Date problem in Excel Though the data in the Tableau website is correct, when opened locally in Excel, Excel might change the day to month and month to day. This would change the date value e.g. march 11, 2020 will change to November 03, 2020, as the day and month might get swapped. We have to correct the data before we start working on the data. The solution to the problem is documented here","title":"Data clean-ups"},{"location":"blog/covid19-dataset-analysis/#understanding_the_data","text":"Understanding the data is critical - what data is captured in the spreadsheet and how to interpret it!? First of all, the date column was not sorted. I had to sort on the date - Column A to set the date in ascending order. This would give me the ability to see how the total cases were captured. Is it incremental or cumulative cases reported per day? Then, I did a quick pivot table analysis to check the cases of a few countries; one being Afghanistan. Afghanistan - as per my analysis with a quick pivot table suggested that there were a total of 418 cases. To understand if the above numbers were correct, I cross-verified with the coronavirus numbers published by John Hopkins University's Corona Dashboard What I found was that the total cases for Afghanistan was only 40! (see image below - as of date Mar 23, 2020) The data-set which I downloaded was also accurate but, my interpretation of the data was wrong. The cases were captured by date and the latest date across any country was the cumulative total on that date. When I did a pivot table and computed by \"Sum of total cases\", I got a grossly wrong number. Instead, the total cases have to be set to \"Max of total cases\" in the pivot table to get the correct number. I did that and I could see that the data was in-line with the data provided in the John Hopkins University website. But there was one more problem!","title":"Understanding the data"},{"location":"blog/covid19-dataset-analysis/#manual_tweaks","text":"When I took the \"Max of total cases\", it would not work for countries like China, US, France, etc as they have multiple districts and province numbers reported separately in each row. What do I mean? Well, for countries like America, it had multiple rows - one for each State in the US like Arizona, Washington, New York, etc. But, for countries like India, Japan, etc there was just one row for the entire country. So, if I take the China data and compute the data based on \"Max of total cases\", it would only report numbers from the Wuhan province as it had the most confirmed cases. For most of the countries like India, Japan etc, I had to go with the \"Max of total cases\" route and for other countries like China, the US etc, I had to compute based on \"Sum of total cases\". This data pre-processing took hours as I didn't expect the pattern of data to change based on countries. At the end, I found it and manually tweaked the data. It was a boring process but I had to do and correct it. Lesson learned: Always double check the data.","title":"Manual tweaks"},{"location":"blog/covid19-dataset-analysis/#additional_cleanups","text":"Concerning data, one more clean up was necessary. The total number of cases were broken down into categories like \"Confirmed\", \"Active\", \"Recovered\" and \"Deaths\" and all these numbers were reported in \"Rows\". [See image below] What I need was - the data to be represented in columns (like the image below) so that aggregating the numbers would be easier. To get it in this format, I had to do a pivot table on the raw data and for each country, I had to translate data in this format. I had to break this task into two days as the dataset had 168 countries and each country had approximately 80 rows of data. Once I had this data cleaned and prepared, I was ready to launch the analysis.","title":"Additional cleanups"},{"location":"blog/covid19-dataset-analysis/#solution_approach","text":"","title":"Solution Approach"},{"location":"blog/covid19-dataset-analysis/#give_me_six_hours_to_cut_a_tree_and_i_will_spend_the_first_four_to_sharpen_the_axe","text":"\u2015 Abraham Lincoln Anybody can cut a tree when enough time is given. But, the challenge is to cut a tree in six hours which not everyone can do. The approach has to be different. My thought process at this point was - how can I efficiently analyze the data!? The dataset is huge; 110K rows which I had aggregated to 9700+ rows for 160+ countries. In short, I thought how can I sharpen the axe so that I can cut down the tree in minimal time. I have a week to complete the analysis. I can take the traditional route and make one worksheet for each country and each sheet can have n n number of pivot tables and graphs which can be analyzed further. I started with this approach and whenever I saved the file, it took almost 10 seconds to save. The reason behind is the master data, cleaned up data was all in one file. I had to move the master data to another sheet to save some time plus size of the file itself. But, that didn't help much. Since I had multiple sheets and multiple graphs, I had to scramble across sheets to find the required data and in the process, I got mad. Yeah! too much data can make you mad - at times. There has to be a better way! What if I can create one master sheet and have it as a base template? When I change a country from a drop-down list, the dataset has to change and the graph should refresh automatically. Sounded better! Well, there was a problem! I didn't know how to do it. I had never worked with Excel dynamic ranges or drop-down lists. I began my research and learned a few topics and watched some videos. (References at the end of this article.)","title":"Give me six hours to cut a tree and I will spend the first four to sharpen the axe"},{"location":"blog/covid19-dataset-analysis/#and_when_you_want_something_all_the_universe_conspires_in_helping_you_to_achieve_it","text":"\u2015 Paulo Coelho One thing leads to another, that thing leads to another and finally, I learned the following functions and features in Excel that helped me to get it done.","title":"And, when you want something, all the universe conspires in helping you to achieve it."},{"location":"blog/covid19-dataset-analysis/#first_iteration","text":"I prepared a time-series chart in a separate sheet to show the confirmed cases, active and recovered cases over time, plotted the map for the specific country (Excel makes it very easy), another time series to show the recovery % and death %, Next 10 countries in the list to show how the selected country is doing with respect to its counterparts, list countries based on their recovery and death percentage - who is doing good? and who is doing bad? And yes! the final thing - Add a list of countries and make all the above charts dynamic. Woohoo! The first iteration is the one below! Clicking each country refreshed the graph, highlighted the specific country in the Recovery % graph and the Death % graph (highlighted in Cyan and Red respectively in the above image). I played around with it and saw what analysis I could get from it. Well! the basic analysis were there - what I needed!","title":"First iteration"},{"location":"blog/covid19-dataset-analysis/#in_three_words_i_can_sum_up_everything_i_have_learned_about_life_it_goes_on","text":"\u2015 Robert Frost Like the quote said, life moved on and a curious question popped up. I wanted to narrow down to a particular period to see how many people were affected in a particular period. This would give me the ability to see how pre-lockdown and post-lockdown trends vary and answer whether the lockdown was effective or not!? Such questions cannot be answered by the current dashboard as it didn't have a provision to narrow down a time-period say e.g. From Feb 21 st to Mar 7 th . This sort of ability can help me breakdown the data into different time-periods like weeks, months and analyze.","title":"In three words, I can sum up everything I have learned about life: it goes on!"},{"location":"blog/covid19-dataset-analysis/#second_iteration","text":"I knew the solution would be to give some sort of date range as input - a start date and an end date. I had to introduce a couple of combo boxes and based on the from date and to date , the results should also filter and give me numbers for that week. I chose to keep the current graph but added one more graph to the bottom to show the trend and numbers for the period. With some combo boxes, INDEX , MATCH , OFFSET functions and the Name Manager feature, I was able to do it. The second iteration of the dashboard looked like the one below. I changed the layout to accommodate more graphs. 1 is the From date , 2 is the To date . Clicking each of these will change the graph 4 and 5 accordingly. Countries list 3 moved to the top. 6 is the numbers on the From date and To date respectively. 7 and 8 represents Death % and Recovery %. Not bad at all! I took it for a test and everything seems to be in place.","title":"Second iteration"},{"location":"blog/covid19-dataset-analysis/#third_iteration","text":"","title":"Third iteration"},{"location":"blog/covid19-dataset-analysis/#to_improve_is_to_change_to_be_perfect_is_to_change_often","text":"\u2015 Sir Winston Churchill Again, I stumbled upon a thought. I am not doing this for myself. I am doing this for everyone who does not have the time to clean the data and do the analysis. Can I add a visual-cue to the graph to indicate the From date and To date so that the users know which window they are looking into? So, I decided to introduce a couple of vertical lines into the graph to show the dates. Also, I wanted to add a line to indicate when the first infected case was reported? This would give me an indication as to when the infection started and how much time the countries had to make decisions before the infections grew exponentially. I tested it and it worked fine. Now, I can find out when the infection started and can narrow down to the period when the first death occurred. I can then find (from Google) when the lockdown period was announced and can describe the trends before and after - and how much lead-time the countries had before going on lockdown and what they could have done differently to minimize the infection.","title":"To improve is to change. To be perfect is to change often."},{"location":"blog/covid19-dataset-analysis/#final_result","text":"This is how the Excel based COVID19 Dashboard looks like. Here is a gif that tells you how it looks like in action.","title":"Final result"},{"location":"blog/covid19-dataset-analysis/#what_next","text":"So, I was happy with the result - not only because of what I could churn out but also I learnt so much in the process. It was time-consuming but, worth it! Now you may ask where is my analysis!? Here is the generic analysis - the one that businesses would expect - one-liners mostly. Detailed analysis would make a separate post.","title":"What next?"},{"location":"blog/covid19-dataset-analysis/#generic_observation","text":"Every country had enough lead time for them from the day of the first infection to when the growth in infection started exponentially. From the samples studied like the US, Italy, Spain, France, etc, the death rates (not the number of deaths) have gone up - which is worrying. As of Mar 22, 2020, India's death rate was 2% but, it has gone upto 2.2% now (Mar 27, 2020)","title":"Generic observation"},{"location":"blog/covid19-dataset-analysis/#western_countries","text":"European countries like Italy, France, Spain, Germany all had infections within a 1 week window. Germany has managed better than the US and the United Kingdom despite having a large number of confirmed cases. Germany's death rate is less than 1%. Switzerland - a neighbor sharing borders with Italy was infected almost a month later than Italy. On 25 th Feb 2020, when Italy had almost 300+ confirmed cases, Switzerland had the first confirmed case reported. Still, It didn't introduce the lockdown till March 2 nd week. It costed Switzerland with 6575 confirmed cases. The US is one of the worst mis-managed countries with 307 deaths and 17 recoveries despite having a well-established health care system. The death rate to the recovery rate is too high. This is due to bad decisions like undermining the seriousness of the situation, not practicing social-distancing, not enforcing lockdowns etc.","title":"Western countries"},{"location":"blog/covid19-dataset-analysis/#developing_countries","text":"Most Middle-east countries have managed the virus much better than European countries despite having lesser resources. They have lesser death rate and good recovery rates when compared to European countries. Iran was affected most because of economic sanctions. With 8% of confirmed cases dead, it has closed the gap between the active cases and recovered cases with 37% recoveries - only next to China and Bahrain. Most developing countries have death rates above 5%. E.g Algeria at 11%, Indonesia, Bangladesh, Iran, Iraq at 8%, Mauritius, Ukraine, Jamaica, etc above 6%. This might be largely due to poor health-care infrastructure.","title":"Developing countries"},{"location":"blog/covid19-dataset-analysis/#african_countries","text":"African countries are outliers (less number of cases so far) but, if they are affected, the death rates could be higher than other countries to as close as 20% due to poor health care infrastructure. At the time of analysis, the infection count is less but, once it starts, the fatality rate would be higher than other nations.","title":"African countries"},{"location":"blog/covid19-dataset-analysis/#summary","text":"This exercise was time consuming yet, rewarding. On the learning front, some key take-aways Data analysis comprises a lot of data-preparation activity (read that as 'nasty' work) which is about 60% to 80% but, by the time it is over, we may already get a feel of the data. It is boring work we have to live with. Useful data (cleaned data) will give a lot of insights and there is no end to how much value we can churn from it. It is only a matter of time - we need to decide what is most important and what is not. We have to prioritize based on business objectives. Sharpening the axe (building right tools - in this case, a dashboard) is an important skill and if we do it faster, cutting the tree (analysis) will be a breeze. Excel is a versatile tool. It may trick with its simplicity but, it provides so much for people who look into it deeply enough. Data science - in other words, means you have to keep learning new tools, techniques and assimilate whatever is thrown-upon on you faster.","title":"Summary"},{"location":"blog/covid19-dataset-analysis/#what_does_it_have_for_others","text":"Well, you can download the COVID19 Excel dashboard (link below) and can come up with your analysis. That way, this is open-ended and gives you the opportunity to explore more and come up with analysis. In short, this is my version of the sharpened axe you can take and go for cutting the tree! If you have any feedback on how this can be improved, let me know. Thanks for reading! Be socially responsible, practice social-distancing and stay safe!","title":"What does it have for others?"},{"location":"blog/covid19-dataset-analysis/#download_covid19_dashboard","text":"The Excel dashboard discussed in this post can be downloaded from the following link. Excel - COVID19 Dashboard","title":"Download Covid19 Dashboard"},{"location":"exercises/","text":"List of exercises and the solutions # Statistics Exercises # Course Class Exercise Number + Link Exercise Description Statistics 01 Exercise01 Statistics 01 Exercise02 Statistics 02 Exercise03 Statistics 02 Exercise04 Statistics 03 Exercise05 Normal Distribution Vs Probability Distribution Statistics 03 Exercise06 Confidence Interval Python Exercises # Course Class Exercise Number + Link Exercise Description Python 01 Exercise01 Python variables and datatypes","title":"Exercises Index"},{"location":"exercises/#list_of_exercises_and_the_solutions","text":"","title":"List of exercises and the solutions"},{"location":"exercises/#statistics_exercises","text":"Course Class Exercise Number + Link Exercise Description Statistics 01 Exercise01 Statistics 01 Exercise02 Statistics 02 Exercise03 Statistics 02 Exercise04 Statistics 03 Exercise05 Normal Distribution Vs Probability Distribution Statistics 03 Exercise06 Confidence Interval","title":"Statistics Exercises"},{"location":"exercises/#python_exercises","text":"Course Class Exercise Number + Link Exercise Description Python 01 Exercise01 Python variables and datatypes","title":"Python Exercises"},{"location":"exercises/exercise05/","text":"Normal Distribution is a probability distribution. # The topic in question is - Is normal distribution a probability distribution? Yes, it is and this writeup provides justification as to why it is so! Revisiting the Normal distribution # Let's revisit the normal distribution once - its characteristics and emprical rule Normal distribution is the distribution of data that is symmetric over the mean. 50% of the data lie to the left and 50% of the data lie to the right At the origin, mean = median = mode = 0 Emprical rule # The emprical rule states that - 68% of the data will be within one standard deviation of the mean - 95% of the data will be within two standard deviations from the mean - 99.7% of the data will be within three standard deviations from the mean Lets see that in the figure below Example Dataset # Let's plot a normal distribution graph of 300 people with a mean salary of 50,000 and standard deviation of 8700 (sigma) Descriptive Statistics Value Mean 50000 Median 50000 Std dev p 8700 Mode 50000 Then the following will be the ranges of the salary from the mean. Sigma Range Salary -1 sigma 41300 +1 sigma 58700 -2 sigma 32600 +2 sigma 67400 -3 sigma 23900 +3 sigma 76100 The number of people with the respective salary range and the normal distribution is mentioned below. Salary Range No of people Norm Dist 23000-33000 0 0 33001-41000 64 0.00118224 41001-50000 88 0.003468298 50001-59000 87 0.003404399 59001-67000 63 0.001154427 67001-76000 0 0 Normal distribution computed with NORMAL.DIST function in EXCEL So, we have all of our 300 people salaries mapped as a normal distribution. Random sample # Let's take a random sample from our range of salaries. NOTE: use the EXCEL RANDBETWEEN function to generate a random sample provided a range. Here our range is from -3 sigma to +3 sigma which is from 23900 to 76100 The random sample i.e. salary taken is 47243 Question? # What is the chance that this random sample (e.g. 47243 ) falls within -3 sigma to +3 sigma? i.e. the area under the curve (highlighted in red color)? Can we say that 99.7% of the time of the time the random sample taken will fall within the -3 sigma to +3 sigma (i.e. the area under the curve) This 99.7% chance is called as the probability of the sample falling within the range -3 sigma to +3 sigma. The range of Probability is from 0 to 1 . i.e 0 for no chance and 1 for 100% chance. One more sample # Let's take one more sample between -3 sigma and +3 sigma. What is the chance that it will fall under the area of the curve highlighted below (in red color) i.e. from -1 sigma to +1 sigma? Can we say that it will be 68% or 0.68 probability? Lets prove it! The area under the chart is from -1 sigma to +1 sigma. Correct? Let's revisit Z (zee) scores once z scores is the number of units of standard deviation from the mean. Here the range in consideration is -1 sigma to +1 sigma i.e we want to find out if a random sample (salary) falls within this range. So the z scores are -1 and +1 . To be precise we can say -1.0 and +1.0 Let's bring Z-Tables. Z-Tables provides us the area under the curve / probability. Using Z-tables lets find the area under the curve of -1.0Z Since -1 is a negative Z, we see the negative Z score table and see that the probability is 0.15866 (see figure below) i.e if we take a random sample between -3 sigma and +3 sigma, the chance of the random sample falling under the area of the curve of -1Z is 15.866% . Thus Z tables us gives the probability. For +1Z , let's refer to the positive Z score table. The area under the curve is 0.84134 . (See fig below). To summarize, to find out the area under the curve -1Z to +1Z , we need to subtract the area under +1Z with -1Z Note: Zee is 1 unit of standard deviation. So it is -1 sigma to +1 sigma i.e 0.84134 - 0.15866 = 0.68268 . Thus 68.268% is the probability that a given random sample will fall under the range of -1 sigma to +1 sigma. In this way, we can find out the probability of a random variable falling under the area of the curve of the normal distribution for any Zee values. i.e from -3 sigma to -2 sigma. or from -2 sigma to +3 sigma etc. Conclusion # Thus we have emprically concluded that the normal distribution is a probability distribution as it helps to find the probability of a random variable falling within the normal distribution range.","title":"Exercise05"},{"location":"exercises/exercise05/#normal_distribution_is_a_probability_distribution","text":"The topic in question is - Is normal distribution a probability distribution? Yes, it is and this writeup provides justification as to why it is so!","title":"Normal Distribution is a probability distribution."},{"location":"exercises/exercise05/#revisiting_the_normal_distribution","text":"Let's revisit the normal distribution once - its characteristics and emprical rule Normal distribution is the distribution of data that is symmetric over the mean. 50% of the data lie to the left and 50% of the data lie to the right At the origin, mean = median = mode = 0","title":"Revisiting the Normal distribution"},{"location":"exercises/exercise05/#emprical_rule","text":"The emprical rule states that - 68% of the data will be within one standard deviation of the mean - 95% of the data will be within two standard deviations from the mean - 99.7% of the data will be within three standard deviations from the mean Lets see that in the figure below","title":"Emprical rule"},{"location":"exercises/exercise05/#example_dataset","text":"Let's plot a normal distribution graph of 300 people with a mean salary of 50,000 and standard deviation of 8700 (sigma) Descriptive Statistics Value Mean 50000 Median 50000 Std dev p 8700 Mode 50000 Then the following will be the ranges of the salary from the mean. Sigma Range Salary -1 sigma 41300 +1 sigma 58700 -2 sigma 32600 +2 sigma 67400 -3 sigma 23900 +3 sigma 76100 The number of people with the respective salary range and the normal distribution is mentioned below. Salary Range No of people Norm Dist 23000-33000 0 0 33001-41000 64 0.00118224 41001-50000 88 0.003468298 50001-59000 87 0.003404399 59001-67000 63 0.001154427 67001-76000 0 0 Normal distribution computed with NORMAL.DIST function in EXCEL So, we have all of our 300 people salaries mapped as a normal distribution.","title":"Example Dataset"},{"location":"exercises/exercise05/#random_sample","text":"Let's take a random sample from our range of salaries. NOTE: use the EXCEL RANDBETWEEN function to generate a random sample provided a range. Here our range is from -3 sigma to +3 sigma which is from 23900 to 76100 The random sample i.e. salary taken is 47243","title":"Random sample"},{"location":"exercises/exercise05/#question","text":"What is the chance that this random sample (e.g. 47243 ) falls within -3 sigma to +3 sigma? i.e. the area under the curve (highlighted in red color)? Can we say that 99.7% of the time of the time the random sample taken will fall within the -3 sigma to +3 sigma (i.e. the area under the curve) This 99.7% chance is called as the probability of the sample falling within the range -3 sigma to +3 sigma. The range of Probability is from 0 to 1 . i.e 0 for no chance and 1 for 100% chance.","title":"Question?"},{"location":"exercises/exercise05/#one_more_sample","text":"Let's take one more sample between -3 sigma and +3 sigma. What is the chance that it will fall under the area of the curve highlighted below (in red color) i.e. from -1 sigma to +1 sigma? Can we say that it will be 68% or 0.68 probability?","title":"One more sample"},{"location":"exercises/exercise05/#conclusion","text":"Thus we have emprically concluded that the normal distribution is a probability distribution as it helps to find the probability of a random variable falling within the normal distribution range.","title":"Conclusion"},{"location":"exercises/exercise06/","text":"Confidence level Vs the accuracy of estimation of the population mean # Does higher the confidence level in the confidence interval mean more the accurate prediction of the estimation of the population mean? Confidence Interval # A confidence interval is the range of numbers that is believed to include an unknown population parameter. Confidence Level # It is the measure of confidence that the unknown parameter (of the population) lies within the confidence interval. A case study # Comcast, the computer services company, is planning to invest heavily in online television service. As part of the decision, the company wants to estimate the average number of online shows a family of four would watch per day. A random sample of 100 families is obtained, and in this sample the average number of shows viewed per day is 6.5 and the population standard deviation is known to be 3.2 . Construct a 95% confidence interval for the average number of online television shows watched by the entire population of families of four. Known measures sample size - $$ n = 100 $$ sample mean - $$ \\bar{x} = 6.5 $$ population standard deviation - $$ \\sigma = 3.2 $$ confidence interval $$ Z = 1.96 (for \\space the \\space value \\space of \\space 95 \\space percent) $$ To be found: Confidence interval within which the population mean i.e. the average number of online television shows are watched by the entire population Let's picturize it Let's understand the picture Let's compute the confidence interval Confidence interval = \\mu \\pm Z * (\\sigma / \\sqrt n) \\mu \\pm Z * (\\sigma / \\sqrt n) Since we don't know the population parameter $$ \\mu $$ here, we will replace it with the value we know about the sample i.e. the sample mean $$ \\bar{x} $$ So the confidence interval in this case is $$ \\bar{x} \\pm Z * (\\sigma / \\sqrt n) $$ When substituted with values above, we get $$ confidence interval = 6.5 \\pm 1.96 * (3.2 / \\sqrt(100) $$ Note: The Z value for 95% confidence is 1.96 We get the confidence interval as [5.8728, 7.1272] If we reduce the confidence level to 80%, what happens? Note: the Z score for 80% confidence is 1.282 $$ confidence interval = 6.5 \\pm 1.28 * (3.2 / \\sqrt(100) $$ The confidence interval becomes [6.0904, 6.9096] Inference So, when the confidence level decreases from 95% to 80%, the confidence interval becomes narrorwer - which would mean our chance of finding the population mean reduces by 15% So it is better to always have a high confidence level to get more accurate estimations of the population mean What if i need to have a good confidence level i.e. 95% but the confidence interval should be narrower to get closer to the population mean - more accuracy in predicting? Answer Increase the sample size i.e. increase the sample size n=100 provided above to n=200 . The confidence interval becomes [6.0565, 6.9434] Though the confidence interval is narrower, the population mean will be falling within this range 95% of the times as 95% is the confidence level.","title":"Exercise06"},{"location":"exercises/exercise06/#confidence_level_vs_the_accuracy_of_estimation_of_the_population_mean","text":"Does higher the confidence level in the confidence interval mean more the accurate prediction of the estimation of the population mean?","title":"Confidence level Vs the accuracy of estimation of the population mean"},{"location":"exercises/exercise06/#confidence_interval","text":"A confidence interval is the range of numbers that is believed to include an unknown population parameter.","title":"Confidence Interval"},{"location":"exercises/exercise06/#confidence_level","text":"It is the measure of confidence that the unknown parameter (of the population) lies within the confidence interval.","title":"Confidence Level"},{"location":"exercises/exercise06/#a_case_study","text":"Comcast, the computer services company, is planning to invest heavily in online television service. As part of the decision, the company wants to estimate the average number of online shows a family of four would watch per day. A random sample of 100 families is obtained, and in this sample the average number of shows viewed per day is 6.5 and the population standard deviation is known to be 3.2 . Construct a 95% confidence interval for the average number of online television shows watched by the entire population of families of four.","title":"A case study"},{"location":"exercises/exercises-todo-class5/","text":"Exercises to do - Class 5 # The following are the four exercises that needs to be completed; given as part of Class 05 - Statistics If the population standard deviation is provided and the number of samples < 30, which test do we go for? i.e. Z test or t test - with justification Which test is to be used to measure the association between two categorical variables / qualitative variables F' test in excel using the data analysis plugin - for the same problem we saw in the class Different types of Discrete distributions - 2 liner summary","title":"Exercises to do - Class 5"},{"location":"exercises/exercises-todo-class5/#exercises_to_do_-_class_5","text":"The following are the four exercises that needs to be completed; given as part of Class 05 - Statistics If the population standard deviation is provided and the number of samples < 30, which test do we go for? i.e. Z test or t test - with justification Which test is to be used to measure the association between two categorical variables / qualitative variables F' test in excel using the data analysis plugin - for the same problem we saw in the class Different types of Discrete distributions - 2 liner summary","title":"Exercises to do - Class 5"},{"location":"exercises/python/exercise01/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Solutions for Exercises - Python Basics # The solution is provided after each exercise question Exercise Question 1 # Given an input list removes the element at index 4 and add it to the 2 nd position and also, at the end of the list For example: List = [54, 44, 27, 79, 91, 41] 1 Original list [34, 54, 67, 89, 11, 43, 94] List After removing element at index 4 [34, 54, 67, 89, 43, 94] List after Adding element at index 2 [34, 54, 11, 67, 89, 43, 94] List after Adding element at last [34, 54, 11, 67, 89, 43, 94, 11] Solution for Question 1 # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 given_list = [ 34 , 54 , 67 , 89 , 11 , 43 , 94 ] # remove the element at index 4 # add the removed element to the second position # add the removed element to the last position # first step - find the element at index 4 element4 = given_list [ 4 ] # this would give the 4th element - output 11 # remove the 4th element from the given_list given_list . remove ( element4 ) # will remove 11 from the given list - output [34, 54, 67, 89, 43, 94] # second step - add the removed element to the 2nd position in the given list given_list . insert ( 2 , element4 ) # will add 11 to the given list at index 2 - output [34, 54, 11, 67, 89, 43, 94] # third step - add the removed element to the last position given_list . append ( element4 ) # will add 11 to the given list at the last position - output [34, 54, 11, 67, 89, 43, 94, 11] print ( given_list ) [34, 54, 11, 67, 89, 43, 94, 11] Exercise Question 2 # Given a two list of equal size create a list of unique elements from both the lists into a seperate list 1 First List [2, 3, 4, 5, 6, 7, 8] Second List [4, 9, 16, 25, 36, 49, 64] [64, 2, 3, 4, 5, 6, 7, 8, 9, 36, 16, 49, 25] Solution for Question 2 # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # two lists are given. Need to create a list of unique elements from both the lists first_list = [ 2 , 3 , 4 , 5 , 6 , 7 , 8 ] second_list = [ 4 , 9 , 16 , 25 , 36 , 49 , 64 ] # python gives the ability to concatenate two lists using the + symbol consolidated_list = first_list + second_list # output = [2, 3, 4, 5, 6, 7, 8, 4, 9, 16, 25, 36, 49, 64] # since we need uniqe elements, we need to create a set out of this list as set can keep only unique elements # so, we need to type cast the list as set. We can use the set function to do it. unique_list = set ( consolidated_list ) print ( unique_list ) {64, 2, 3, 4, 5, 6, 7, 8, 9, 36, 16, 49, 25} Exercise Question 3 # Remove duplicate from a list and create a tuple and find the minimum and maximum number (Hint: Try Functions Min() and Max() ) 1 Original list [87, 52, 44, 53, 54, 87, 52, 53] unique list [44, 52, 53, 54, 87] tuple (44, 52, 53, 54, 87) Minimum number is: 44 Maximum number is: 87 Solution for Question 3 # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Remove duplicate from the given list # create a tuple of that unique list # find min number and max number of the tuple original_list = [ 87 , 52 , 44 , 53 , 54 , 87 , 52 , 53 ] # to remove the duplicates, we can type cast to set unique_list = set ( original_list ) # output - {44, 52, 53, 54, 87} # create a tuple of unique list by type casting to tuple using the tuple function unique_tuple = tuple ( unique_list ) # output - (44, 52, 53, 54, 87) # find the maximum number from the tuple - use the max function max_number = max ( unique_tuple ) # find the minimum number from the tuple - use the min function min_number = min ( unique_tuple ) # output the values print ( \"Minimum number is:\" , min_number ) print ( \"Maximum number is:\" , max_number ) Minimum number is: 44 Maximum number is: 87 Exercise Question 4 # Display the each word in the string Count the number of words in a string and display it (Including the white spaces) 1 2 3 #Printing each words seperately a = \"what's up?\" print ( * a ) w h a t ' s u p ? 1 The sample string: Welcome to Python Printing each words seperately: W e l c o m e t o P y t h o n The Length of the string 17 Solution for Question 4 # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Sample string given - welcome to python # display each word in the string # count the number of words in the string # display the string with whitespaces sample_string = \"Welcome to Python\" # to print each letters seperately, use the * before the string print ( * sample_string ) # to find the length of the string, use the len function len_of_string = len ( sample_string ) print ( \"The length of the string is:\" , len_of_string ) W e l c o m e t o P y t h o n The length of the string is: 17 Exercise Question 5 # Write a Python program to access dictionary keys element by index. i.e. Use indexing methods to print the first key 1 The dictionary is: {'physics': 80, 'math': 90, 'chemistry': 86} The key element accesed by index: physics Solution for Question 5 # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # dictionary object is given # Need to find the key of the first element (80) sample_dict = { 'physics' : 80 , 'math' : 90 , 'chemistry' : 86 } # We know that the dict is a key value pair and we can get the respective keys and values using keys() and values() dict_keys = sample_dict . keys () # this would give a dict object with physics, math and chemistry dict_values = sample_dict . values () # this would give 80, 90 and 86 # Since the object is of type dict_keys and dict_values, we need to convert it into a list using type casting key_list = list ( dict_keys ) # output - ['physics', 'math', 'chemistry'] value_list = list ( dict_values ) # output - [80, 90, 86] # since we need to use indexing methods, we need to find the index of the first element in the dict first_element_index = value_list . index ( 80 ) # index of physics - output 0 # for this index, we need to find the corresponding key first_element_key = key_list [ first_element_index ] # output is physics print ( \"The key element accesed by index:\" , first_element_key ) The key element accesed by index: physics","title":"Exercise01"},{"location":"exercises/python/exercise01/#solutions_for_exercises_-_python_basics","text":"","title":"Solutions for Exercises - Python Basics"},{"location":"exercises/python/exercise01/#exercise_question_1","text":"Given an input list removes the element at index 4 and add it to the 2 nd position and also, at the end of the list For example: List = [54, 44, 27, 79, 91, 41] 1 Original list [34, 54, 67, 89, 11, 43, 94] List After removing element at index 4 [34, 54, 67, 89, 43, 94] List after Adding element at index 2 [34, 54, 11, 67, 89, 43, 94] List after Adding element at last [34, 54, 11, 67, 89, 43, 94, 11]","title":"Exercise Question 1"},{"location":"exercises/python/exercise01/#solution_for_question_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 given_list = [ 34 , 54 , 67 , 89 , 11 , 43 , 94 ] # remove the element at index 4 # add the removed element to the second position # add the removed element to the last position # first step - find the element at index 4 element4 = given_list [ 4 ] # this would give the 4th element - output 11 # remove the 4th element from the given_list given_list . remove ( element4 ) # will remove 11 from the given list - output [34, 54, 67, 89, 43, 94] # second step - add the removed element to the 2nd position in the given list given_list . insert ( 2 , element4 ) # will add 11 to the given list at index 2 - output [34, 54, 11, 67, 89, 43, 94] # third step - add the removed element to the last position given_list . append ( element4 ) # will add 11 to the given list at the last position - output [34, 54, 11, 67, 89, 43, 94, 11] print ( given_list ) [34, 54, 11, 67, 89, 43, 94, 11]","title":"Solution for Question 1"},{"location":"exercises/python/exercise01/#exercise_question_2","text":"Given a two list of equal size create a list of unique elements from both the lists into a seperate list 1 First List [2, 3, 4, 5, 6, 7, 8] Second List [4, 9, 16, 25, 36, 49, 64] [64, 2, 3, 4, 5, 6, 7, 8, 9, 36, 16, 49, 25]","title":"Exercise Question 2"},{"location":"exercises/python/exercise01/#solution_for_question_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # two lists are given. Need to create a list of unique elements from both the lists first_list = [ 2 , 3 , 4 , 5 , 6 , 7 , 8 ] second_list = [ 4 , 9 , 16 , 25 , 36 , 49 , 64 ] # python gives the ability to concatenate two lists using the + symbol consolidated_list = first_list + second_list # output = [2, 3, 4, 5, 6, 7, 8, 4, 9, 16, 25, 36, 49, 64] # since we need uniqe elements, we need to create a set out of this list as set can keep only unique elements # so, we need to type cast the list as set. We can use the set function to do it. unique_list = set ( consolidated_list ) print ( unique_list ) {64, 2, 3, 4, 5, 6, 7, 8, 9, 36, 16, 49, 25}","title":"Solution for Question 2"},{"location":"exercises/python/exercise01/#exercise_question_3","text":"Remove duplicate from a list and create a tuple and find the minimum and maximum number (Hint: Try Functions Min() and Max() ) 1 Original list [87, 52, 44, 53, 54, 87, 52, 53] unique list [44, 52, 53, 54, 87] tuple (44, 52, 53, 54, 87) Minimum number is: 44 Maximum number is: 87","title":"Exercise Question 3"},{"location":"exercises/python/exercise01/#solution_for_question_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Remove duplicate from the given list # create a tuple of that unique list # find min number and max number of the tuple original_list = [ 87 , 52 , 44 , 53 , 54 , 87 , 52 , 53 ] # to remove the duplicates, we can type cast to set unique_list = set ( original_list ) # output - {44, 52, 53, 54, 87} # create a tuple of unique list by type casting to tuple using the tuple function unique_tuple = tuple ( unique_list ) # output - (44, 52, 53, 54, 87) # find the maximum number from the tuple - use the max function max_number = max ( unique_tuple ) # find the minimum number from the tuple - use the min function min_number = min ( unique_tuple ) # output the values print ( \"Minimum number is:\" , min_number ) print ( \"Maximum number is:\" , max_number ) Minimum number is: 44 Maximum number is: 87","title":"Solution for Question 3"},{"location":"exercises/python/exercise01/#exercise_question_4","text":"Display the each word in the string Count the number of words in a string and display it (Including the white spaces) 1 2 3 #Printing each words seperately a = \"what's up?\" print ( * a ) w h a t ' s u p ? 1 The sample string: Welcome to Python Printing each words seperately: W e l c o m e t o P y t h o n The Length of the string 17","title":"Exercise Question 4"},{"location":"exercises/python/exercise01/#solution_for_question_4","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Sample string given - welcome to python # display each word in the string # count the number of words in the string # display the string with whitespaces sample_string = \"Welcome to Python\" # to print each letters seperately, use the * before the string print ( * sample_string ) # to find the length of the string, use the len function len_of_string = len ( sample_string ) print ( \"The length of the string is:\" , len_of_string ) W e l c o m e t o P y t h o n The length of the string is: 17","title":"Solution for Question 4"},{"location":"exercises/python/exercise01/#exercise_question_5","text":"Write a Python program to access dictionary keys element by index. i.e. Use indexing methods to print the first key 1 The dictionary is: {'physics': 80, 'math': 90, 'chemistry': 86} The key element accesed by index: physics","title":"Exercise Question 5"},{"location":"exercises/python/exercise01/#solution_for_question_5","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # dictionary object is given # Need to find the key of the first element (80) sample_dict = { 'physics' : 80 , 'math' : 90 , 'chemistry' : 86 } # We know that the dict is a key value pair and we can get the respective keys and values using keys() and values() dict_keys = sample_dict . keys () # this would give a dict object with physics, math and chemistry dict_values = sample_dict . values () # this would give 80, 90 and 86 # Since the object is of type dict_keys and dict_values, we need to convert it into a list using type casting key_list = list ( dict_keys ) # output - ['physics', 'math', 'chemistry'] value_list = list ( dict_values ) # output - [80, 90, 86] # since we need to use indexing methods, we need to find the index of the first element in the dict first_element_index = value_list . index ( 80 ) # index of physics - output 0 # for this index, we need to find the corresponding key first_element_key = key_list [ first_element_index ] # output is physics print ( \"The key element accesed by index:\" , first_element_key ) The key element accesed by index: physics","title":"Solution for Question 5"},{"location":"exercises/python/exercise02/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Solutions for Exercises - Python Basics # The solution is provided after each exercise question Exercise Question 1 # Write a Python program to find those numbers which are divisible by 7 and multiple of 5, between 1500 and 2700 (both included). Solution for Question 1 # 1 2 3 4 for i in range ( 1500 , 2700 ): if i % 7 == 0 : if i % 5 == 0 : print ( i ) 1505 1540 1575 1610 1645 1680 1715 1750 1785 1820 1855 1890 1925 1960 1995 2030 2065 2100 2135 2170 2205 2240 2275 2310 2345 2380 2415 2450 2485 2520 2555 2590 2625 2660 2695 Exercise Question 2 # Write a Python program to construct the following pattern, using a nested for loop.\u00c2\u00b6 Solution for Question 2 # 1 2 3 4 5 6 7 8 9 for i in range ( 1 , 10 ): if i > 5 : for j in range ( 1 , 5 ): print ( '* ' * ( i - ( j * 2 ))) i += 1 continue break else : print ( '* ' * i ) * * * * * * * * * * * * * * * * * * * * * * * * * Exercise Question 3 # Write a Python program to count the number of even and odd numbers from a series of numbers. Solution for Question 3 # 1 2 3 4 5 6 7 8 9 10 11 12 list_odd_even = list ( range ( 1 , 10 )) even_num = 0 odd_num = 0 for i in list_odd_even : if i % 2 == 0 : even_num += 1 else : odd_num += 1 print ( \"Number of even numbers :\" , even_num ) print ( \"Number of odd numbers :\" , odd_num ) Number of even numbers : 4 Number of odd numbers : 5 Exercise Question 4 # Write a Python program to find numbers between 100 and 400 (both included) where each digit of a number is an even number. The numbers obtained should be printed in a comma-separated sequence Solution for Question 4 # 1 2 3 4 5 6 7 8 9 10 range_100_400 = list ( range ( 100 , 400 )) list_output = [] for i in range_100_400 : if int ( str ( i )[ 0 ]) % 2 == 0 : if int ( str ( i )[ 1 ]) % 2 == 0 : if int ( str ( i )[ 2 ]) % 2 == 0 : list_output . append ( i ) print ( list_output ) [200, 202, 204, 206, 208, 220, 222, 224, 226, 228, 240, 242, 244, 246, 248, 260, 262, 264, 266, 268, 280, 282, 284, 286, 288] Exercise Question 5 # Write a Python program to calculate a dog's age in dog's years. Note: For the first two years, a dog year is equal to 10.5 human years. After that, each dog year equals 4 human years. Solution for Question 5 # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 human_years = 0 dog_years = 0 dog_age_input = input ( \"Input a dog's age in human years:\" ) if int ( dog_age_input ) > 2 : dog_years = int ( dog_age_input ) - 2 human_years += 2 * 10.5 human_years += dog_years * 4 elif int ( dog_age_input ) < 3 : if int ( dog_age_input ) == 2 : human_years += 2 * 10.5 else : human_years += 10.5 print ( \"The dog's age in dog's years is\" , int ( human_years )) Input a dog's age in human years:20 The dog's age in dog's years is 93 Exercise Question 6 # Write a Python function to find the Max of three numbers Solution for Question 6 # 1 2 3 4 def find_max ( list_of_num ): return max ( list_of_num ) find_max ([ 3 , 6 , - 5 ]) 6 Exercise Question 7 # Write a Python function that takes a number as a parameter and check the number is prime or not Solution for Question 7 # 1 2 3 4 5 6 7 8 9 10 def prime ( num ): factor = 0 for i in range ( 2 , num + 1 ): if num % i == 0 : factor += 1 print ( i , factor ) if factor > 1 : return False return True Exercise Question 8 # Write a Python function that accepts a string and calculate the number of upper case letters and lower case letters Solution for Question 8 # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def upper_lower_case_letters ( input_str ): upper_case_letters = 0 lower_case_letters = 0 for i in range ( 0 , len ( input_str )): if input_str [ i ] != \" \" : if input_str [ i ] . isupper (): upper_case_letters += 1 else : lower_case_letters += 1 return upper_case_letters , lower_case_letters uc , lc = upper_lower_case_letters ( \"Inceptz is One of The BeSt Places TO LEarn DataSciEnce\" ) print ( \"No of upper case characters :\" , uc ) print ( \"No of lower case characters :\" , lc ) No of upper case characters : 13 No of lower case characters : 32 Exercise Question 9 # Write a Python program to reverse a string. Solution for Question 9 # 1 2 given_string = \"Hello, World!\" print ( given_string [ - 1 :: - 1 ]) !dlroW ,olleH Exercise Question 10 # Write a Python program to find the greatest common divisor (gcd) of two integers. Solution for Question 10 # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Python program to find H.C.F of two numbers # define a function def compute_hcf ( x , y ): # choose the smaller number if x > y : smaller = y else : smaller = x for i in range ( 1 , smaller + 1 ): if (( x % i == 0 ) and ( y % i == 0 )): hcf = i return hcf num1 = 54 num2 = 24 print ( \"The H.C.F. is\" , compute_hcf ( num1 , num2 )) The H.C.F. is 6","title":"Exercise02"},{"location":"exercises/python/exercise02/#solutions_for_exercises_-_python_basics","text":"","title":"Solutions for Exercises - Python Basics"},{"location":"exercises/python/exercise02/#exercise_question_1","text":"Write a Python program to find those numbers which are divisible by 7 and multiple of 5, between 1500 and 2700 (both included).","title":"Exercise Question 1"},{"location":"exercises/python/exercise02/#solution_for_question_1","text":"1 2 3 4 for i in range ( 1500 , 2700 ): if i % 7 == 0 : if i % 5 == 0 : print ( i ) 1505 1540 1575 1610 1645 1680 1715 1750 1785 1820 1855 1890 1925 1960 1995 2030 2065 2100 2135 2170 2205 2240 2275 2310 2345 2380 2415 2450 2485 2520 2555 2590 2625 2660 2695","title":"Solution for Question 1"},{"location":"exercises/python/exercise02/#exercise_question_2","text":"Write a Python program to construct the following pattern, using a nested for loop.\u00c2\u00b6","title":"Exercise Question 2"},{"location":"exercises/python/exercise02/#solution_for_question_2","text":"1 2 3 4 5 6 7 8 9 for i in range ( 1 , 10 ): if i > 5 : for j in range ( 1 , 5 ): print ( '* ' * ( i - ( j * 2 ))) i += 1 continue break else : print ( '* ' * i ) * * * * * * * * * * * * * * * * * * * * * * * * *","title":"Solution for Question 2"},{"location":"exercises/python/exercise02/#exercise_question_3","text":"Write a Python program to count the number of even and odd numbers from a series of numbers.","title":"Exercise Question 3"},{"location":"exercises/python/exercise02/#solution_for_question_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 list_odd_even = list ( range ( 1 , 10 )) even_num = 0 odd_num = 0 for i in list_odd_even : if i % 2 == 0 : even_num += 1 else : odd_num += 1 print ( \"Number of even numbers :\" , even_num ) print ( \"Number of odd numbers :\" , odd_num ) Number of even numbers : 4 Number of odd numbers : 5","title":"Solution for Question 3"},{"location":"exercises/python/exercise02/#exercise_question_4","text":"Write a Python program to find numbers between 100 and 400 (both included) where each digit of a number is an even number. The numbers obtained should be printed in a comma-separated sequence","title":"Exercise Question 4"},{"location":"exercises/python/exercise02/#solution_for_question_4","text":"1 2 3 4 5 6 7 8 9 10 range_100_400 = list ( range ( 100 , 400 )) list_output = [] for i in range_100_400 : if int ( str ( i )[ 0 ]) % 2 == 0 : if int ( str ( i )[ 1 ]) % 2 == 0 : if int ( str ( i )[ 2 ]) % 2 == 0 : list_output . append ( i ) print ( list_output ) [200, 202, 204, 206, 208, 220, 222, 224, 226, 228, 240, 242, 244, 246, 248, 260, 262, 264, 266, 268, 280, 282, 284, 286, 288]","title":"Solution for Question 4"},{"location":"exercises/python/exercise02/#exercise_question_5","text":"Write a Python program to calculate a dog's age in dog's years. Note: For the first two years, a dog year is equal to 10.5 human years. After that, each dog year equals 4 human years.","title":"Exercise Question 5"},{"location":"exercises/python/exercise02/#solution_for_question_5","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 human_years = 0 dog_years = 0 dog_age_input = input ( \"Input a dog's age in human years:\" ) if int ( dog_age_input ) > 2 : dog_years = int ( dog_age_input ) - 2 human_years += 2 * 10.5 human_years += dog_years * 4 elif int ( dog_age_input ) < 3 : if int ( dog_age_input ) == 2 : human_years += 2 * 10.5 else : human_years += 10.5 print ( \"The dog's age in dog's years is\" , int ( human_years )) Input a dog's age in human years:20 The dog's age in dog's years is 93","title":"Solution for Question 5"},{"location":"exercises/python/exercise02/#exercise_question_6","text":"Write a Python function to find the Max of three numbers","title":"Exercise Question 6"},{"location":"exercises/python/exercise02/#solution_for_question_6","text":"1 2 3 4 def find_max ( list_of_num ): return max ( list_of_num ) find_max ([ 3 , 6 , - 5 ]) 6","title":"Solution for Question 6"},{"location":"exercises/python/exercise02/#exercise_question_7","text":"Write a Python function that takes a number as a parameter and check the number is prime or not","title":"Exercise Question 7"},{"location":"exercises/python/exercise02/#solution_for_question_7","text":"1 2 3 4 5 6 7 8 9 10 def prime ( num ): factor = 0 for i in range ( 2 , num + 1 ): if num % i == 0 : factor += 1 print ( i , factor ) if factor > 1 : return False return True","title":"Solution for Question 7"},{"location":"exercises/python/exercise02/#exercise_question_8","text":"Write a Python function that accepts a string and calculate the number of upper case letters and lower case letters","title":"Exercise Question 8"},{"location":"exercises/python/exercise02/#solution_for_question_8","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def upper_lower_case_letters ( input_str ): upper_case_letters = 0 lower_case_letters = 0 for i in range ( 0 , len ( input_str )): if input_str [ i ] != \" \" : if input_str [ i ] . isupper (): upper_case_letters += 1 else : lower_case_letters += 1 return upper_case_letters , lower_case_letters uc , lc = upper_lower_case_letters ( \"Inceptz is One of The BeSt Places TO LEarn DataSciEnce\" ) print ( \"No of upper case characters :\" , uc ) print ( \"No of lower case characters :\" , lc ) No of upper case characters : 13 No of lower case characters : 32","title":"Solution for Question 8"},{"location":"exercises/python/exercise02/#exercise_question_9","text":"Write a Python program to reverse a string.","title":"Exercise Question 9"},{"location":"exercises/python/exercise02/#solution_for_question_9","text":"1 2 given_string = \"Hello, World!\" print ( given_string [ - 1 :: - 1 ]) !dlroW ,olleH","title":"Solution for Question 9"},{"location":"exercises/python/exercise02/#exercise_question_10","text":"Write a Python program to find the greatest common divisor (gcd) of two integers.","title":"Exercise Question 10"},{"location":"exercises/python/exercise02/#solution_for_question_10","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Python program to find H.C.F of two numbers # define a function def compute_hcf ( x , y ): # choose the smaller number if x > y : smaller = y else : smaller = x for i in range ( 1 , smaller + 1 ): if (( x % i == 0 ) and ( y % i == 0 )): hcf = i return hcf num1 = 54 num2 = 24 print ( \"The H.C.F. is\" , compute_hcf ( num1 , num2 )) The H.C.F. is 6","title":"Solution for Question 10"},{"location":"probability/","text":"Probability Index #","title":"_Probability Home"},{"location":"probability/#probability_index","text":"","title":"Probability Index"},{"location":"probability/probability_theory/","text":"Probability # A probability is a quantitative measure of uncertainity . Basic Definitions # Set A set is a collection of elements Empty set A set containing no elements Universal set A set containing all elements. Represented by S Compliment of a set The complement of set A is the set containing all the elements in the universal set S that are not members of set A. Represented by \\bar{A} \\bar{A} Intersection of a set Denoted by A \\cap B A \\cap B , the intersection is the set containing all elements that are members present in both A and B. Union of a set Denoted by A \\cup B A \\cup B , is the set containing all the elements that are members of both A and B Experiment and Outcome An experiment is a process that leads to one or several possible outcomes. An outcome of an experiment is some observation or measurement. Example Drawing a card out of a deck of 52 cards is an experiment. One outcome of the experiment may be that the queen of diamonds is drawn. Sample space Sample space is the set of all possible outcomes of an experiment. Example The sample space for the experiment of drawing a card out of a deck is the set of all cards in the deck. Event An event is a subset of a sample space. Example An ace is drawn out of a deck of cards. Probability of event A P(A) = \\frac{n(A)}{n(S)} P(A) = \\frac{n(A)}{n(S)} where n(A)= n(A)= the number of elements in the set of the event A n(S)= n(S)= the number of elements in the sample space S Example The probability of drawing an Ace is P(A) = \\frac{n(A)}{n(S)} = \\frac{4}{52} P(A) = \\frac{n(A)}{n(S)} = \\frac{4}{52} Note: There are 4 Ace's in a deck of cards - Spade, Club, Heart, and Diamond Practice Roulette is a popular casino game. As the game is played in Las Vegas or Atlantic City, the roulette wheel has 36 numbers, 1 through 36, and the number 0 as well as the number 00 (double zero). What is the probability of winning on a single number that you bet? Solution: total elements in the set = 36 + 2 (include 0 and 00) = 38 Probability of winning a single number = \\frac{1}{38} \\frac{1}{38} Basic rules of probability # The range of values For any event A, the probability P(A) P(A) lies between 0 and 1. i.e. 0 \\le P(A) \\le 1 0 \\le P(A) \\le 1 When a probability cannot occur, the probability is zero. Example What is the probability of drawing a green card from the deck of cards? It's 0 as the deck of cards has only red/black cards. When a probability is certain to occur, the probability is one. Rule of complements Probability of a complement is P(\\bar{A}) = 1 - P(A) P(\\bar{A}) = 1 - P(A) Example The probability of rain tomorrow is 0.3. This means the probability of no rain is 1 - 0.3 = 0.7 Rule of unions The rule of unions: P(A \\cup B) = P(A) + P(B) - P(A \\cap B) P(A \\cup B) = P(A) + P(B) - P(A \\cap B) Example The probability of an Ace in a deck of cards is \\frac{4}{52} \\frac{4}{52} and the probability of a heart in a deck of cards is \\frac{13}{52} \\frac{13}{52} . The probability of an ace with heart is \\frac{1}{52} \\frac{1}{52} So P(A \\cup \\heartsuit) = \\frac{4}{52} + \\frac{13}{52} - \\frac{1}{52} = \\frac{16}{52} P(A \\cup \\heartsuit) = \\frac{4}{52} + \\frac{13}{52} - \\frac{1}{52} = \\frac{16}{52} Mutually exclusive events # When the sets corresponding to two events are disjoint (i.e. have not an intersection), the two events are called mutually exclusive or in other words, they can't occur together. To simplify, events are mutually exclusive if they cannot occur simultaneously. Intersection: For mutually exclusive events A and B: P(A \\cap B) = 0 P(A \\cap B) = 0 Since there is no intersection, it is always 0 Union: For mutualy exclusive events A and B: P(A \\cup B) = P(A) + P(B) P(A \\cup B) = P(A) + P(B) Note Since there is no intersection, P(A \\cup B) = P(A) + P(B) - P(A \\cap B) P(A \\cup B) = P(A) + P(B) - P(A \\cap B) is reduced to P(A \\cup B) = P(A) + P(B) P(A \\cup B) = P(A) + P(B) Example What is the probability of drawing either a heart or a club? The probability of drawing a heart is \\frac{13}{52} \\frac{13}{52} . The probability of drawing a club is \\frac{13}{52} \\frac{13}{52} . Since they are mutually exclusive, P(\\heartsuit \\cup \\clubsuit) = P(\\heartsuit) + P(\\clubsuit) P(\\heartsuit \\cup \\clubsuit) = P(\\heartsuit) + P(\\clubsuit) which is \\frac{13}{52} + \\frac{13}{52} = \\frac{26}{52} = \\frac{1}{2} \\frac{13}{52} + \\frac{13}{52} = \\frac{26}{52} = \\frac{1}{2} Conditional probability # The probability depends on information. Say for example - what is the probability that the company Mannar & Co's stock price would go up? Well, it would depend on the information we have about the company like it's performance in recent times. So the probability of the stock price going up depends upon some information. This is called as conditional probability Formula The conditional probability of an event A given the occurrence of event B is P(A|B) = \\frac{P(A \\cap B)}{P(B)} P(A|B) = \\frac{P(A \\cap B)}{P(B)} The vertical line in P(A|B) is read given or conditional upon . Example A consulting firm is bidding for two jobs, one with each of two large multinational corporations. The company executives estimate that the probability of obtaining the consulting job with firm A, event A, is 0.45. The executives also feel that if the company should get the job with firm A, then there is a 0.90 probability that firm B will also give the company the consulting job. What are the company\u2019s chances of getting both jobs? Solution: P(A) = 0.45 \\\\ P(B|A) = 0.90 P(A) = 0.45 \\\\ P(B|A) = 0.90 We are looking for P(A \\cap B) P(A \\cap B) - i.e. chance of getting both jobs We know the formula is P(B|A) = \\frac{P(B \\cap A)}{P(A)} P(B|A) = \\frac{P(B \\cap A)}{P(A)} So the formula becomes, P(B \\cap A) = P(B|A).P(A) P(B \\cap A) = P(B|A).P(A) which is 0.90 * 0.45 = 0.405 0.90 * 0.45 = 0.405 Note Conditional probability is not symmetrical P(A|B) \\ne P(B|A) P(A|B) \\ne P(B|A) Independence of events # As an example of independent events, consider the following: Suppose I roll a single die. What is the probability that the number 6 will turn up? The answer is \\frac{1}{6} \\frac{1}{6} . Now suppose that I told you that I just tossed a coin and it turned up heads. What is now the probability that the die will show the number 6? The answer is unchanged, \\frac{1}{6} \\frac{1}{6} , because events of the die and the coin are independent of each other. We see that P(6 | H) = P(6) P(6 | H) = P(6) So when do we say that the events are independent of each other? Conditions for the independence of two events A and B The events are said to be independent when they meet the below conditions: P(A|B) = P(A) \\\\ P(B|A) = P(B) \\\\ and \\\\ P(A \\cap B) = P(A)P(B) P(A|B) = P(A) \\\\ P(B|A) = P(B) \\\\ and \\\\ P(A \\cap B) = P(A)P(B) The first equation P(A|B)=P(A) P(A|B)=P(A) implies that the probability of A would not be affected by the probability of B as they are both independent. So the probability of A would remain the same. Similarly, the second equation P(B|A)=P(B) P(B|A)=P(B) implies that the probability of B would remain the same as B is not affected by the probability of A since both are independent. The third equation P(A \\cap B)= P(A)P(B) P(A \\cap B)= P(A)P(B) is called the intersection rule for independent events . Intersection rule for independent events P(A \\cap B)= P(A)P(B) P(A \\cap B)= P(A)P(B) The probability of the intersection of several independent events is just the product of the separate probabilities. Let's understand this rule with an example. Example The rate of defects in corks of wine bottles is very high say 75%. Assuming independence, if four bottles are opened, what is the probability that all four corks are defective? Solution: P (all 4 are defective) = P (first cork is defective) * P (second cork is defective) * P (third cork is defective) * P (fourth cork is defective) which is equal to P (all 4 are defective) = 0.75 * 0.75 * 0.75 * 0.75 = 0.316 Tip Whenever a random sampling is done, it would mean independence - as the outcome of one event will not be dependent on the outcome of another event. Question What is the difference between a mutually exclusive event and an independent event? mutually exclusive - Two events cannot occur at the same time. e.g., the head & tail of a coin cannot occur at the same time. They're dependent in fact. independent - Two events can occur at the same time and they are not dependent on each other. e.g., events of tossing a coin & events of rolling a dice at the same time are independent. Bayes Theorem # Bayes theorem is a technique for calculating a conditional probability. Derivation of Bayes Theorem # We have already seen the conditional probability which is Equation 1: P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\space provided \\space that \\space P(B) > 0 P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\space provided \\space that \\space P(B) > 0 Equation 2: This also means that P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\space provided \\space that \\space P(A) > 0 P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\space provided \\space that \\space P(A) > 0 So from equation 2, we can say that (Equation 3) P(B \\cap A) = P(B|A)P(A) P(B \\cap A) = P(B|A)P(A) We also know that P(A \\cap B) = P(B \\cap A) P(A \\cap B) = P(B \\cap A) So substituting the Equation 3 in Equation 1, we get the Bayes theorem. P(A|B) = \\frac{P(B|A).P(A)}{P(B)} \\space provided \\space that \\space P(A), P(B) > 0 P(A|B) = \\frac{P(B|A).P(A)}{P(B)} \\space provided \\space that \\space P(A), P(B) > 0 Example # Refer to Example 1 (Elderly fall and death) in this article for how to apply the Bayes theorem Summary # In this post, we had seen what probability is, definition of terms used in probability, mutually-exclusive and independent events, conditional probability and then the Bayes theorem.","title":"Probability theory"},{"location":"probability/probability_theory/#probability","text":"A probability is a quantitative measure of uncertainity .","title":"Probability"},{"location":"probability/probability_theory/#basic_definitions","text":"","title":"Basic Definitions"},{"location":"probability/probability_theory/#basic_rules_of_probability","text":"","title":"Basic rules of probability"},{"location":"probability/probability_theory/#mutually_exclusive_events","text":"When the sets corresponding to two events are disjoint (i.e. have not an intersection), the two events are called mutually exclusive or in other words, they can't occur together. To simplify, events are mutually exclusive if they cannot occur simultaneously. Intersection: For mutually exclusive events A and B: P(A \\cap B) = 0 P(A \\cap B) = 0 Since there is no intersection, it is always 0 Union: For mutualy exclusive events A and B: P(A \\cup B) = P(A) + P(B) P(A \\cup B) = P(A) + P(B) Note Since there is no intersection, P(A \\cup B) = P(A) + P(B) - P(A \\cap B) P(A \\cup B) = P(A) + P(B) - P(A \\cap B) is reduced to P(A \\cup B) = P(A) + P(B) P(A \\cup B) = P(A) + P(B) Example What is the probability of drawing either a heart or a club? The probability of drawing a heart is \\frac{13}{52} \\frac{13}{52} . The probability of drawing a club is \\frac{13}{52} \\frac{13}{52} . Since they are mutually exclusive, P(\\heartsuit \\cup \\clubsuit) = P(\\heartsuit) + P(\\clubsuit) P(\\heartsuit \\cup \\clubsuit) = P(\\heartsuit) + P(\\clubsuit) which is \\frac{13}{52} + \\frac{13}{52} = \\frac{26}{52} = \\frac{1}{2} \\frac{13}{52} + \\frac{13}{52} = \\frac{26}{52} = \\frac{1}{2}","title":"Mutually exclusive events"},{"location":"probability/probability_theory/#conditional_probability","text":"The probability depends on information. Say for example - what is the probability that the company Mannar & Co's stock price would go up? Well, it would depend on the information we have about the company like it's performance in recent times. So the probability of the stock price going up depends upon some information. This is called as conditional probability","title":"Conditional probability"},{"location":"probability/probability_theory/#independence_of_events","text":"As an example of independent events, consider the following: Suppose I roll a single die. What is the probability that the number 6 will turn up? The answer is \\frac{1}{6} \\frac{1}{6} . Now suppose that I told you that I just tossed a coin and it turned up heads. What is now the probability that the die will show the number 6? The answer is unchanged, \\frac{1}{6} \\frac{1}{6} , because events of the die and the coin are independent of each other. We see that P(6 | H) = P(6) P(6 | H) = P(6) So when do we say that the events are independent of each other?","title":"Independence of events"},{"location":"probability/probability_theory/#bayes_theorem","text":"Bayes theorem is a technique for calculating a conditional probability.","title":"Bayes Theorem"},{"location":"probability/probability_theory/#derivation_of_bayes_theorem","text":"We have already seen the conditional probability which is Equation 1: P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\space provided \\space that \\space P(B) > 0 P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\space provided \\space that \\space P(B) > 0 Equation 2: This also means that P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\space provided \\space that \\space P(A) > 0 P(B|A) = \\frac{P(B \\cap A)}{P(A)} \\space provided \\space that \\space P(A) > 0 So from equation 2, we can say that (Equation 3) P(B \\cap A) = P(B|A)P(A) P(B \\cap A) = P(B|A)P(A) We also know that P(A \\cap B) = P(B \\cap A) P(A \\cap B) = P(B \\cap A) So substituting the Equation 3 in Equation 1, we get the Bayes theorem. P(A|B) = \\frac{P(B|A).P(A)}{P(B)} \\space provided \\space that \\space P(A), P(B) > 0 P(A|B) = \\frac{P(B|A).P(A)}{P(B)} \\space provided \\space that \\space P(A), P(B) > 0","title":"Derivation of Bayes Theorem"},{"location":"probability/probability_theory/#example","text":"Refer to Example 1 (Elderly fall and death) in this article for how to apply the Bayes theorem","title":"Example"},{"location":"probability/probability_theory/#summary","text":"In this post, we had seen what probability is, definition of terms used in probability, mutually-exclusive and independent events, conditional probability and then the Bayes theorem.","title":"Summary"},{"location":"python/","text":"Python Index #","title":"_Python Home"},{"location":"python/#python_index","text":"","title":"Python Index"},{"location":"python/numpy-basics/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); By hazeez Apr 14, 2020 in Python Numpy arrays are used for manipulation of data. Some of the common operations that can be done by the numpy arrays are - split, reshape and join the arrays. The types of operations shown below may be dry as we don't get to see the use cases immediately, the below operations forms as the building blocks for real use-case scenarios when we encounter them. Importing Numpy 1 2 3 # importing numpy with a common alias np import numpy as np Getting the numpy version 1 2 3 4 # numpy version # The double underscore is referred to as dunder print ( np . __version__ ) 1.18.1 Defining a numpy array 1 2 3 # defining a numpy array arr = np . array ([ 1 , 2 , 58 ]) 1 2 3 # Getting the type of the array type ( arr ) numpy.ndarray This shows that it's an 'n' dimension array. Let's check how many dimension it has - using the ndim function 1 2 3 # Getting the number of dimensions of an array arr . ndim 1 The above array 'arr' has got only one dimensions as it has just one row Let's define a few arrays - one dimension, two dimension, three dimension arrays The random module in numpy is used to generate a sequence of random numbres. We will use the random module to generate the arrays 1 2 3 # use a seed value for reproducibility np . random . seed ( 0 ) 1 2 3 4 # Generate 6 values within the range of 0 to 10 x1 = np . random . randint ( 10 , size = 6 ) # one-dimension array print ( x1 ) [8 8 1 6 7 7] 1 2 3 4 5 # Generate an array of size - 3 rows and 4 columns # with a range of values 0 to 10 x2 = np . random . randint ( 10 , size = ( 3 , 4 )) # two-dimension array print ( x2 ) [[8 1 5 9] [8 9 4 3] [0 3 5 0]] 1 2 3 4 # Generate an array of size - 3 elements of size 4 rows and 5 column arrays x3 = np . random . randint ( 10 , size = ( 3 , 4 , 5 )) # three dimension array print ( x3 ) [[[5 9 3 0 5] [0 1 2 4 2] [0 3 2 0 7] [5 9 0 2 7]] [[2 9 2 3 3] [2 3 4 1 2] [9 1 4 6 8] [2 3 0 0 6]] [[0 6 3 3 8] [8 8 2 3 2] [0 8 8 3 8] [2 8 4 3 0]]] Array attributes Each array has attributes ndim (the number of dimensions), shape (the size of each dimension), and size (the total size of the array): 1 2 3 4 5 # attributes of the one dimension array print ( \"x1 ndim: \" , x1 . ndim ) print ( \"x1 shape:\" , x1 . shape ) print ( \"x1 size: \" , x1 . size ) x1 ndim: 1 x1 shape: (6,) x1 size: 6 1 2 3 4 5 # attributes of the two dimension array print ( \"x2 ndim: \" , x2 . ndim ) print ( \"x2 shape:\" , x2 . shape ) print ( \"x2 size: \" , x2 . size ) x2 ndim: 2 x2 shape: (3, 4) x2 size: 12 1 2 3 4 5 # attributes of the three dimension array print ( \"x3 ndim: \" , x3 . ndim ) print ( \"x3 shape:\" , x3 . shape ) print ( \"x3 size: \" , x3 . size ) x3 ndim: 3 x3 shape: (3, 4, 5) x3 size: 60 Another useful attribute is the dtype , the data type of the array 1 2 3 # Getting the type of the array print ( 'dtype:' , x3 . dtype ) dtype: int32 Other attributes include itemsize , which lists the size (in bytes) of each array element, and nbytes , which lists the total size (in bytes) of the array: 1 2 3 4 # getting the itemsize and the number of bytes of the array print ( \"itemsize:\" , x3 . itemsize , \"bytes\" ) print ( \"nbytes:\" , x3 . nbytes , \"bytes\" ) itemsize: 4 bytes nbytes: 240 bytes In general, we expect that nbytes is equal to itemsize times size . Array Indexing: Accessing single elements # 1 2 3 # Get the value of the x1 array x1 array([8, 8, 1, 6, 7, 7]) 1 2 3 # Access the first element x1 [ 0 ] 8 1 2 3 # Access the last element x1 [ - 1 ] 7 In a multi-dimensional array, items can be accessed using a comma-separated tuple of indices: 1 2 3 # Get the value of x2 x2 array([[8, 1, 5, 9], [8, 9, 4, 3], [0, 3, 5, 0]]) 1 2 3 # Access the first row, first element x2 [ 0 , 0 ] 8 1 2 3 # Access the second row, third element x2 [ 1 , 2 ] 4 1 2 3 # Access the last row, last before element x2 [ 2 , - 2 ] 5 Values can also be modified using any of the above index notation 1 2 3 4 5 6 7 8 9 10 11 12 13 # Let's change the first row, first element to 12 # printing the previous x2 print ( x2 ) # change the first row, first element to 12 x2 [ 0 , 0 ] = 12 # Get the new x2 print ( \"---\" ) print ( x2 ) [[8 1 5 9] [8 9 4 3] [0 3 5 0]] --- [[12 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] Keep in mind that, unlike Python lists, NumPy arrays have a fixed type. This means, for example, that if you attempt to insert a floating-point value to an integer array, the value will be silently truncated. Don't be caught unaware by this behavior! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Lets change the first row, first element to # a floating point number e.g. 3.1415 # print old x2 print ( x2 ) # Assign a floating point number to the first element # This will be truncated x2 [ 0 , 0 ] = 3.1415 print ( \"---\" ) # print new x2 print ( x2 ) [[3 1 5 9] [8 9 4 3] [0 3 5 0]] --- [[3 1 5 9] [8 9 4 3] [0 3 5 0]] You can see that thought we have assigned the number as 3.1415 , it has been truncated to 3 as the array x2 is of integer type and hence cannot take floating point numbers. Array Slicing: Accessing Subarrays # Just as we can use square brackets to access individual array elements, we can also use them to access subarrays with the slice notation, marked by the colon ( : ) character. The NumPy slicing syntax follows that of the standard Python list; to access a slice of an array x , use this: x[start:stop:step] If any of these are unspecified, they default to the values start=0 , stop=size of dimension , step=1 . We'll take a look at accessing sub-arrays in one dimension and in multiple dimensions. 1 2 3 4 # Let's define an one dimension array x = np . arange ( 10 ) print ( x ) [0 1 2 3 4 5 6 7 8 9] 1 2 3 # Get the first five elements x [: 5 ] array([0, 1, 2, 3, 4]) 1 2 3 # elements after index 5 x [ 5 :] array([5, 6, 7, 8, 9]) 1 2 3 # slicing from the middle x [ 4 : 7 ] array([4, 5, 6]) 1 2 3 # Every second element x [:: 2 ] array([0, 2, 4, 6, 8]) 1 2 3 # Every second element starting from 1 x [ 1 :: 2 ] array([1, 3, 5, 7, 9]) A potentially confusing case is when the step value is negative. In this case, the defaults for start and stop are swapped. This becomes a convenient way to reverse an array: 1 2 3 # reversing the entire array x [:: - 1 ] array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) 1 2 3 4 # reversing every other element from index 5 # remember to sway the start and stop position x [ 5 :: - 2 ] array([5, 3, 1]) Multi-Dimension Subarrays # Multi-dimensional slices work in the same way, with multiple slices separated by commas. 1 2 3 # Let's get x2 we defined earlier x2 array([[3, 1, 5, 9], [8, 9, 4, 3], [0, 3, 5, 0]]) 1 2 3 # Getting the first two rows and three columns x2 [: 2 ,: 3 ] array([[3, 1, 5], [8, 9, 4]]) 1 2 3 # Getting all rows and every other column x2 [:,:: 2 ] array([[3, 5], [8, 4], [0, 5]]) Since the range is for all rows, we don't give the start and the stop range. For columns, since we need every other column in the range, we dont specify the start and the stop range but just the step Subarray dimensions can be reversed as well. 1 2 3 # Printing original array x2 array([[3, 1, 5, 9], [8, 9, 4, 3], [0, 3, 5, 0]]) 1 2 3 # reverse the array x2 [:: - 1 , :: - 1 ] array([[0, 5, 3, 0], [3, 4, 9, 8], [9, 5, 1, 3]]) Accessing array rows and columns # One commonly needed routine is accessing of single rows or columns of an array. This can be done by combining indexing and slicing, using an empty slice marked by a single colon ( : ): 1 2 3 # Get the x2 array x2 array([[3, 1, 5, 9], [8, 9, 4, 3], [0, 3, 5, 0]]) 1 2 3 # Get the first column of x2 x2 [:, 0 ] array([3, 8, 0]) 1 2 3 # get the first row of x2 x2 [ 0 ,:] array([3, 1, 5, 9]) In case of row access, the empty slice can be omitted for a more compact syntax 1 2 3 # equivalent to x2[0, :] print ( x2 [ 0 ]) [3 1 5 9] Subarrays as no-copy views # One important and extremely useful thing to know about array slices is that they return views rather than copies of the array data. This is one area in which NumPy array slicing differs from Python list slicing: in lists, slices will be copies. Consider our two dimensional array from before: 1 2 3 # get the x2 array print ( x2 ) [[3 1 5 9] [8 9 4 3] [0 3 5 0]] 1 2 3 4 # Let's extract a 2 x 2 subarray from x2 x2_sub = x2 [: 2 ,: 2 ] print ( x2_sub ) [[3 1] [8 9]] Now, if we modify the array, the original array is changed 1 2 3 4 # change the first row, first element to 99 x2_sub [ 0 , 0 ] = 99 print ( x2_sub ) [[99 1] [ 8 9]] 1 2 3 # Get the x2 array print ( x2 ) [[99 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] This default behavior is actually quite useful: it means that when we work with large datasets, we can access and process pieces of these datasets without the need to copy the underlying data buffer. Creating copies of arrays # Despite the nice features of array views, it is sometimes useful to instead explicitly copy the data within an array or a subarray. This can be most easily done with the copy() method: 1 2 3 # Let's start with the x2 array print ( x2 ) [[99 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] 1 2 3 4 # Let get the first two rows and two columns x2_sub_arr = x2 [: 2 ,: 2 ] print ( x2_sub_arr ) [[99 1] [ 8 9]] 1 2 3 4 5 # Let create a copy of this sub array # use the .copy method x2_sub_arr_copy = x2_sub_arr . copy () print ( x2_sub_arr_copy ) [[99 1] [ 8 9]] 1 2 3 4 # Let change the first row, first element to 0 x2_sub_arr_copy [ 0 , 0 ] = 0 print ( x2_sub_arr_copy ) [[0 1] [8 9]] 1 2 3 # Let's print the original x2 print ( x2 ) [[99 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] As you can see, the original x2 array still has the number 99 as the first element as the original array x2 is not touched. Reshaping for arrays # Another useful type of operation is reshaping of arrays. The most flexible way of doing this is with the reshape method. For example, if you want to put the numbers 1 through 9 in a 3 x 3 grid, you can do the following: 1 2 3 4 # Let's form a grid of 9 elements grid = np . arange ( 1 , 10 ) print ( grid ) [1 2 3 4 5 6 7 8 9] 1 2 3 # Let's reshape this to a 3 x 3 grid grid . reshape ( 3 , 3 ) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Note that for this to work, the size of the initial array must match the size of the reshaped array. We can also reshape a one-dimension array 1 2 3 # Let's declare a one-dimension array x = np . array ([ 1 , 2 , 3 ]) 1 2 3 4 5 # Let's re-shape this to a 1,3 array # i.e. 1 row, 3 columns array x . reshape ( 1 , 3 ) print ( x ) [1 2 3] 1 2 3 4 # Let's re-shape x to a 3,1 array # i.e. 3 rows and 1 column array x . reshape ( 3 , 1 ) array([[1], [2], [3]]) 1 2 3 4 # The other way of doing this # is by using .newaxis method x [:, np . newaxis ] array([[1], [2], [3]]) 1 2 3 # To change back to 1 row, 3 columns x [ np . newaxis , :] array([[1, 2, 3]]) Array concatenation # All of the preceding routines worked on single arrays. It's also possible to combine multiple arrays into one, and to conversely split a single array into multiple arrays. Concatenation, or joining of two arrays in NumPy, is primarily accomplished using the routines np.concatenate , np.vstack , and np.hstack 1 2 3 4 5 6 # np.concatenate x = np . array ([ 1 , 2 , 3 ]) y = np . array ([ 4 , 5 , 6 ]) np . concatenate ([ x , y ]) # Note the arrays are within [ ] array([1, 2, 3, 4, 5, 6]) 1 2 3 4 5 # More than two arrays can also be concatenated z = [ 99 , 99 , 99 ] np . concatenate ([ x , y , z ]) array([ 1, 2, 3, 4, 5, 6, 99, 99, 99]) 1 2 3 4 5 6 7 # Concatenating two-dimension arrays grid = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ]]) # concatenating along the first axis i.e. columns np . concatenate ([ grid , grid ]) array([[1, 2, 3], [4, 5, 6], [1, 2, 3], [4, 5, 6]]) 1 2 3 # concatenating along the second axis i.e rows np . concatenate ([ grid , grid ], axis = 1 ) array([[1, 2, 3, 1, 2, 3], [4, 5, 6, 4, 5, 6]]) For working with arrays of mixed dimensions , it can be clearer to use the np.vstack (vertical stack) and np.hstack (horizontal stack) functions: 1 2 3 4 5 6 7 8 9 10 # lets define a single dimension array x = np . array ([ 1 , 2 , 3 ]) # let's define a two dimension array grid = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ]]) print ( x ) print ( grid ) [1 2 3] [[1 2 3] [4 5 6]] 1 2 3 4 # let's vertically stack the arrays # use the vstack method np . vstack ([ x , grid ]) array([[1, 2, 3], [1, 2, 3], [4, 5, 6]]) 1 2 3 4 5 6 7 # let's stack the arrays horizontally # use the hstack method y = np . array ([[ 99 ], [ 99 ]]) np . hstack ([ grid , y ]) array([[ 1, 2, 3, 99], [ 4, 5, 6, 99]]) Array Splitting # The opposite of concatenation is splitting, which is implemented by the functions np.split , np.hsplit , and np.vsplit 1 2 3 4 5 6 7 8 9 10 11 12 # Let's define a array x = [ 1 , 2 , 3 , 99 , 99 , 3 , 2 , 1 ] # split from index 3 to 5 # this would give three sub arrays # [1,2,3] - [99,99] - [3,2,1] # Get these 3 sub arrays in 3 variables x_1 , x_2 , x_3 = np . split ( x , [ 3 , 5 ]) print ( x_1 , x_2 , x_3 ) [1 2 3] [99 99] [3 2 1] 1 2 3 4 5 6 # vertically splitting an array # Let's make a multi-dim array grid = np . arange ( 16 ) . reshape ( 4 , 4 ) print ( grid ) [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]] 1 2 3 4 5 6 # use the vsplit method to split the array # vertically upper , lower = np . vsplit ( grid , [ 2 ]) print ( upper ) print ( lower ) [[0 1 2 3] [4 5 6 7]] [[ 8 9 10 11] [12 13 14 15]] 1 2 3 4 5 6 # use the hsplit method to split # the array horizontally left , right = np . hsplit ( grid , [ 2 ]) print ( left ) print ( right ) [[ 0 1] [ 4 5] [ 8 9] [12 13]] [[ 2 3] [ 6 7] [10 11] [14 15]] Iterating thru arrays # 1 2 3 4 5 6 7 8 # Let get the array x # and iterate thru it print ( x ) # 1D array for i in x : print ( i ) [1, 2, 3, 99, 99, 3, 2, 1] 1 2 3 99 99 3 2 1 1 2 3 4 5 6 7 8 # Lets get the 2d array x2 # and iterate thru it print ( x2 ) for i in x2 : for j in i : print ( j ) [[99 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] 99 1 5 9 8 9 4 3 0 3 5 0 1 2 3 4 5 6 7 8 9 10 11 # Let's get the 3d array x3 # and iterate thru it print ( x3 ) print ( \"---\" ) for i in x3 : for j in i : print ( j ) # for k in j: print(k) [[[5 9 3 0 5] [0 1 2 4 2] [0 3 2 0 7] [5 9 0 2 7]] [[2 9 2 3 3] [2 3 4 1 2] [9 1 4 6 8] [2 3 0 0 6]] [[0 6 3 3 8] [8 8 2 3 2] [0 8 8 3 8] [2 8 4 3 0]]] --- [5 9 3 0 5] [0 1 2 4 2] [0 3 2 0 7] [5 9 0 2 7] [2 9 2 3 3] [2 3 4 1 2] [9 1 4 6 8] [2 3 0 0 6] [0 6 3 3 8] [8 8 2 3 2] [0 8 8 3 8] [2 8 4 3 0] Addition, subtraction, multiplication in a numpy array # 1 2 3 4 5 # Let's define the first array # of 16 elements arr1 = np . array ( range ( 1 , 17 , 1 )) print ( arr1 ) [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16] 1 2 3 4 # Let's reshape it to a 4 x 4 matrix arr1 = arr1 . reshape ( 4 , 4 ) print ( arr1 ) [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12] [13 14 15 16]] 1 2 3 4 5 # Let's define the second array # of 16 elements with difference 10 arr2 = np . array ( range ( 10 , 170 , 10 )) print ( arr2 ) [ 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160] 1 2 3 4 # Let's reshape it to a 4 x 4 matrix arr2 = arr2 . reshape ( 4 , 4 ) print ( arr2 ) [[ 10 20 30 40] [ 50 60 70 80] [ 90 100 110 120] [130 140 150 160]] 1 2 3 # addition of two arrays arr1 + arr2 array([[ 11, 22, 33, 44], [ 55, 66, 77, 88], [ 99, 110, 121, 132], [143, 154, 165, 176]]) 1 2 3 # subtracting two arrays arr2 - arr1 array([[ 9, 18, 27, 36], [ 45, 54, 63, 72], [ 81, 90, 99, 108], [117, 126, 135, 144]]) 1 2 3 # Multiplying two arrays arr1 * arr2 array([[ 10, 40, 90, 160], [ 250, 360, 490, 640], [ 810, 1000, 1210, 1440], [1690, 1960, 2250, 2560]]) 1 2 3 # Dividing the arrays arr2 / arr1 array([[10., 10., 10., 10.], [10., 10., 10., 10.], [10., 10., 10., 10.], [10., 10., 10., 10.]]) 1 2 3 4 # array multiplication with a scalar value # e.g arr1 * 10 arr1 * 10 array([[ 10, 20, 30, 40], [ 50, 60, 70, 80], [ 90, 100, 110, 120], [130, 140, 150, 160]]) 1 2 3 4 # dot product # dot product is different from multiplication np . dot ( arr1 , arr2 ) array([[ 900, 1000, 1100, 1200], [2020, 2280, 2540, 2800], [3140, 3560, 3980, 4400], [4260, 4840, 5420, 6000]]) Sorting and searching # 1 2 3 # Let's define an array print ( x ) [1, 2, 3, 99, 99, 3, 2, 1] 1 2 3 # sorting 1D array np . sort ( x ) array([ 1, 1, 2, 2, 3, 3, 99, 99]) 1 2 3 # Lets get a 2d array print ( x2 ) [[99 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] 1 2 3 4 # sorting a 2d array # will sort elements in each rows np . sort ( x2 ) array([[ 1, 5, 9, 99], [ 3, 4, 8, 9], [ 0, 0, 3, 5]]) Statistics with numpy # 1 2 3 4 # Let's define an array stat_arr = np . arange ( 1 , 10 ) print ( stat_arr ) [1 2 3 4 5 6 7 8 9] 1 2 3 # Get the mean of the array np . mean ( stat_arr ) 5.0 1 2 3 # Get the median of the array np . median ( stat_arr ) 5.0 1 2 3 # get the std dev of the array np . std ( stat_arr ) 2.581988897471611 1 2 3 # get the variance np . var ( stat_arr ) 6.666666666666667 1 2 3 4 # get the percentile scores = [ 80 , 90 , 70 , 85 , 60 , 55 , 75 , 89 , 12 , 13 , 15 ] np . percentile ( scores , 80 ) 85.0 1 2 3 4 5 6 7 8 # mean of a 2d array print ( x2 ) # getting the mean across rows # axis = 1 np . mean ( x2 , axis = 1 ) [[99 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] array([28.5, 6. , 2. ]) 1 2 3 # getting the mean across columns np . mean ( x2 , axis = 0 ) array([35.66666667, 4.33333333, 4.66666667, 4. ]) 1 2 3 4 5 6 # Drawing an histogram import matplotlib.pyplot as plt norm_data = np . random . normal ( 10 , 1.0 , 5000000 ) plt . hist ( norm_data , 200 ) # 200 is the number of bins plt . show () 1 2 3 4 # Getting a random integer # in a range np . random . randint ( 1 , 100 ) 91 1 2 3 4 5 # generating a random number # first param : range # second param : size np . random . rand ( 1 , 100 ) array([[0.47390871, 0.9424515 , 0.08067525, 0.50477582, 0.75355236, 0.6443007 , 0.68009591, 0.85948974, 0.79562236, 0.67030161, 0.21086869, 0.43761942, 0.14953803, 0.73310741, 0.0978983 , 0.78410335, 0.74009511, 0.23071498, 0.13325923, 0.19633374, 0.68353164, 0.23094352, 0.17681563, 0.14249063, 0.51898069, 0.21801461, 0.0914506 , 0.28495553, 0.09474098, 0.55464705, 0.94250659, 0.24068165, 0.80642805, 0.34753732, 0.16368083, 0.49636519, 0.22354509, 0.76651945, 0.03019415, 0.64006132, 0.035494 , 0.21898449, 0.51010665, 0.41423507, 0.91151493, 0.6051262 , 0.77595892, 0.43230851, 0.77152591, 0.85263481, 0.16105187, 0.06416947, 0.41576968, 0.28921585, 0.03823162, 0.11454612, 0.45932632, 0.78403473, 0.40116525, 0.94245161, 0.98495574, 0.40247529, 0.76547687, 0.76620485, 0.3622606 , 0.66219648, 0.0360825 , 0.23129832, 0.77402102, 0.23150597, 0.66776317, 0.15584173, 0.64016945, 0.69802422, 0.55093934, 0.80262937, 0.59689222, 0.55314997, 0.19660366, 0.25421008, 0.67910571, 0.75892023, 0.15447376, 0.52093862, 0.37407023, 0.88858665, 0.10001336, 0.63902172, 0.71561141, 0.44134696, 0.77242583, 0.08833249, 0.61827113, 0.22744314, 0.77190824, 0.78893133, 0.34247442, 0.35901415, 0.68427893, 0.2549437 ]]) 1 2 3 4 5 6 # getting a choice in a given range # random.choice takes two parameters # first param: list of numbers # second param: size in rows and columns np . random . choice ([ 1 , 3 , 5 , 7 , 10 ], size = ( 1 , 3 )) array([[ 7, 10, 10]]) 1 2 3 4 5 6 7 8 # plotting a chi-square graph import matplotlib.pyplot as plt import seaborn as sns chi = np . random . chisquare ( df = 1 , size = 100000 ) sns . distplot ( chi , hist = False ) plt . show () Functions involved in generating array # 1 2 3 # Let's create an array and reshape it a = np . arange ( 1 , 17 ) . reshape ( 4 , 4 ) 1 print ( a ) [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12] [13 14 15 16]] 1 2 3 # linspace : uniformly spaced elements np . linspace ( 0 , 100 , 5 ) array([ 0., 25., 50., 75., 100.]) 1 2 3 # arange: generates values based on the step np . arange ( 0 , 100 , 5 ) array([ 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]) 1 2 3 # Zero: fills the shape specified in the param as zeros np . zeros (( 3 , 3 )) array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) 1 2 3 # Ones - fills the shapes as ones np . ones (( 3 , 3 )) array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) 1 2 3 # Identity matrix: ones in the diagonal np . eye ( 3 ) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) 1 2 3 # Diagonal matrix: only the diagonal has numbers np . diag ([ 1 , 2 , 3 , 4 , 5 ]) array([[1, 0, 0, 0, 0], [0, 2, 0, 0, 0], [0, 0, 3, 0, 0], [0, 0, 0, 4, 0], [0, 0, 0, 0, 5]]) 1 2 3 4 # can we reshape a string array? arr = np . array ([ 1 , 2 , 3 , \"Inceptz\" ]) arr . reshape ( 2 , 2 ) array([['1', '2'], ['3', 'Inceptz']], dtype='<U11')","title":"Numpy basics"},{"location":"python/numpy-basics/#array_indexing_accessing_single_elements","text":"1 2 3 # Get the value of the x1 array x1 array([8, 8, 1, 6, 7, 7]) 1 2 3 # Access the first element x1 [ 0 ] 8 1 2 3 # Access the last element x1 [ - 1 ] 7 In a multi-dimensional array, items can be accessed using a comma-separated tuple of indices: 1 2 3 # Get the value of x2 x2 array([[8, 1, 5, 9], [8, 9, 4, 3], [0, 3, 5, 0]]) 1 2 3 # Access the first row, first element x2 [ 0 , 0 ] 8 1 2 3 # Access the second row, third element x2 [ 1 , 2 ] 4 1 2 3 # Access the last row, last before element x2 [ 2 , - 2 ] 5 Values can also be modified using any of the above index notation 1 2 3 4 5 6 7 8 9 10 11 12 13 # Let's change the first row, first element to 12 # printing the previous x2 print ( x2 ) # change the first row, first element to 12 x2 [ 0 , 0 ] = 12 # Get the new x2 print ( \"---\" ) print ( x2 ) [[8 1 5 9] [8 9 4 3] [0 3 5 0]] --- [[12 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] Keep in mind that, unlike Python lists, NumPy arrays have a fixed type. This means, for example, that if you attempt to insert a floating-point value to an integer array, the value will be silently truncated. Don't be caught unaware by this behavior! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Lets change the first row, first element to # a floating point number e.g. 3.1415 # print old x2 print ( x2 ) # Assign a floating point number to the first element # This will be truncated x2 [ 0 , 0 ] = 3.1415 print ( \"---\" ) # print new x2 print ( x2 ) [[3 1 5 9] [8 9 4 3] [0 3 5 0]] --- [[3 1 5 9] [8 9 4 3] [0 3 5 0]] You can see that thought we have assigned the number as 3.1415 , it has been truncated to 3 as the array x2 is of integer type and hence cannot take floating point numbers.","title":"Array Indexing: Accessing single elements"},{"location":"python/numpy-basics/#array_slicing_accessing_subarrays","text":"Just as we can use square brackets to access individual array elements, we can also use them to access subarrays with the slice notation, marked by the colon ( : ) character. The NumPy slicing syntax follows that of the standard Python list; to access a slice of an array x , use this: x[start:stop:step] If any of these are unspecified, they default to the values start=0 , stop=size of dimension , step=1 . We'll take a look at accessing sub-arrays in one dimension and in multiple dimensions. 1 2 3 4 # Let's define an one dimension array x = np . arange ( 10 ) print ( x ) [0 1 2 3 4 5 6 7 8 9] 1 2 3 # Get the first five elements x [: 5 ] array([0, 1, 2, 3, 4]) 1 2 3 # elements after index 5 x [ 5 :] array([5, 6, 7, 8, 9]) 1 2 3 # slicing from the middle x [ 4 : 7 ] array([4, 5, 6]) 1 2 3 # Every second element x [:: 2 ] array([0, 2, 4, 6, 8]) 1 2 3 # Every second element starting from 1 x [ 1 :: 2 ] array([1, 3, 5, 7, 9]) A potentially confusing case is when the step value is negative. In this case, the defaults for start and stop are swapped. This becomes a convenient way to reverse an array: 1 2 3 # reversing the entire array x [:: - 1 ] array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) 1 2 3 4 # reversing every other element from index 5 # remember to sway the start and stop position x [ 5 :: - 2 ] array([5, 3, 1])","title":"Array Slicing: Accessing Subarrays"},{"location":"python/numpy-basics/#multi-dimension_subarrays","text":"Multi-dimensional slices work in the same way, with multiple slices separated by commas. 1 2 3 # Let's get x2 we defined earlier x2 array([[3, 1, 5, 9], [8, 9, 4, 3], [0, 3, 5, 0]]) 1 2 3 # Getting the first two rows and three columns x2 [: 2 ,: 3 ] array([[3, 1, 5], [8, 9, 4]]) 1 2 3 # Getting all rows and every other column x2 [:,:: 2 ] array([[3, 5], [8, 4], [0, 5]]) Since the range is for all rows, we don't give the start and the stop range. For columns, since we need every other column in the range, we dont specify the start and the stop range but just the step Subarray dimensions can be reversed as well. 1 2 3 # Printing original array x2 array([[3, 1, 5, 9], [8, 9, 4, 3], [0, 3, 5, 0]]) 1 2 3 # reverse the array x2 [:: - 1 , :: - 1 ] array([[0, 5, 3, 0], [3, 4, 9, 8], [9, 5, 1, 3]])","title":"Multi-Dimension Subarrays"},{"location":"python/numpy-basics/#accessing_array_rows_and_columns","text":"One commonly needed routine is accessing of single rows or columns of an array. This can be done by combining indexing and slicing, using an empty slice marked by a single colon ( : ): 1 2 3 # Get the x2 array x2 array([[3, 1, 5, 9], [8, 9, 4, 3], [0, 3, 5, 0]]) 1 2 3 # Get the first column of x2 x2 [:, 0 ] array([3, 8, 0]) 1 2 3 # get the first row of x2 x2 [ 0 ,:] array([3, 1, 5, 9]) In case of row access, the empty slice can be omitted for a more compact syntax 1 2 3 # equivalent to x2[0, :] print ( x2 [ 0 ]) [3 1 5 9]","title":"Accessing array rows and columns"},{"location":"python/numpy-basics/#subarrays_as_no-copy_views","text":"One important and extremely useful thing to know about array slices is that they return views rather than copies of the array data. This is one area in which NumPy array slicing differs from Python list slicing: in lists, slices will be copies. Consider our two dimensional array from before: 1 2 3 # get the x2 array print ( x2 ) [[3 1 5 9] [8 9 4 3] [0 3 5 0]] 1 2 3 4 # Let's extract a 2 x 2 subarray from x2 x2_sub = x2 [: 2 ,: 2 ] print ( x2_sub ) [[3 1] [8 9]] Now, if we modify the array, the original array is changed 1 2 3 4 # change the first row, first element to 99 x2_sub [ 0 , 0 ] = 99 print ( x2_sub ) [[99 1] [ 8 9]] 1 2 3 # Get the x2 array print ( x2 ) [[99 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] This default behavior is actually quite useful: it means that when we work with large datasets, we can access and process pieces of these datasets without the need to copy the underlying data buffer.","title":"Subarrays as no-copy views"},{"location":"python/numpy-basics/#creating_copies_of_arrays","text":"Despite the nice features of array views, it is sometimes useful to instead explicitly copy the data within an array or a subarray. This can be most easily done with the copy() method: 1 2 3 # Let's start with the x2 array print ( x2 ) [[99 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] 1 2 3 4 # Let get the first two rows and two columns x2_sub_arr = x2 [: 2 ,: 2 ] print ( x2_sub_arr ) [[99 1] [ 8 9]] 1 2 3 4 5 # Let create a copy of this sub array # use the .copy method x2_sub_arr_copy = x2_sub_arr . copy () print ( x2_sub_arr_copy ) [[99 1] [ 8 9]] 1 2 3 4 # Let change the first row, first element to 0 x2_sub_arr_copy [ 0 , 0 ] = 0 print ( x2_sub_arr_copy ) [[0 1] [8 9]] 1 2 3 # Let's print the original x2 print ( x2 ) [[99 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] As you can see, the original x2 array still has the number 99 as the first element as the original array x2 is not touched.","title":"Creating copies of arrays"},{"location":"python/numpy-basics/#reshaping_for_arrays","text":"Another useful type of operation is reshaping of arrays. The most flexible way of doing this is with the reshape method. For example, if you want to put the numbers 1 through 9 in a 3 x 3 grid, you can do the following: 1 2 3 4 # Let's form a grid of 9 elements grid = np . arange ( 1 , 10 ) print ( grid ) [1 2 3 4 5 6 7 8 9] 1 2 3 # Let's reshape this to a 3 x 3 grid grid . reshape ( 3 , 3 ) array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Note that for this to work, the size of the initial array must match the size of the reshaped array. We can also reshape a one-dimension array 1 2 3 # Let's declare a one-dimension array x = np . array ([ 1 , 2 , 3 ]) 1 2 3 4 5 # Let's re-shape this to a 1,3 array # i.e. 1 row, 3 columns array x . reshape ( 1 , 3 ) print ( x ) [1 2 3] 1 2 3 4 # Let's re-shape x to a 3,1 array # i.e. 3 rows and 1 column array x . reshape ( 3 , 1 ) array([[1], [2], [3]]) 1 2 3 4 # The other way of doing this # is by using .newaxis method x [:, np . newaxis ] array([[1], [2], [3]]) 1 2 3 # To change back to 1 row, 3 columns x [ np . newaxis , :] array([[1, 2, 3]])","title":"Reshaping for arrays"},{"location":"python/numpy-basics/#array_concatenation","text":"All of the preceding routines worked on single arrays. It's also possible to combine multiple arrays into one, and to conversely split a single array into multiple arrays. Concatenation, or joining of two arrays in NumPy, is primarily accomplished using the routines np.concatenate , np.vstack , and np.hstack 1 2 3 4 5 6 # np.concatenate x = np . array ([ 1 , 2 , 3 ]) y = np . array ([ 4 , 5 , 6 ]) np . concatenate ([ x , y ]) # Note the arrays are within [ ] array([1, 2, 3, 4, 5, 6]) 1 2 3 4 5 # More than two arrays can also be concatenated z = [ 99 , 99 , 99 ] np . concatenate ([ x , y , z ]) array([ 1, 2, 3, 4, 5, 6, 99, 99, 99]) 1 2 3 4 5 6 7 # Concatenating two-dimension arrays grid = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ]]) # concatenating along the first axis i.e. columns np . concatenate ([ grid , grid ]) array([[1, 2, 3], [4, 5, 6], [1, 2, 3], [4, 5, 6]]) 1 2 3 # concatenating along the second axis i.e rows np . concatenate ([ grid , grid ], axis = 1 ) array([[1, 2, 3, 1, 2, 3], [4, 5, 6, 4, 5, 6]]) For working with arrays of mixed dimensions , it can be clearer to use the np.vstack (vertical stack) and np.hstack (horizontal stack) functions: 1 2 3 4 5 6 7 8 9 10 # lets define a single dimension array x = np . array ([ 1 , 2 , 3 ]) # let's define a two dimension array grid = np . array ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ]]) print ( x ) print ( grid ) [1 2 3] [[1 2 3] [4 5 6]] 1 2 3 4 # let's vertically stack the arrays # use the vstack method np . vstack ([ x , grid ]) array([[1, 2, 3], [1, 2, 3], [4, 5, 6]]) 1 2 3 4 5 6 7 # let's stack the arrays horizontally # use the hstack method y = np . array ([[ 99 ], [ 99 ]]) np . hstack ([ grid , y ]) array([[ 1, 2, 3, 99], [ 4, 5, 6, 99]])","title":"Array concatenation"},{"location":"python/numpy-basics/#array_splitting","text":"The opposite of concatenation is splitting, which is implemented by the functions np.split , np.hsplit , and np.vsplit 1 2 3 4 5 6 7 8 9 10 11 12 # Let's define a array x = [ 1 , 2 , 3 , 99 , 99 , 3 , 2 , 1 ] # split from index 3 to 5 # this would give three sub arrays # [1,2,3] - [99,99] - [3,2,1] # Get these 3 sub arrays in 3 variables x_1 , x_2 , x_3 = np . split ( x , [ 3 , 5 ]) print ( x_1 , x_2 , x_3 ) [1 2 3] [99 99] [3 2 1] 1 2 3 4 5 6 # vertically splitting an array # Let's make a multi-dim array grid = np . arange ( 16 ) . reshape ( 4 , 4 ) print ( grid ) [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]] 1 2 3 4 5 6 # use the vsplit method to split the array # vertically upper , lower = np . vsplit ( grid , [ 2 ]) print ( upper ) print ( lower ) [[0 1 2 3] [4 5 6 7]] [[ 8 9 10 11] [12 13 14 15]] 1 2 3 4 5 6 # use the hsplit method to split # the array horizontally left , right = np . hsplit ( grid , [ 2 ]) print ( left ) print ( right ) [[ 0 1] [ 4 5] [ 8 9] [12 13]] [[ 2 3] [ 6 7] [10 11] [14 15]]","title":"Array Splitting"},{"location":"python/numpy-basics/#iterating_thru_arrays","text":"1 2 3 4 5 6 7 8 # Let get the array x # and iterate thru it print ( x ) # 1D array for i in x : print ( i ) [1, 2, 3, 99, 99, 3, 2, 1] 1 2 3 99 99 3 2 1 1 2 3 4 5 6 7 8 # Lets get the 2d array x2 # and iterate thru it print ( x2 ) for i in x2 : for j in i : print ( j ) [[99 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] 99 1 5 9 8 9 4 3 0 3 5 0 1 2 3 4 5 6 7 8 9 10 11 # Let's get the 3d array x3 # and iterate thru it print ( x3 ) print ( \"---\" ) for i in x3 : for j in i : print ( j ) # for k in j: print(k) [[[5 9 3 0 5] [0 1 2 4 2] [0 3 2 0 7] [5 9 0 2 7]] [[2 9 2 3 3] [2 3 4 1 2] [9 1 4 6 8] [2 3 0 0 6]] [[0 6 3 3 8] [8 8 2 3 2] [0 8 8 3 8] [2 8 4 3 0]]] --- [5 9 3 0 5] [0 1 2 4 2] [0 3 2 0 7] [5 9 0 2 7] [2 9 2 3 3] [2 3 4 1 2] [9 1 4 6 8] [2 3 0 0 6] [0 6 3 3 8] [8 8 2 3 2] [0 8 8 3 8] [2 8 4 3 0]","title":"Iterating thru arrays"},{"location":"python/numpy-basics/#addition_subtraction_multiplication_in_a_numpy_array","text":"1 2 3 4 5 # Let's define the first array # of 16 elements arr1 = np . array ( range ( 1 , 17 , 1 )) print ( arr1 ) [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16] 1 2 3 4 # Let's reshape it to a 4 x 4 matrix arr1 = arr1 . reshape ( 4 , 4 ) print ( arr1 ) [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12] [13 14 15 16]] 1 2 3 4 5 # Let's define the second array # of 16 elements with difference 10 arr2 = np . array ( range ( 10 , 170 , 10 )) print ( arr2 ) [ 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160] 1 2 3 4 # Let's reshape it to a 4 x 4 matrix arr2 = arr2 . reshape ( 4 , 4 ) print ( arr2 ) [[ 10 20 30 40] [ 50 60 70 80] [ 90 100 110 120] [130 140 150 160]] 1 2 3 # addition of two arrays arr1 + arr2 array([[ 11, 22, 33, 44], [ 55, 66, 77, 88], [ 99, 110, 121, 132], [143, 154, 165, 176]]) 1 2 3 # subtracting two arrays arr2 - arr1 array([[ 9, 18, 27, 36], [ 45, 54, 63, 72], [ 81, 90, 99, 108], [117, 126, 135, 144]]) 1 2 3 # Multiplying two arrays arr1 * arr2 array([[ 10, 40, 90, 160], [ 250, 360, 490, 640], [ 810, 1000, 1210, 1440], [1690, 1960, 2250, 2560]]) 1 2 3 # Dividing the arrays arr2 / arr1 array([[10., 10., 10., 10.], [10., 10., 10., 10.], [10., 10., 10., 10.], [10., 10., 10., 10.]]) 1 2 3 4 # array multiplication with a scalar value # e.g arr1 * 10 arr1 * 10 array([[ 10, 20, 30, 40], [ 50, 60, 70, 80], [ 90, 100, 110, 120], [130, 140, 150, 160]]) 1 2 3 4 # dot product # dot product is different from multiplication np . dot ( arr1 , arr2 ) array([[ 900, 1000, 1100, 1200], [2020, 2280, 2540, 2800], [3140, 3560, 3980, 4400], [4260, 4840, 5420, 6000]])","title":"Addition, subtraction, multiplication in a numpy array"},{"location":"python/numpy-basics/#sorting_and_searching","text":"1 2 3 # Let's define an array print ( x ) [1, 2, 3, 99, 99, 3, 2, 1] 1 2 3 # sorting 1D array np . sort ( x ) array([ 1, 1, 2, 2, 3, 3, 99, 99]) 1 2 3 # Lets get a 2d array print ( x2 ) [[99 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] 1 2 3 4 # sorting a 2d array # will sort elements in each rows np . sort ( x2 ) array([[ 1, 5, 9, 99], [ 3, 4, 8, 9], [ 0, 0, 3, 5]])","title":"Sorting and searching"},{"location":"python/numpy-basics/#statistics_with_numpy","text":"1 2 3 4 # Let's define an array stat_arr = np . arange ( 1 , 10 ) print ( stat_arr ) [1 2 3 4 5 6 7 8 9] 1 2 3 # Get the mean of the array np . mean ( stat_arr ) 5.0 1 2 3 # Get the median of the array np . median ( stat_arr ) 5.0 1 2 3 # get the std dev of the array np . std ( stat_arr ) 2.581988897471611 1 2 3 # get the variance np . var ( stat_arr ) 6.666666666666667 1 2 3 4 # get the percentile scores = [ 80 , 90 , 70 , 85 , 60 , 55 , 75 , 89 , 12 , 13 , 15 ] np . percentile ( scores , 80 ) 85.0 1 2 3 4 5 6 7 8 # mean of a 2d array print ( x2 ) # getting the mean across rows # axis = 1 np . mean ( x2 , axis = 1 ) [[99 1 5 9] [ 8 9 4 3] [ 0 3 5 0]] array([28.5, 6. , 2. ]) 1 2 3 # getting the mean across columns np . mean ( x2 , axis = 0 ) array([35.66666667, 4.33333333, 4.66666667, 4. ]) 1 2 3 4 5 6 # Drawing an histogram import matplotlib.pyplot as plt norm_data = np . random . normal ( 10 , 1.0 , 5000000 ) plt . hist ( norm_data , 200 ) # 200 is the number of bins plt . show () 1 2 3 4 # Getting a random integer # in a range np . random . randint ( 1 , 100 ) 91 1 2 3 4 5 # generating a random number # first param : range # second param : size np . random . rand ( 1 , 100 ) array([[0.47390871, 0.9424515 , 0.08067525, 0.50477582, 0.75355236, 0.6443007 , 0.68009591, 0.85948974, 0.79562236, 0.67030161, 0.21086869, 0.43761942, 0.14953803, 0.73310741, 0.0978983 , 0.78410335, 0.74009511, 0.23071498, 0.13325923, 0.19633374, 0.68353164, 0.23094352, 0.17681563, 0.14249063, 0.51898069, 0.21801461, 0.0914506 , 0.28495553, 0.09474098, 0.55464705, 0.94250659, 0.24068165, 0.80642805, 0.34753732, 0.16368083, 0.49636519, 0.22354509, 0.76651945, 0.03019415, 0.64006132, 0.035494 , 0.21898449, 0.51010665, 0.41423507, 0.91151493, 0.6051262 , 0.77595892, 0.43230851, 0.77152591, 0.85263481, 0.16105187, 0.06416947, 0.41576968, 0.28921585, 0.03823162, 0.11454612, 0.45932632, 0.78403473, 0.40116525, 0.94245161, 0.98495574, 0.40247529, 0.76547687, 0.76620485, 0.3622606 , 0.66219648, 0.0360825 , 0.23129832, 0.77402102, 0.23150597, 0.66776317, 0.15584173, 0.64016945, 0.69802422, 0.55093934, 0.80262937, 0.59689222, 0.55314997, 0.19660366, 0.25421008, 0.67910571, 0.75892023, 0.15447376, 0.52093862, 0.37407023, 0.88858665, 0.10001336, 0.63902172, 0.71561141, 0.44134696, 0.77242583, 0.08833249, 0.61827113, 0.22744314, 0.77190824, 0.78893133, 0.34247442, 0.35901415, 0.68427893, 0.2549437 ]]) 1 2 3 4 5 6 # getting a choice in a given range # random.choice takes two parameters # first param: list of numbers # second param: size in rows and columns np . random . choice ([ 1 , 3 , 5 , 7 , 10 ], size = ( 1 , 3 )) array([[ 7, 10, 10]]) 1 2 3 4 5 6 7 8 # plotting a chi-square graph import matplotlib.pyplot as plt import seaborn as sns chi = np . random . chisquare ( df = 1 , size = 100000 ) sns . distplot ( chi , hist = False ) plt . show ()","title":"Statistics with numpy"},{"location":"python/numpy-basics/#functions_involved_in_generating_array","text":"1 2 3 # Let's create an array and reshape it a = np . arange ( 1 , 17 ) . reshape ( 4 , 4 ) 1 print ( a ) [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12] [13 14 15 16]] 1 2 3 # linspace : uniformly spaced elements np . linspace ( 0 , 100 , 5 ) array([ 0., 25., 50., 75., 100.]) 1 2 3 # arange: generates values based on the step np . arange ( 0 , 100 , 5 ) array([ 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]) 1 2 3 # Zero: fills the shape specified in the param as zeros np . zeros (( 3 , 3 )) array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) 1 2 3 # Ones - fills the shapes as ones np . ones (( 3 , 3 )) array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) 1 2 3 # Identity matrix: ones in the diagonal np . eye ( 3 ) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) 1 2 3 # Diagonal matrix: only the diagonal has numbers np . diag ([ 1 , 2 , 3 , 4 , 5 ]) array([[1, 0, 0, 0, 0], [0, 2, 0, 0, 0], [0, 0, 3, 0, 0], [0, 0, 0, 4, 0], [0, 0, 0, 0, 5]]) 1 2 3 4 # can we reshape a string array? arr = np . array ([ 1 , 2 , 3 , \"Inceptz\" ]) arr . reshape ( 2 , 2 ) array([['1', '2'], ['3', 'Inceptz']], dtype='<U11')","title":"Functions involved in generating array"},{"location":"python/python-basics-01/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); By hazeez Mar 29, 2020 in Python Python Basics # This post covers the following python topics Keywords Variables Data types Comments in Python # Single line comment Comments in python start with a hash (#) The shortcut to make a comment in jupyter notebook is Ctrl + / Example of a comment below 1 # This is a comment Multi-line comment Comments can also be multi-lined. Multi-lined comments should start and end with three apostrophies ''' or three double quotes \"\"\" Example below 1 2 3 4 5 ''' This is a multi-line comment ''' '\\nThis is a\\nmulti-line\\ncomment\\n' Printing a statement in python. Use the print keyword. Syntax below 1 print ( \"Hello, World!\" ) Hello, World! Keywords # What are keywords? Keywords are reserved words in python which python uses for internal operations. E.g of keywords are print, True, False, for, if etc Do we have a list of keywords pre-defined in python? Yes. The list of keywords can be obtained from python using the following code. 1 2 import keyword keyword . kwlist ['False', 'None', 'True', 'and', 'as', 'assert', 'async', 'await', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield'] Identifiers # Identifiers are user defined names to any variables, functions, classes or module. Variables are memory holders or entities whose value will change during the course of execution. 1 2 3 # The following is a variable a = 20 The same can be printed via a print statement 1 print ( \"The value is:\" , a ) The value is: 20 Identifier rules There are a set of rules that we need to follow to define a variable Cannot be keywords (e.g break = 10 will result in syntax error since break is a python keyword) Cannot start with numbers (e.g. 20A = 1 will result in syntax error) No space in between a variable name (e.g. my name = \"Azeez\" will result in syntax error. Correct syntax is my_name = \"Azeez\" ) No special characters are allowed like (!,@,# etc) It should be with range a-z or A-Z It is case sensitive. Bad examples of variable names are mentioned below 1 break = 10 File \"<ipython-input-8-9f64e1454aa4>\" , line 1 break = 10 ^ SyntaxError : invalid syntax 1 2 A = 20 File \"<ipython-input-10-ee2fe4234176>\" , line 1 2A = 20 ^ SyntaxError : invalid syntax 1 my name = \"Azeez\" File \"<ipython-input-102-d7528faba6da>\" , line 1 my name = \"Azeez\" ^ SyntaxError : invalid syntax 1 @a = 20 File \"<ipython-input-11-ed37feed5ec1>\" , line 1 @a = 20 ^ SyntaxError : invalid syntax Correct examples of variables are 1 2 a = 20 b = 20 1 2 3 4 5 a = 20 A = 27 # note the change in case here. print ( \"The value if a is: \" , a ) print ( \"The value if A is: \" , A ) The value if a is: 20 The value if A is: 27 Multiple variables can be assigned in a single line Examples below 1 2 3 a = b = c = 1 a , b , c = 1 , 2 , \"Red\" print ( \"The color is:\" , c ) The color is: Red We can also check if we have inadvertently taken a keyword as a variable. Say for e.g. I took try as a variable and assigned number 5 to it. 1 2 try = 5 print ( \"The value is :\" , try ) File \"<ipython-input-15-d569dc03481f>\" , line 1 try = 5 ^ SyntaxError : invalid syntax This will raise a syntax error and we might not know why the system actually raises an error. In such a situation, we need to check if the variable is a keyword or not. The following code can help us in finding if a word is a keyword or not. 1 2 import keyword keyword . iskeyword ( \"try\" ) True When it returns True as in this case, it means that the variable is a keyword. If instead of try , we provide it as try1 and assign the value of 5 to it. What happens? 1 2 try1 = 5 print ( try1 ) 5 It does not result in a error. The reason is try1 is not a keyword. we can check it with the following code 1 keyword . iskeyword ( \"try1\" ) False It returned False which means that the try1 is a valid variable and not a keyword. Data Types # There are multiple datatypes suppored by python. Some of them are int float boolean string complex list tuples set dict Integers Whole numbers are defined by the integer datatype or int. 1 2 3 4 5 6 7 # This is an int datatype a = 10 # Check the type of the variable - using type method type ( a ) int This returns as int which means the variable is of the type integer. Decimals Decimals or floating point numbers are represented as a float datatype 1 2 b = 10.0 type ( b ) float This returs as float which means the variable is of type decimal. String String data types are used to represent alpha numeric characters like name. String data types are declared within double quotes. 1 2 3 4 5 my_color = \"red\" my_address = \"123, Canary Wharf, London, UK\" type ( my_color ) type ( my_address ) str The above code returns as str which is a string datatype. Complex Similarly, complex numbers can also be represented as complex datatype 1 2 c = 1 + 6 j type ( c ) complex This returns as complex , which means that the variable represents a complex number. List List is a data structure that allows heterogenous elements in it. It can have zero or more elements in it. The properties of a list datatype are Allows heterogenous elements in it Index always start from 0 Elements can be accessed via the index It can be mutated (can be changed) Example of a list is mentioned below 1 2 3 4 5 6 7 8 9 10 11 12 13 # Sample list allows heterogenous elements in it sample_list = [ 12 , 43 , \"fd\" , 1 + 3 j ] # List can be accessed via an index element0 = sample_list [ 0 ] print ( element0 ) element3 = sample_list [ 3 ] print ( element3 ) # List can be mutated sample_list [ 0 ] = \"Red\" print ( sample_list ) 12 (1+3j) ['Red', 43, 'fd', (1+3j)] The methods supported by list can be seen by using the dir command. 1 2 # access all the supported methods by list dir ( sample_list ) ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort'] The supported methods are 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort' Similarly, help can be sought using the help command 1 2 # what does the clear method do? help ( sample_list . clear ) Help on built-in function clear: clear() method of builtins.list instance Remove all items from list. 1 2 # what does the list.remove method to? help ( sample_list . remove ) Help on built-in function remove: remove(value, /) method of builtins.list instance Remove first occurrence of value. Raises ValueError if the value is not present. List operations # List supports a multitude of operations. Let's us take an example list and see the operations 1 2 3 4 5 6 # example list mylist = [ 45 , 32 , 45 , 65 , 35 ] # Sorting a list mylist . sort () print ( mylist ) [32, 35, 45, 45, 65] 1 2 3 # Sorting a list in descending order mylist . sort ( reverse = True ) print ( mylist ) [65, 45, 45, 35, 32] 1 2 3 # Counting the number of items in the list # It takes one argument which is the element mylist . count ( 45 ) 2 1 2 # When it doesn't have the element, it will return zero mylist . count ( 1 ) 0 1 2 3 4 5 6 7 8 # Extending a list # Extending the list takes one iterable as an argument # Here [1,2] is the iterable argument # iterable means - more than one element in it mylist . extend ([ 1 , 2 ]) print ( mylist ) [65, 45, 45, 35, 32, 1, 2] 1 2 3 4 5 # Appending to a list # Append the given element to the end of the list mylist . append ( 5 ) print ( mylist ) [65, 45, 45, 35, 32, 1, 2, 5] 1 2 3 4 5 6 7 # Inserting to a list # Inserting can add elements anywhere in the list # Takes two arguments - index and the element # Lets add 10 to the second position mylist . insert ( 2 , 10 ) 1 print ( mylist ) [65, 45, 10, 45, 35, 32, 1, 2, 5] 1 2 3 4 # Removing the last element from the list # Use pop to remove the last element mylist . pop () 5 1 print ( mylist ) [65, 45, 10, 45, 35, 32, 1, 2] 1 2 3 4 5 # Remove any element from the list # Mention the element in the remove method mylist . remove ( 45 ) print ( mylist ) [65, 10, 45, 35, 32, 1, 2] Please note that there are two elements of 45 and it removed the first occurance of the element List slicing Cutting the list to form new lists 1 2 3 4 5 6 # Lets slice our mylist # We need the first three elements of mylist # mylist = [65, 10, 45, 35, 32, 1, 2] mylist [ 0 : 3 ] [65, 10, 45] List slicing takes a range - Here the start range is 0 and the end range is 3. The end range is not included. So, we don't get the 4 th element 35. 1 2 3 4 5 # We can also slice from the middle of the list to the end # mylist = [65, 10, 45, 35, 32, 1, 2] # Suppose we need from 35 to the end mylist [ 3 :] [35, 32, 1, 2] Note that the start range is 3 and the end range is left empty. This means that we need the rest of the list. 1 2 3 # we can also slice from the middle to the beginning mylist [: 3 ] [65, 10, 45] Note that the start range is empty and the end range is provided 1 2 3 4 # Get the list from the end to the beginning # In this case from 1 to 65 mylist [: - 1 ] [65, 10, 45, 35, 32, 1] 1 2 3 # Inserting multiple elements in a particular position in a list mylist [ 1 : 2 ] = 6 , 7 1 print ( mylist ) [65, 6, 7, 45, 35, 32, 1, 2] Lists can get complex too and it provides the ability to slice to any degree. Let's take an example and see. 1 2 3 # my complex list my_complex_list = [[ 1 , 2 ],[ 3 , 5 ],[ \"a\" ,[ \"gr\" , \"ewd\" ]]] Let's try to get the element gr which is within the 2 nd index. 1 2 3 # Lets get the second index in which element gr is present print ( my_complex_list [ 2 ]) ['a', ['gr', 'ewd']] This gave the element in the 2 nd position ['a',['gr','ewd]] . But, what we need is gr which is again in the 1 st index position. Let's get that. 1 2 3 # getting gr again print ( my_complex_list [ 2 ][ 1 ]) ['gr', 'ewd'] This gave ['gr','ewd'] . Again, we gr is in the position 0. To get that, we have to again filter to index 0. 1 2 3 # get gr print ( my_complex_list [ 2 ][ 1 ][ 0 ]) gr Now, we got gr from our complex list [[1,2],[3,5],[\"a\",[\"gr\",\"ewd\"]]] Tuples # Tuples are immutable data types that allows heterogenous values. They are declared within a round bracket () 1 2 3 4 # Declaring a tuple sample_tuple = ( 1 , 43 , 56 , 3.5 , \"hi\" , 1 + 2 j ) print ( sample_tuple ) (1, 43, 56, 3.5, 'hi', (1+2j)) 1 2 3 4 # tuples are immutable # Let's try to change the tuple sample_tuple [ 0 ] = \"change tuple\" --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-51-0d5e00df77cd> in <module> 2 # Let's try to change the tuple 3 ----> 4 sample_tuple [ 0 ] = \"change tuple\" TypeError : 'tuple' object does not support item assignment A type error is thrown that states that tuple does not support an item assignment. They are immutable. When we try to change a tuple, we get this error. 1 2 3 4 5 6 # Counting elements in a tuple # sample_tuple = (1,43,56,3.5,\"hi\",1+2j) # Let's count the number of hi sample_tuple . count ( \"hi\" ) 1 1 2 3 4 # is sample_tuple a tuple # we can check it using the type function type ( sample_tuple ) tuple Yes, it is a tuple! 1 2 3 4 # Accessing tuple elements via an index # Let's find the index for the element \"hi\" sample_tuple . index ( \"hi\" ) 4 1 2 3 4 5 # deleting a tuple # Use the del command to delete a tuple del sample_tuple 1 2 3 # Let's try to access sample_tuple print ( sample_tuple ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-60-2ae4cf4309ef> in <module> 1 # Let's try to access sample_tuple 2 ----> 3 print ( sample_tuple ) NameError : name 'sample_tuple' is not defined Since we have already deleted the tuple, a Name error is thrown Difference between a list and a tuple List is declared with a square bracket [ ] and tuples using round brackets ( ) . Lists are mutable and tuples are immutable Sets # Sets are data structures that are does not allow duplicate values in it. The properties of sets are Allows heterogenous values Does not allow duplicates Cannot be accessed using index Cannot be changed Sometimes can be ordered Let's see a sample set. 1 sample_set = { 0 , 1 , 21 , 22 , 23 , 25 , 3 , 32 , 43 , 45 , 66 , 67 , 'a' , 'abc' } Let's try to get an element from the set using an index 1 print ( sample_set [ 2 ]) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-63-4dd3666b0133> in <module> ----> 1 print ( sample_set [ 2 ] ) TypeError : 'set' object is not subscriptable Accessing the set elements via an index raises a TypeError 1 2 3 4 5 # sets does not allow duplicate values # Let's create a sample set sample_set1 = { 1 , 1 , 2 , 3 , 4 , 5 , 5 , 5 , 6 , 6 , 6 , 6 } print ( sample_set1 ) {1, 2, 3, 4, 5, 6} As we can see here, sample_set1 only allows unique values and removes the duplicates automatically. Also to note that sets are defined within the curly braces { } Dictionary # Dictionaries are data structures that store elements as a key-value pair The properties of dictionaries are They have key-value pairs in it Keys are unique and values may have duplicates Can retrieve the value using key Can change the value using key The key is immutable Let's declare a sample dictionary! 1 sample_dict = { \"a\" : \"alpha\" , 1 : \"first\" , 2 : \"second\" , 3 : \"third\" , 1 : \"fourth\" , 5 : \"fourth\" , 6 : \"sixth\" } Here, let's take the first key-value pair \"a\":\"alpha\" . a is the key (left side of the colon :) alpha is the value (right side of the colon :) Let's retrieve the first value using the key. 1 2 3 # retrieve the value alpha using the key 'a' sample_dict [ 'a' ] 'alpha' We mention the key within the [ ] square brackets and retrieve the value. 1 2 3 4 5 # Retrieving the sixth value # We know the key for the value \"Sixth\" is \"6\" # The value can be retrieved using the key \"6\" sample_dict [ 6 ] 'sixth' Dictionaries values are mutable. Let's see with an example. Let's change the 6 th element value to fifth instead of fourth 1 2 3 4 5 # Right now in the sample dict the value for the 6th element is fourth # Let's change it to fifth sample_dict [ \"5\" ] = \"fifth\" print ( sample_dict ) {'a': 'alpha', 1: 'fourth', 2: 'second', 3: 'third', 5: 'fourth', 6: 'sixth', '5': 'fifth'} There are two things to note here We have changed the value of key 5 to \"fifth\" The order of the elements have changed. Dictionaries does not guarantee the order of the elements. Hence, we cannot retrieve values using indexes but, only using keys. Let's try to remove an element from the dictionary. Again, we have to use keys to remove the respective value 1 2 3 4 5 # Lets remove 'alpha' from the sample dict # We can do that using the pop method sample_dict . pop ( \"a\" ) print ( sample_dict ) {1: 'fourth', 2: 'second', 3: 'third', 5: 'fourth', 6: 'sixth', '5': 'fifth'} Note that the value of alpha is removed from the dictionary. Let's try to add a new key-value pair to the dictionary 1 2 3 4 5 # Lets's add the color red to sample_dict # Here the 'color' is the key and 'red' is the value sample_dict [ \"color\" ] = \"red\" print ( sample_dict ) {1: 'fourth', 2: 'second', 3: 'third', 5: 'fourth', 6: 'sixth', '5': 'fifth', 'color': 'red'} As you can see, we have added the key value pair \"color\":\"red\" to the dictionary. Strings # String is an important datatype in python. It is useful in manipulating text. The properties of string datatypes are String can be retrieved using index String is immutable Let's take an example and see! 1 2 3 # Let's declare a string fav_movie = \"The world is round!\" Let's determine the type of this fav_movie variable. 1 2 # Determining the type type ( fav_movie ) str This states that the type of fav_movie is str which is a string. String can be accessed using indexes. 1 2 3 4 # let's retrieve the first letter from fav_movie # which is \"T\" fav_movie [ 0 ] 'T' String indexes like any other index starts with 0 . Using the index, we can retrieve the corresponding value. Strings are immutable. We cannot change the string. Let's see with an example. 1 2 3 4 # Strings are immutable # Let's try to replace the first letter of fav_movie with S fav_movie [ 0 ] = \"S\" --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-86-5f0b5dce1f58> in <module> 2 # Let's try to replace the first letter of fav_movie with S 3 ----> 4 fav_movie [ 0 ] = \"S\" TypeError : 'str' object does not support item assignment This gives us a TypeError as the string object fav_movie is immutable. Type conversion # Type conversion is also technically called type-casting. That is to say that we convert one type to another type. E.g A string is converted to an integer and so on. 1 2 3 # Let's declare an integer a = 10 print ( type ( a )) <class 'int'> Here, 10 is of the type integer. Let's declare a string and check the type 1 2 3 4 # Declaring a string greetings = \"Hello, World!\" print ( type ( greetings )) <class 'str'> So, greetings is of type String. So, when is type conversion needed? Let's try to add greetings to a . Python gives us the ability to add using the operator + 1 2 3 4 # add 'a' to 'greetings' new_word = a + greetings print ( new_word ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-90-de1961e67afc> in <module> 1 # add 'a' to 'greetings' 2 ----> 3 new_word = a + greetings 4 print ( new_word ) TypeError : unsupported operand type(s) for +: 'int' and 'str' This results in a TypeError . Why? a is of type integer and greetings is of type String. So, we cannot add both of them. Let's try to convert a of type int to type string . We need to use type conversion for that. 1 2 3 4 5 6 7 # changing from int type to string type # here str function will convert a of type int to string new_a = str ( a ) # Let's check the type after type casting print ( type ( new_a )) <class 'str'> Now, a has been converted to type string . Let's try our above operation again - adding this new_a to greetings 1 2 new_word = new_a + greetings print ( new_word ) 10Hello, World! Now, we have successfully completed type casting. Type casting is needed as python determines the type of the variable during runtime as its a loosely-typed language. Strongly typed languages like Java warrant the user to specify the type of the object during declaration. Similarly, we can type cast list to tuples, tuples to sets etc. Let's see a list conversion to tuple with an example 1 2 3 4 5 6 7 # lets declare a list my_list = [ 'a' , 'z' , 'e' , 'e' , 'z' ] # let's check the type of this list print ( type ( my_list )) <class 'list'> Ok; this is of type list. Remember, list is within [ ] square brackets Let's try to change that to a tuple. 1 2 3 4 5 6 7 # type casting list to tuple my_tuple = tuple ( my_list ) # let's check the type of this tuple print ( type ( my_tuple )) <class 'tuple'> So, we have successfully managed to change the list to a tuple. 1 2 3 # Let see how my_tuple looks like print ( my_tuple ) ('a', 'z', 'e', 'e', 'z') As we can see the list has been converted to tuple. Tuples are within round brackets ( ) Summary # In this post, we saw the python basics - keywords, identifiers and variables. Python gives us a rich set of datatypes starting from int, float, boolean, complex and then gives us robust datatypes like list, set, tuples and dictionaries. We also saw how to slice down lists, retrieve unique values using sets, form immutable objects using tuples and found values with the keys from dictionaries.","title":"Python basics 01"},{"location":"python/python-basics-01/#python_basics","text":"This post covers the following python topics Keywords Variables Data types","title":"Python Basics"},{"location":"python/python-basics-01/#comments_in_python","text":"","title":"Comments in Python"},{"location":"python/python-basics-01/#keywords","text":"What are keywords? Keywords are reserved words in python which python uses for internal operations. E.g of keywords are print, True, False, for, if etc Do we have a list of keywords pre-defined in python? Yes. The list of keywords can be obtained from python using the following code. 1 2 import keyword keyword . kwlist ['False', 'None', 'True', 'and', 'as', 'assert', 'async', 'await', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield']","title":"Keywords"},{"location":"python/python-basics-01/#identifiers","text":"Identifiers are user defined names to any variables, functions, classes or module. Variables are memory holders or entities whose value will change during the course of execution. 1 2 3 # The following is a variable a = 20 The same can be printed via a print statement 1 print ( \"The value is:\" , a ) The value is: 20","title":"Identifiers"},{"location":"python/python-basics-01/#data_types","text":"There are multiple datatypes suppored by python. Some of them are int float boolean string complex list tuples set dict","title":"Data Types"},{"location":"python/python-basics-01/#list_operations","text":"List supports a multitude of operations. Let's us take an example list and see the operations 1 2 3 4 5 6 # example list mylist = [ 45 , 32 , 45 , 65 , 35 ] # Sorting a list mylist . sort () print ( mylist ) [32, 35, 45, 45, 65] 1 2 3 # Sorting a list in descending order mylist . sort ( reverse = True ) print ( mylist ) [65, 45, 45, 35, 32] 1 2 3 # Counting the number of items in the list # It takes one argument which is the element mylist . count ( 45 ) 2 1 2 # When it doesn't have the element, it will return zero mylist . count ( 1 ) 0 1 2 3 4 5 6 7 8 # Extending a list # Extending the list takes one iterable as an argument # Here [1,2] is the iterable argument # iterable means - more than one element in it mylist . extend ([ 1 , 2 ]) print ( mylist ) [65, 45, 45, 35, 32, 1, 2] 1 2 3 4 5 # Appending to a list # Append the given element to the end of the list mylist . append ( 5 ) print ( mylist ) [65, 45, 45, 35, 32, 1, 2, 5] 1 2 3 4 5 6 7 # Inserting to a list # Inserting can add elements anywhere in the list # Takes two arguments - index and the element # Lets add 10 to the second position mylist . insert ( 2 , 10 ) 1 print ( mylist ) [65, 45, 10, 45, 35, 32, 1, 2, 5] 1 2 3 4 # Removing the last element from the list # Use pop to remove the last element mylist . pop () 5 1 print ( mylist ) [65, 45, 10, 45, 35, 32, 1, 2] 1 2 3 4 5 # Remove any element from the list # Mention the element in the remove method mylist . remove ( 45 ) print ( mylist ) [65, 10, 45, 35, 32, 1, 2] Please note that there are two elements of 45 and it removed the first occurance of the element","title":"List operations"},{"location":"python/python-basics-01/#tuples","text":"Tuples are immutable data types that allows heterogenous values. They are declared within a round bracket () 1 2 3 4 # Declaring a tuple sample_tuple = ( 1 , 43 , 56 , 3.5 , \"hi\" , 1 + 2 j ) print ( sample_tuple ) (1, 43, 56, 3.5, 'hi', (1+2j)) 1 2 3 4 # tuples are immutable # Let's try to change the tuple sample_tuple [ 0 ] = \"change tuple\" --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-51-0d5e00df77cd> in <module> 2 # Let's try to change the tuple 3 ----> 4 sample_tuple [ 0 ] = \"change tuple\" TypeError : 'tuple' object does not support item assignment A type error is thrown that states that tuple does not support an item assignment. They are immutable. When we try to change a tuple, we get this error. 1 2 3 4 5 6 # Counting elements in a tuple # sample_tuple = (1,43,56,3.5,\"hi\",1+2j) # Let's count the number of hi sample_tuple . count ( \"hi\" ) 1 1 2 3 4 # is sample_tuple a tuple # we can check it using the type function type ( sample_tuple ) tuple Yes, it is a tuple! 1 2 3 4 # Accessing tuple elements via an index # Let's find the index for the element \"hi\" sample_tuple . index ( \"hi\" ) 4 1 2 3 4 5 # deleting a tuple # Use the del command to delete a tuple del sample_tuple 1 2 3 # Let's try to access sample_tuple print ( sample_tuple ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-60-2ae4cf4309ef> in <module> 1 # Let's try to access sample_tuple 2 ----> 3 print ( sample_tuple ) NameError : name 'sample_tuple' is not defined Since we have already deleted the tuple, a Name error is thrown","title":"Tuples"},{"location":"python/python-basics-01/#sets","text":"Sets are data structures that are does not allow duplicate values in it. The properties of sets are Allows heterogenous values Does not allow duplicates Cannot be accessed using index Cannot be changed Sometimes can be ordered Let's see a sample set. 1 sample_set = { 0 , 1 , 21 , 22 , 23 , 25 , 3 , 32 , 43 , 45 , 66 , 67 , 'a' , 'abc' } Let's try to get an element from the set using an index 1 print ( sample_set [ 2 ]) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-63-4dd3666b0133> in <module> ----> 1 print ( sample_set [ 2 ] ) TypeError : 'set' object is not subscriptable Accessing the set elements via an index raises a TypeError 1 2 3 4 5 # sets does not allow duplicate values # Let's create a sample set sample_set1 = { 1 , 1 , 2 , 3 , 4 , 5 , 5 , 5 , 6 , 6 , 6 , 6 } print ( sample_set1 ) {1, 2, 3, 4, 5, 6} As we can see here, sample_set1 only allows unique values and removes the duplicates automatically. Also to note that sets are defined within the curly braces { }","title":"Sets"},{"location":"python/python-basics-01/#dictionary","text":"Dictionaries are data structures that store elements as a key-value pair The properties of dictionaries are They have key-value pairs in it Keys are unique and values may have duplicates Can retrieve the value using key Can change the value using key The key is immutable Let's declare a sample dictionary! 1 sample_dict = { \"a\" : \"alpha\" , 1 : \"first\" , 2 : \"second\" , 3 : \"third\" , 1 : \"fourth\" , 5 : \"fourth\" , 6 : \"sixth\" } Here, let's take the first key-value pair \"a\":\"alpha\" . a is the key (left side of the colon :) alpha is the value (right side of the colon :) Let's retrieve the first value using the key. 1 2 3 # retrieve the value alpha using the key 'a' sample_dict [ 'a' ] 'alpha' We mention the key within the [ ] square brackets and retrieve the value. 1 2 3 4 5 # Retrieving the sixth value # We know the key for the value \"Sixth\" is \"6\" # The value can be retrieved using the key \"6\" sample_dict [ 6 ] 'sixth' Dictionaries values are mutable. Let's see with an example. Let's change the 6 th element value to fifth instead of fourth 1 2 3 4 5 # Right now in the sample dict the value for the 6th element is fourth # Let's change it to fifth sample_dict [ \"5\" ] = \"fifth\" print ( sample_dict ) {'a': 'alpha', 1: 'fourth', 2: 'second', 3: 'third', 5: 'fourth', 6: 'sixth', '5': 'fifth'} There are two things to note here We have changed the value of key 5 to \"fifth\" The order of the elements have changed. Dictionaries does not guarantee the order of the elements. Hence, we cannot retrieve values using indexes but, only using keys. Let's try to remove an element from the dictionary. Again, we have to use keys to remove the respective value 1 2 3 4 5 # Lets remove 'alpha' from the sample dict # We can do that using the pop method sample_dict . pop ( \"a\" ) print ( sample_dict ) {1: 'fourth', 2: 'second', 3: 'third', 5: 'fourth', 6: 'sixth', '5': 'fifth'} Note that the value of alpha is removed from the dictionary. Let's try to add a new key-value pair to the dictionary 1 2 3 4 5 # Lets's add the color red to sample_dict # Here the 'color' is the key and 'red' is the value sample_dict [ \"color\" ] = \"red\" print ( sample_dict ) {1: 'fourth', 2: 'second', 3: 'third', 5: 'fourth', 6: 'sixth', '5': 'fifth', 'color': 'red'} As you can see, we have added the key value pair \"color\":\"red\" to the dictionary.","title":"Dictionary"},{"location":"python/python-basics-01/#strings","text":"String is an important datatype in python. It is useful in manipulating text. The properties of string datatypes are String can be retrieved using index String is immutable Let's take an example and see! 1 2 3 # Let's declare a string fav_movie = \"The world is round!\" Let's determine the type of this fav_movie variable. 1 2 # Determining the type type ( fav_movie ) str This states that the type of fav_movie is str which is a string. String can be accessed using indexes. 1 2 3 4 # let's retrieve the first letter from fav_movie # which is \"T\" fav_movie [ 0 ] 'T' String indexes like any other index starts with 0 . Using the index, we can retrieve the corresponding value. Strings are immutable. We cannot change the string. Let's see with an example. 1 2 3 4 # Strings are immutable # Let's try to replace the first letter of fav_movie with S fav_movie [ 0 ] = \"S\" --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-86-5f0b5dce1f58> in <module> 2 # Let's try to replace the first letter of fav_movie with S 3 ----> 4 fav_movie [ 0 ] = \"S\" TypeError : 'str' object does not support item assignment This gives us a TypeError as the string object fav_movie is immutable.","title":"Strings"},{"location":"python/python-basics-01/#type_conversion","text":"Type conversion is also technically called type-casting. That is to say that we convert one type to another type. E.g A string is converted to an integer and so on. 1 2 3 # Let's declare an integer a = 10 print ( type ( a )) <class 'int'> Here, 10 is of the type integer. Let's declare a string and check the type 1 2 3 4 # Declaring a string greetings = \"Hello, World!\" print ( type ( greetings )) <class 'str'> So, greetings is of type String. So, when is type conversion needed? Let's try to add greetings to a . Python gives us the ability to add using the operator + 1 2 3 4 # add 'a' to 'greetings' new_word = a + greetings print ( new_word ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-90-de1961e67afc> in <module> 1 # add 'a' to 'greetings' 2 ----> 3 new_word = a + greetings 4 print ( new_word ) TypeError : unsupported operand type(s) for +: 'int' and 'str' This results in a TypeError . Why? a is of type integer and greetings is of type String. So, we cannot add both of them. Let's try to convert a of type int to type string . We need to use type conversion for that. 1 2 3 4 5 6 7 # changing from int type to string type # here str function will convert a of type int to string new_a = str ( a ) # Let's check the type after type casting print ( type ( new_a )) <class 'str'> Now, a has been converted to type string . Let's try our above operation again - adding this new_a to greetings 1 2 new_word = new_a + greetings print ( new_word ) 10Hello, World! Now, we have successfully completed type casting. Type casting is needed as python determines the type of the variable during runtime as its a loosely-typed language. Strongly typed languages like Java warrant the user to specify the type of the object during declaration. Similarly, we can type cast list to tuples, tuples to sets etc. Let's see a list conversion to tuple with an example 1 2 3 4 5 6 7 # lets declare a list my_list = [ 'a' , 'z' , 'e' , 'e' , 'z' ] # let's check the type of this list print ( type ( my_list )) <class 'list'> Ok; this is of type list. Remember, list is within [ ] square brackets Let's try to change that to a tuple. 1 2 3 4 5 6 7 # type casting list to tuple my_tuple = tuple ( my_list ) # let's check the type of this tuple print ( type ( my_tuple )) <class 'tuple'> So, we have successfully managed to change the list to a tuple. 1 2 3 # Let see how my_tuple looks like print ( my_tuple ) ('a', 'z', 'e', 'e', 'z') As we can see the list has been converted to tuple. Tuples are within round brackets ( )","title":"Type conversion"},{"location":"python/python-basics-01/#summary","text":"In this post, we saw the python basics - keywords, identifiers and variables. Python gives us a rich set of datatypes starting from int, float, boolean, complex and then gives us robust datatypes like list, set, tuples and dictionaries. We also saw how to slice down lists, retrieve unique values using sets, form immutable objects using tuples and found values with the keys from dictionaries.","title":"Summary"},{"location":"python/python-basics-02/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); By hazeez Mar 29, 2020 in Python Work in Progress Post. # Do not refer to it yet! String Manipulation # We don't have characters in python - only strings 1 2 a = \"Inceptz\" type ( a ) str 1 a [ 2 ] 'c' 1 print ( a [ 2 :]) ceptz 1 print ( a [ 2 : 4 ]) ce 1 2 3 # Getting an input a = input ( \"Enter a input:\" ) Enter a input:Azeez 1 print ( a ) Azeez 1 2 3 # Let's try to input a number anum = input ( \"Enter a number:\" ) Enter a number:2 1 2 # lets check the type of anum type ( anum ) str 1 2 3 # to change the str to int, we have to type cast anum = int ( anum ) 1 2 # let's check the type again type ( anum ) int Negative indexing 1 print ( a [ - 6 : - 1 ]) Azee 1 2 # finding the length of the string print ( len ( a )) 5 String Functions # 1 2 3 4 # what are the methods that string has? # use the dir command dir ( a ) 1 2 3 # Let's declare a string mystring = \"hello, world!\" 1 2 3 # capitalize mystring mystring . capitalize () 'Hello, world!' 1 2 3 # change string to upper case mystring . upper () 'HELLO, WORLD!' 1 2 # change string to lower case mystring . lower () 'hello, world!' 1 2 3 4 5 6 7 8 9 10 # Multiple line string assignment # There are two methods # First method - triple single quotes - ''' ''' # Second method - triple double quotes - \"\"\" \"\"\" description = \"\"\" Inceptez is one of the top institutions in chennai. Their curriculum is awesome. \"\"\" 1 description . upper () '\\nINCEPTEZ IS ONE OF THE TOP INSTITUTIONS IN CHENNAI. \\nTHEIR CURRICULUM IS AWESOME.\\n' 1 2 3 4 5 6 7 8 9 10 11 # Let's take a movie review review = \"\"\"The headphones are awesome. I totally loved it!\"\"\" # Understand the sentiment # we have to take the adjective of the sentence # there will be a sentiment corpus # need to convert all the words to lower case review . lower () 'the headphones are awesome. \\ni totally loved it!' Encodings in python Different types of encodings in python What is an encoding? 1 2 from encodings.aliases import aliases aliases 1 review . encode ( \"utf-16\" ) 1 2 review = review print ( review ) The headphones are awesome. I totally loved it! 1 review = \"The headphones are Awesome. But, I the size didn't match\" 1 review . split ( '.' ) ['The headphones are Awesome', \" But, I the size didn't match\"] 1 2 hal = \"Excellent performance in Italy\" \"Italy\" in hal True 1 2 3 mentor1 = 'Laxmi' mentor2 = 'Sheik' print ( \"Mentor1 is {} , Mentor2 is {} \" . format ( mentor1 , mentor2 )) Mentor1 is Laxmi, Mentor2 is Sheik 1 2 # If order needs to be changed, we can do that print ( \"Mentor1 is {1} , Mentor2 is {0} \" . format ( mentor1 , mentor2 )) Mentor1 is Sheik, Mentor2 is Laxmi 1 2 # Escape characters to print special characters print ( \"The classes we see now are the most \\' important \\' '\" ) The classes we see now are the most 'important'' 1 2 3 4 5 # Let's take a review that has n number of spaces review = \" The headphones are Awesome. But, I the size didn't match\" # lstrip review . lstrip () \"The headphones are Awesome. But, I the size didn't match\" 1 2 review = \"The headphones are Awesome. But, I the size didn't match \" review . rstrip () \"The headphones are Awesome. But, I the size didn't match\" 1 2 3 # what if we have spaces at the start and end review = \" The headphones are Awesome. But, I the size didn't match \" review . strip () \"The headphones are Awesome. But, I the size didn't match\" 1 2 3 # what if i need to remove other character than whitespace review = \"The headphones are Awesome. But, I the size didn't match *************\" review . strip ( \"*\" ) \"The headphones are Awesome. But, I the size didn't match \" 1 2 3 # what if we need to remove a list of characters review = \"The headphones are Awesome. But, I the size didn't match !!!!!!!!!!!*************$$$$$$\" review . strip ( \"!*$\" ) \"The headphones are Awesome. But, I the size didn't match \" 1 2 review = \"The headphones are Awesome. But, I the size didn't match \" review . strip () \"The headphones are Awesome. But, I the size didn't match\" 1 2 3 # this does not remove the white space review1 = review . split () print ( review1 ) ['The', 'headphones', 'are', 'Awesome.', 'But,', 'I', 'the', 'size', \"didn't\", 'match'] 1 \" \" . join ( review1 ) \"The headphones are Awesome. But, I the size didn't match\" 1 2 3 # count of words in a string review = \"there are apples in the table. 2 green apples and 5 red apples\" review . count ( \"apples\" ) 3 1 review . replace ( \"apples\" , \"oranges\" ) 'there are oranges in the table. 2 green oranges and 5 red oranges' Conditional loops # python supports the usual logical conditions from mathematics: Equals: a==b Not Equals: a!=b Less than: a < b Less than or equal to a <= b Greather than: a > b Greater than or equal to: a >= b An \"if statement\" is written by using the if keyword Check for condition: satisfied 1) Execute a statement Check for condition: Not satisfied 2) Execute the else statement 1 2 3 4 5 name = input ( \"please input a name:\" ) if name == \"azeez\" : print ( \"name is azeez\" ) else : print ( \"name is\" , name ) please input a name:sarath name is sarath 1 2 3 4 5 6 7 # demo of greater than or less than a = int ( input ( \"Enter number a:\" )) b = int ( input ( \"Enter number b\" )) sum = a + b if ( sum >= 10 ): print ( sum ) Enter number a:5 Enter number b5 10 1 2 3 4 5 6 7 8 9 # demo of else a = int ( input ( \"Enter number a:\" )) b = int ( input ( \"Enter number b\" )) sum = a + b if ( sum >= 10 ): print ( sum ) else : print ( \"total is lesser than 10\" ) Enter number a:3 Enter number b4 total is lesser than 10 1 2 3 4 5 6 7 8 9 10 11 12 13 # demo of elif a = int ( input ( \"Enter number a: \" )) b = int ( input ( \"Enter number b: \" )) sum = a + b if ( sum >= 10 ): print ( sum ) elif ( sum <= 7 ): print ( \"total is lesser than 7\" ) elif ( sum == 0 ): print ( \"total is 0\" ) else : print ( \"nothing is verified\" ) Enter number a: 0 Enter number b: -1 total is lesser than 7 Shortcut for if else 1 2 3 a = 100 b = 20 if a > b : print ( \"a is bigger than b\" ) a is bigger than b 1 2 3 dhoni = 60 kohli = 30 print ( \"Dhoni is the best\" ) if dhoni > kohli else print ( \"Kohli is the best\" ) Dhoni is the best Logical operators can be made use","title":"Python basics 02"},{"location":"python/python-basics-02/#work_in_progress_post","text":"Do not refer to it yet!","title":"Work in Progress Post."},{"location":"python/python-basics-02/#string_manipulation","text":"We don't have characters in python - only strings 1 2 a = \"Inceptz\" type ( a ) str 1 a [ 2 ] 'c' 1 print ( a [ 2 :]) ceptz 1 print ( a [ 2 : 4 ]) ce 1 2 3 # Getting an input a = input ( \"Enter a input:\" ) Enter a input:Azeez 1 print ( a ) Azeez 1 2 3 # Let's try to input a number anum = input ( \"Enter a number:\" ) Enter a number:2 1 2 # lets check the type of anum type ( anum ) str 1 2 3 # to change the str to int, we have to type cast anum = int ( anum ) 1 2 # let's check the type again type ( anum ) int","title":"String Manipulation"},{"location":"python/python-basics-02/#string_functions","text":"1 2 3 4 # what are the methods that string has? # use the dir command dir ( a ) 1 2 3 # Let's declare a string mystring = \"hello, world!\" 1 2 3 # capitalize mystring mystring . capitalize () 'Hello, world!' 1 2 3 # change string to upper case mystring . upper () 'HELLO, WORLD!' 1 2 # change string to lower case mystring . lower () 'hello, world!' 1 2 3 4 5 6 7 8 9 10 # Multiple line string assignment # There are two methods # First method - triple single quotes - ''' ''' # Second method - triple double quotes - \"\"\" \"\"\" description = \"\"\" Inceptez is one of the top institutions in chennai. Their curriculum is awesome. \"\"\" 1 description . upper () '\\nINCEPTEZ IS ONE OF THE TOP INSTITUTIONS IN CHENNAI. \\nTHEIR CURRICULUM IS AWESOME.\\n' 1 2 3 4 5 6 7 8 9 10 11 # Let's take a movie review review = \"\"\"The headphones are awesome. I totally loved it!\"\"\" # Understand the sentiment # we have to take the adjective of the sentence # there will be a sentiment corpus # need to convert all the words to lower case review . lower () 'the headphones are awesome. \\ni totally loved it!'","title":"String Functions"},{"location":"python/python-basics-02/#conditional_loops","text":"python supports the usual logical conditions from mathematics: Equals: a==b Not Equals: a!=b Less than: a < b Less than or equal to a <= b Greather than: a > b Greater than or equal to: a >= b An \"if statement\" is written by using the if keyword Check for condition: satisfied 1) Execute a statement Check for condition: Not satisfied 2) Execute the else statement 1 2 3 4 5 name = input ( \"please input a name:\" ) if name == \"azeez\" : print ( \"name is azeez\" ) else : print ( \"name is\" , name ) please input a name:sarath name is sarath 1 2 3 4 5 6 7 # demo of greater than or less than a = int ( input ( \"Enter number a:\" )) b = int ( input ( \"Enter number b\" )) sum = a + b if ( sum >= 10 ): print ( sum ) Enter number a:5 Enter number b5 10 1 2 3 4 5 6 7 8 9 # demo of else a = int ( input ( \"Enter number a:\" )) b = int ( input ( \"Enter number b\" )) sum = a + b if ( sum >= 10 ): print ( sum ) else : print ( \"total is lesser than 10\" ) Enter number a:3 Enter number b4 total is lesser than 10 1 2 3 4 5 6 7 8 9 10 11 12 13 # demo of elif a = int ( input ( \"Enter number a: \" )) b = int ( input ( \"Enter number b: \" )) sum = a + b if ( sum >= 10 ): print ( sum ) elif ( sum <= 7 ): print ( \"total is lesser than 7\" ) elif ( sum == 0 ): print ( \"total is 0\" ) else : print ( \"nothing is verified\" ) Enter number a: 0 Enter number b: -1 total is lesser than 7","title":"Conditional loops"},{"location":"statistics/","text":"Statistics Index #","title":"_Stats Home"},{"location":"statistics/#statistics_index","text":"","title":"Statistics Index"},{"location":"statistics/Inferential_stats_basics/","text":"Inferential Statistics # In real-life situations, we may not be able to know the entire population; in which case, we use the known random sample to extract information about the unknown population from which the sample is drawn. To put it simply, using the sample to come up with an estimation of the population is called as Inferential statistics Parameter and Statistic # A numerical measure of a population is called a population parameter or more simply, a parameter . Examples of parameters are population mean, population variance, population standard deviation, etc. A numerical measure of the sample is called a sample statistic , or simply a statistic . Examples of sample statistics are the sample mean, sample variance, sample standard deviation, etc. In inferential statistics, population parameters are estimated by sample statistics. When a simple statistic is used to estimate the population parameter, the statistic is called an estimator of the parameter. Notations of parameter and statistic # Population Symbol Sample Symbol Mean \\mu \\mu Mean x x Variance \\sigma^2 \\sigma^2 Variance s^2 s^2 Std deviation \\sigma \\sigma Std deviation s s To summarize, we have the following relationships between a sample statistic and a population parameter \\bar{x} \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\mu \\\\ \\\\ S^2 \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\sigma^2 \\bar{x} \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\mu \\\\ \\\\ S^2 \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\sigma^2 i.e the sample mean estimates the population mean and the sample variance estimates the population variance. Confidence levels and confidence intervals # To understand confidence level and confidence intervals, we will need to understand what a population distribution and a sampling distribution is. Population distribution # The population is the entire set of values/data points that we are interested in. For example, if we know the age of all Indian residents, it is called the population. The population characteristics are mean \\mu \\mu , standard deviation \\sigma \\sigma , median, percentiles, etc. Sampling distribution # The sample is a subset of the population which is used to estimate the population characteristic. This sample has a few characteristics like mean \\bar{x} \\bar{x} , standard deviation s s , etc. Samples are used to draw inferences about the population. This is done by drawing samples and computing the sample statistics. When the sample statistics are plotted, the distribution (represented as a histogram) is called the sampling distribution. Note The sampling distribution is also called as theoretical distribution How is a sampling distribution plotted? From the population, we take N N samples of n n size and compute the mean. Then we compute the frequency of these means and plot a histogram that would give the sampling distribution. Note If the population is normally distributed, then the sampling distribution will also be normally distributed. Standard error # The standard error SE SE is the standard deviation of the sampling distribution Standard error of the mean # The standard error (or deviation) of the mean can be expressed as SE = \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} SE = \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} where \\sigma \\sigma is the standard deviation of the population n n is the size of the sample Since the standard deviation of the population is rarely known, the standard error of the mean is usually estimated as the sample standard deviation divided by the square root of the sample size i.e. SE = \\sigma_{\\bar{x}} \\approx \\frac{s}{\\sqrt n} SE = \\sigma_{\\bar{x}} \\approx \\frac{s}{\\sqrt n} where s s is the sample standard deviation n n is the size of the sample. Confidence interval # When we use samples to project population estimates, we cannot be CERTAIN that it will be accurate. There is an amount of uncertainty that needs to be factored in/calculated. So we come up with a range and state that the population parameter (e.g. population means \\mu \\mu ) would fall within this range. This is called the confidence interval Definition A confidence interval is the range of numbers that is believed to include an unknown population parameter. Confidence level Let's recall the empirical split. Recall Empirical rule/split The empirical rules state that for a normal distribution, nearly all of the data will fall within three standard deviations of the mean. 68% data will fall within 1 standard deviation 95% data will fall within 2 standard deviation 99.7% data will fall within 3 standard deviation This rule is called as the 3 sigma rule or 68-95-99.7 rule We know that if a population is normally distributed, then 68% of the data will fall under 1 standard deviation. This also means that we are 68% confident that the population mean is within the 1 standard deviation range. Similarly, a 95% confidence level means that the population mean will be within the 2 standard deviation range. So confidence levels are expressed as a percentage (for example, a 95% confidence level). It means that should you repeat an experiment or survey over and over again, 95 percent of the time your results will match the results you get from a population. Formula The formula for computing the confidence interval is CI = \\mu \\pm z *SE CI = \\mu \\pm z *SE where \\mu \\mu is the mean, z z is the quantile, SE SE is the standard error Most of the time, the population mean is not known. In that case, the formula becomes CI = \\bar{x} \\pm z *SE CI = \\bar{x} \\pm z *SE where \\bar{x} \\bar{x} is the mean of the sampling distribution. Substituting the standard error formula (mentioned above), we get CI = \\bar{x} \\pm z * \\frac{\\sigma}{\\sqrt n} CI = \\bar{x} \\pm z * \\frac{\\sigma}{\\sqrt n} This would give the confidence interval - the range within which the population parameter would fall. Margin of error # The margin of error is the range of the expected variation for a given survey result. If we keep repeating the survey using the same methodology, the results of the survey would fall within that range of variation. In short, half of the confidence interval is called the margin of error. The margin of error (ME) is denoted by the formula z * SE z * SE which is ME = z * \\frac{\\sigma}{\\sqrt n} ME = z * \\frac{\\sigma}{\\sqrt n} The following image represents the confidence interval and the margin of error in a sampling distribution. Example # A survey was taken of US companies that do business with firms in India. One of the survey questions was: Approximately how many years has your company been trading with firms in India? A random sample of 44 responses to this question yielded a mean of 10.455 years . Suppose the population standard deviation for this question is 7.7 years, construct a 90% confidence interval of the mean number of years that a company has been trading in India for the population of US companies trading with firms in India. Known parameters n = 44 \\\\ \\bar{x} = 10.455 \\\\ \\sigma = 7.7 \\\\ confidence \\space level = 90% n = 44 \\\\ \\bar{x} = 10.455 \\\\ \\sigma = 7.7 \\\\ confidence \\space level = 90% z value for 90% confidence interval is 1.64 To find: confidence interval that has the population mean Solution approach We know the formula for the confidence interval CI = \\bar{x} \\pm z * \\frac{\\sigma}{\\sqrt n} CI = \\bar{x} \\pm z * \\frac{\\sigma}{\\sqrt n} substituting, we get CI = 10.455 \\pm 1.64 * \\frac{7.7}{\\sqrt 44} CI = 10.455 \\pm 1.64 * \\frac{7.7}{\\sqrt 44} which is CI = [8.545 , 12.365] CI = [8.545 , 12.365] The analyst is 90% confident that the actual population mean number of trading years of firms would be between 8.545 and 12.365. Example 2 # The lung function in 57 people is tested using FEVI (Forced Expiratory Volume in 1 Second) measurements. The mean FEVI value for this sample is 4.062 litres and standard deviation , s is 0.67 litres. Construct the 95% Confidence Interval. Data 2.85 3.42 3.7 4.14 4.47 4.9 2.85 3.48 3.75 4.16 4.5 5 2.98 3.5 3.78 4.2 4.5 5.1 3.04 3.54 3.83 4.2 4.56 5.1 3.1 3.54 3.9 4.3 4.68 5.2 3.1 3.57 3.96 4.3 4.7 5.3 3.19 3.6 4.05 4.32 4.71 5.43 3.2 3.6 4.08 4.44 4.78 3.3 3.69 4.1 4.47 4.8 3.39 3.7 4.14 4.47 4.8 Known parameters \\bar{x} = 4.062 \\\\ s = 0.67 \\\\ n = 57 \\bar{x} = 4.062 \\\\ s = 0.67 \\\\ n = 57 The z value for a 95% confidence level is 1.96 Solution approach We know the formula for the confidence interval CI = \\bar{x} \\pm z * \\frac{\\sigma}{\\sqrt n} CI = \\bar{x} \\pm z * \\frac{\\sigma}{\\sqrt n} substituting, we get CI = 4.062 \\pm 1.96 * \\frac{0.67}{\\sqrt 57} CI = 4.062 \\pm 1.96 * \\frac{0.67}{\\sqrt 57} CI = [3.89, 4.23] CI = [3.89, 4.23] Additional material What happens to confidence interval as confidence level changes? Refer to this article for the answer. Z-Score # Z-score denotes the number of units of standard deviation from the mean the data point is. A Z-score will also help in finding the probability i.e. area under the curve of where the data point would fall in the normal distribution More info here Formula Z-score is derived using the formula z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma} where x is the data point we are interested in, \\mu \\mu is the mean and \\sigma \\sigma is the standard deviation. Example # Take a look at the weight of newborn babies. Suppose that the mean weight of newborns is 7.5 pounds and the standard deviation is 1.25 pounds . Say you\u2019re interested in determining the probability that a newborn weighs less than 6 pounds. How do you do that? Known parameters \\mu = 7.5 \\\\ \\sigma = 1.25 \\\\ data \\space point \\space x = 6 \\mu = 7.5 \\\\ \\sigma = 1.25 \\\\ data \\space point \\space x = 6 To find the probability of the new-born weighing less than 6 pounds Solution approach We know the formula of the z-score. z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma} substituting the values, we get z = \\frac{6 - 7.5}{1.25} = -1.20 z = \\frac{6 - 7.5}{1.25} = -1.20 The graph would look like the below To find the probability, the z-table is used. Note There are two Z-tables - one for a positive value of Z and other for a negative value of Z. Here, the value of Z is -1.20 which is on the negative side and hence, we will be using the negative z table. -1.20 is read in the table as -1.2 in the rows and 0 in the columns - highlighted below. The probability that the baby is born less than 6 pounds is 0.1150 or 11.5%. [See image below] What is the probability that the new-born baby might weigh more than 10 pounds? Let's visualize it. Solution approach Let's compute the Z-score We know the formula of the z-score. z = \\frac{x - \\mu}{\\sigma} z = \\frac{x - \\mu}{\\sigma} substituting the values, we get z = \\frac{10 - 7.5}{1.25} = 2.0 z = \\frac{10 - 7.5}{1.25} = 2.0 The graph would look like the below - the shaded area is the probability of newborn weighing less than 10 pounds Looking into the z-table for positive scores, we get the probability value as 0.97725 i.e 97.7% of the newborn is less than or equal to 10 pounds. The question is about the probability of babies weighing more than 10 pounds To find that, we have to subtract the babies weighing less than or equal to 10 pounds from 1 i.e. babies weighing more than 10 pounds = 1 - babies weighing less than 10 pounds Note We are subtracting from 1 because the total probability is equal to 1 So we get Babies weighing more than 10 pounds = 1 - 0.9772 = 0.0228 = 1 - 0.9772 = 0.0228 or 2.2\\% 2.2\\% Summary # To summarize, we started with what is inferential statistics - using the samples to estimate the population, what is a parameter and a statistic, confidence interval and confidence level, standard error and the margin of error, Z- scores and a few examples in each of these. Next post Hypothesis Testing","title":"Inferential Stats - Basics"},{"location":"statistics/Inferential_stats_basics/#inferential_statistics","text":"In real-life situations, we may not be able to know the entire population; in which case, we use the known random sample to extract information about the unknown population from which the sample is drawn. To put it simply, using the sample to come up with an estimation of the population is called as Inferential statistics","title":"Inferential Statistics"},{"location":"statistics/Inferential_stats_basics/#parameter_and_statistic","text":"A numerical measure of a population is called a population parameter or more simply, a parameter . Examples of parameters are population mean, population variance, population standard deviation, etc. A numerical measure of the sample is called a sample statistic , or simply a statistic . Examples of sample statistics are the sample mean, sample variance, sample standard deviation, etc. In inferential statistics, population parameters are estimated by sample statistics. When a simple statistic is used to estimate the population parameter, the statistic is called an estimator of the parameter.","title":"Parameter and Statistic"},{"location":"statistics/Inferential_stats_basics/#notations_of_parameter_and_statistic","text":"Population Symbol Sample Symbol Mean \\mu \\mu Mean x x Variance \\sigma^2 \\sigma^2 Variance s^2 s^2 Std deviation \\sigma \\sigma Std deviation s s To summarize, we have the following relationships between a sample statistic and a population parameter \\bar{x} \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\mu \\\\ \\\\ S^2 \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\sigma^2 \\bar{x} \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\mu \\\\ \\\\ S^2 \\space \\space \\space \\overrightarrow{estimates} \\space \\space \\space \\sigma^2 i.e the sample mean estimates the population mean and the sample variance estimates the population variance.","title":"Notations of parameter and statistic"},{"location":"statistics/Inferential_stats_basics/#confidence_levels_and_confidence_intervals","text":"To understand confidence level and confidence intervals, we will need to understand what a population distribution and a sampling distribution is.","title":"Confidence levels and confidence intervals"},{"location":"statistics/Inferential_stats_basics/#population_distribution","text":"The population is the entire set of values/data points that we are interested in. For example, if we know the age of all Indian residents, it is called the population. The population characteristics are mean \\mu \\mu , standard deviation \\sigma \\sigma , median, percentiles, etc.","title":"Population distribution"},{"location":"statistics/Inferential_stats_basics/#sampling_distribution","text":"The sample is a subset of the population which is used to estimate the population characteristic. This sample has a few characteristics like mean \\bar{x} \\bar{x} , standard deviation s s , etc. Samples are used to draw inferences about the population. This is done by drawing samples and computing the sample statistics. When the sample statistics are plotted, the distribution (represented as a histogram) is called the sampling distribution. Note The sampling distribution is also called as theoretical distribution","title":"Sampling distribution"},{"location":"statistics/Inferential_stats_basics/#standard_error","text":"The standard error SE SE is the standard deviation of the sampling distribution","title":"Standard error"},{"location":"statistics/Inferential_stats_basics/#standard_error_of_the_mean","text":"The standard error (or deviation) of the mean can be expressed as SE = \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} SE = \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} where \\sigma \\sigma is the standard deviation of the population n n is the size of the sample Since the standard deviation of the population is rarely known, the standard error of the mean is usually estimated as the sample standard deviation divided by the square root of the sample size i.e. SE = \\sigma_{\\bar{x}} \\approx \\frac{s}{\\sqrt n} SE = \\sigma_{\\bar{x}} \\approx \\frac{s}{\\sqrt n} where s s is the sample standard deviation n n is the size of the sample.","title":"Standard error of the mean"},{"location":"statistics/Inferential_stats_basics/#confidence_interval","text":"When we use samples to project population estimates, we cannot be CERTAIN that it will be accurate. There is an amount of uncertainty that needs to be factored in/calculated. So we come up with a range and state that the population parameter (e.g. population means \\mu \\mu ) would fall within this range. This is called the confidence interval","title":"Confidence interval"},{"location":"statistics/Inferential_stats_basics/#margin_of_error","text":"The margin of error is the range of the expected variation for a given survey result. If we keep repeating the survey using the same methodology, the results of the survey would fall within that range of variation. In short, half of the confidence interval is called the margin of error. The margin of error (ME) is denoted by the formula z * SE z * SE which is ME = z * \\frac{\\sigma}{\\sqrt n} ME = z * \\frac{\\sigma}{\\sqrt n} The following image represents the confidence interval and the margin of error in a sampling distribution.","title":"Margin of error"},{"location":"statistics/Inferential_stats_basics/#example","text":"A survey was taken of US companies that do business with firms in India. One of the survey questions was: Approximately how many years has your company been trading with firms in India? A random sample of 44 responses to this question yielded a mean of 10.455 years . Suppose the population standard deviation for this question is 7.7 years, construct a 90% confidence interval of the mean number of years that a company has been trading in India for the population of US companies trading with firms in India.","title":"Example"},{"location":"statistics/Inferential_stats_basics/#example_2","text":"The lung function in 57 people is tested using FEVI (Forced Expiratory Volume in 1 Second) measurements. The mean FEVI value for this sample is 4.062 litres and standard deviation , s is 0.67 litres. Construct the 95% Confidence Interval.","title":"Example 2"},{"location":"statistics/Inferential_stats_basics/#z-score","text":"Z-score denotes the number of units of standard deviation from the mean the data point is. A Z-score will also help in finding the probability i.e. area under the curve of where the data point would fall in the normal distribution More info here","title":"Z-Score"},{"location":"statistics/Inferential_stats_basics/#example_1","text":"Take a look at the weight of newborn babies. Suppose that the mean weight of newborns is 7.5 pounds and the standard deviation is 1.25 pounds . Say you\u2019re interested in determining the probability that a newborn weighs less than 6 pounds. How do you do that?","title":"Example"},{"location":"statistics/Inferential_stats_basics/#summary","text":"To summarize, we started with what is inferential statistics - using the samples to estimate the population, what is a parameter and a statistic, confidence interval and confidence level, standard error and the margin of error, Z- scores and a few examples in each of these.","title":"Summary"},{"location":"statistics/descriptive_stats_basics/","text":"Descriptive Statistics # Statistics Basics # What is statistics? # A branch of mathematics taking and transforming numbers into useful information for decision-makers. It originated from the word Stata which means state - i.e. understanding about the state of data. It is a way to get information from data. Complex data can be easily translated into meaningful information using statistics. Example # A college in the US has students from the following countries. Which country is in the majority? US Canada China US India England Mexico Japan China Germany China China Germany US Japan India US China India Japan US Japan India US England China Canada US India China Sweden Mexico India China India Mexico Pakistan Japan China US China US Japan China Japan US India Germany China Japan It is very difficult to get that information from this data when presented this way. However, when the same data is converted into meaningful information, conclusions can arrive very easily. Translating the above table into the below table gives us the answer much faster. This is the use of statistics. Count Frequency Canada 2 China 12 England 2 Germany 3 India 8 Japan 8 Mexico 3 Pakistan 1 Sweden 1 US 10 Example 2 # Data can be misleading. A parent changes the school of their Son who is studying in 11^{th} 11^{th} standard since his academic results are not good in 10^{th} 10^{th} Standard in his current School. They change Student A from ABC school to XYZ school The result was different Ranked 15^{th} 15^{th} in ABC school Ranked 2^{nd} 2^{nd} in XYZ school What's the conclusion? Has the student improved? Well, it depends on the number of students studying school. It seems that in the XYZ school, only 2 students were studying and hence he came 2^{nd} 2^{nd} . Such clarity of information can be obtained via statistics. The knowledge of statistics allows us to make better sense of the use of numbers. Statistics in a nutshell The branch of statistics can be summed up as Collecting data Analyzing data Interpreting data Presenting data Statistics classification # Statistics can be classified into two types Descriptive statistics Presenting, organizing and summarizing data is called descriptive statistics Inferential statistics Drawing conclusions about a population based on data observed in a sample is called as inferential statistics We talked about the population and sample in the definition of the inferential statistics above. What does it mean? Population is the entire set of data points available e.g. elections, 10-year census. Sample is a subset of the data points taken from the population. e.g. opinion polls Parameter and Statistic A parameter is a descriptive measure of the population e.g. population mean, population variance, population standard deviation, etc A statistic is a descriptive measure of a sample e.g. sample mean, sample variance, sample standard deviation, etc. Types of variables # Variables are the fancy name of data in statistics. Variables can be classified as Qualitative or Quantitative Nominal variables Variables that have a name is called as nominal variables E.g. Gender (male/female), Ethnicity (Indian, American, Russian), etc Ordinal variables Variables that are based on order/rank E.g. Movie ratings (1 star to 5 stars), Fortune 50 rankings, etc. Discrete variables Anything that can be counted is a discrete variable e.g number of cats in a room, number of tires in a car Anything that can be measured is a continuous variable e.g. Weight of sugar, time, age, etc Check your understanding # Numerical or categorical variable? Age Gender Major Units Housing GPA 18 Male Psychology 16 Dorm 3.6 21 Male Nursing 15 Parents 3.1 20 Female Business 16 Apartment 2.8 Age, Units, GPA are numerical variable Gender, Major, Housing are categorical variable Dependent variables The variable that is altered based on the input variables. Ideally, there is a relationship between the input variable that makes the dependent variable change. E.g how much sleep you had before you took the test? Studies is a factor that contributes to less sleep. Independent variables The variable that stands alone and is unaffected by other variables E.g. What one eats with the age of the person? Both are un-related and hence are independent variables. Characteristics of frequency distribution # There are four characteristics of a frequency distribution. They are Modality Symmetry Central tendency Variability Modality # The modality is determined by the number of peaks. A unimodal distribution has one peak, a bimodal distribution has two peaks. Symmetry # A distribution can be symmetric or asymmetric . Symmetric distribution A symmetric distribution is a type of distribution where the left side of the distribution mirrors the right side. Asymmetric distribution An asymmetric distribution is a type of distribution where the left side and the right side are NOT mirrors of each other. It can be either negatively skewed or positively skewed. Negatively skewed In a negatively skewed distribution, the long tail is towards the smaller values Positively skewed In a positively skewed distribution, the long tail is towards the larger values Measure of central tendency # A measure of Central Tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data . In other words, the Central Tendency computes the \"center\" around which the data is distributed. There are three measures of central tendency Median Mode Mean Median Median is the central value is a set of data. To find the median , we arrange the observations in order from smallest to largest value. If there is an odd number of observations, the median is the middle value. If there is an even number of observations, the median is the average of the two middle values. The formula for computing the median is Median = \\space \\frac{n + 1}{2} Median = \\space \\frac{n + 1}{2} Note Median is less influenced by outliers. Mode The mode of a set of data values is the value that appears most often. Mean The mean (average) of a data set is found by adding all numbers in the data set and then dividing by the number of values in the set The formula for computing the mean is Mean \\space \\mu = \\frac{\\sum(x)}{n} Mean \\space \\mu = \\frac{\\sum(x)}{n} Measures of central tendency is not sufficient Let's take an example. The following table shows the scores of two players across a series of matches. We need to select one player out of these two. Who would we choose? Note The mean, median and mode are the same. Match Player A Player B 1 40 40 2 40 35 3 7 45 4 40 52 5 0 30 6 90 40 7 3 29 8 11 43 9 120 37 Sum 351 351 Mean 39 39 Median 40 40 In cases like these, where we are not able to conclude based on the mean, median and mode, we go with the measures of dispersion Measures of dispersion # Measures of Dispersion describe the data spread or how far the measurements are from the center. The measures of dispersion are Range Variance Standard deviation Range The range is the difference between the maximum value and the minimum value is a data set. Variance Variance is the difference between each data point and the mean of the data set. It measures how far a set of numbers are spread out from their average value. Variance is given by the formula Variance \\space = \\frac{\\sum(x - \\mu)^2}{n} Variance \\space = \\frac{\\sum(x - \\mu)^2}{n} Standard deviation The Standard Deviation is a measure of how spread out the numbers are. It is denoted by the letter \\sigma \\sigma A large standard deviation indicates that the data points can spread far from the mean and a small standard deviation indicates that they are clustered closely around the mean. The formula for computing the standard deviation (of a population) is \\sigma = \\sqrt \\frac{(x - \\bar{x})^2}{n} \\sigma = \\sqrt \\frac{(x - \\bar{x})^2}{n} or standard deviation \\sigma = \\sqrt {variance} \\sigma = \\sqrt {variance} Standard deviation for the sample The formula for computing the standard deviation (of a population) is \\sigma = \\sqrt \\frac{(x - \\bar{x})^2}{n -1} \\sigma = \\sqrt \\frac{(x - \\bar{x})^2}{n -1} where, n - 1 n - 1 is the degree of freedom. With the measures of dispersion, let's revisit the previous example and compute the standard deviation Match Player A Player B 1 40 40 2 40 35 3 7 45 4 40 52 5 0 30 6 90 40 7 3 29 8 11 43 9 120 37 Sum 351 351 Mean 39 39 Median 40 40 Std Dev 41.518 7.280 We will select player B as the standard deviation is less which means he is more consistent. Percentile / Quartile # Percentile # A percentile (or a centile) is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations falls. For example, the 20 th percentile is the value (or score) below which 20% of the observations may be found. Nth percentile states that there are at least N% of values less than or equal to this value and (100-N) values are greater or equal to this value The formula for finding the number in the N'th percentile is i = \\frac{N}{100}* n i = \\frac{N}{100}* n where N N is the percentile we are interested in and n n is the number of values Key points If i is decimal then round off to next value If i is an integer then take the average of i and i+1 value Computing percentile in Excel Data 3310 3355 3450 3480 3480 3490 3520 3540 3550 3650 3730 3925 Solution approach First, populate the data in excel and then sort it in ascending order Use the PERCENTILE.INC function to get the percentile. Use the PERCENTRANK.INC function to find the percentile for each value in a range Quartile # Quartile means dividing data % into 4 parts Q1 - First Quartile - 25 th percentile Q2 - Second Quartile - 50 th percentile (Median) Q3 - Third Quartile - 75 th percentile Inter Quartile Range (IQR) IQR is the difference between the third quartile and the first quartile IQR = Q3 - Q1 IQR = Q3 - Q1 Let's take the same data and compute the IQR for it Use the excel function QUARTILE.INC to compute the corresponding quartile. Coefficient of Variation # The coefficient of variation shows the extent of variability of data in a sample about the mean of the population. The lesser the coefficient of variation, the better. It is the ratio of the standard deviation to the mean. The formula for computing the coefficient of variation (CV) is CV= \\frac{\\sigma}{\\mu} CV= \\frac{\\sigma}{\\mu} where: \\sigma \\sigma = standard deviation \\mu \\mu = mean\u200b Example # Let's take an example and see why the coefficient of variation plays an important part. Data In an Under 19 World Cup selection squad for 2018, the BCCI needs to select 1 player based on the current performance in 2017 - 2018 Ranji Trophy. There are 2 players with similar stats and the board is not sure whom to select. Can you help the board members with your analysis? Player X Player Y 40 35 20 40 5 7 20 23 10 20 75 26 100 12 25 30 15 27 15 102 20 18 17 17 11 14 5 7 Solution approach Let's compute the mean and the standard deviation Player X Player Y 40 35 20 40 5 7 20 23 10 20 75 26 100 12 25 30 15 27 15 102 20 18 17 17 11 14 5 7 Mean 27 27 Std Dev 27.5317998 23.70978376 The mean is both the same and the standard deviation is almost the same. How do we decide which player to choose? Let's compute the coefficient of variation using the formula CV= \\frac{\\sigma}{\\mu} CV= \\frac{\\sigma}{\\mu} Player X Player Y 40 35 20 40 5 7 20 23 10 20 75 26 100 12 25 30 15 27 15 102 20 18 17 17 11 14 5 7 Mean 27 27 Std Dev 27.5317998 23.70978376 CV 1.019696289 0.878140139 We see that the CV for the Player Y is much lesser than Player X. Hence, player Y is more consistent and should be selected. Correlation and Covariance # What if we need to measure the association between two variables? So far, we have seen only one variable. We have two measures that will help us determine the association between two variables Covariance Correlation coefficient Covariance # Covariance is a measure of how much two random variables vary together. It's similar to variance, but where variance tells you how a single variable varies , covariance tells you how two variables vary together . here is no meaning of covariance numerical value only sign is useful. Since covariance is the variance for two variables, the formula is Cov(X,Y)= \\frac{\\sum{(X_i - \\bar{X})*(Y_i - \\bar{Y})}}{n} Cov(X,Y)= \\frac{\\sum{(X_i - \\bar{X})*(Y_i - \\bar{Y})}}{n} X_i X_i = Observation point of variable \\bar{X} \\bar{X} = Mean of all observations (X) Y_i Y_i = Observation point of variable Y \\bar{Y} \\bar{Y} = Mean of all observations (Y) n n = Number of observations Interpretation Covariance explains causation between two variables. Higher the value of covariance, the stronger the relationship between two variables. Covariance can range from -\\infty \\space to +\\infty -\\infty \\space to +\\infty Example # Data Let us take two variables - temperature and the number of customers and try to understand the covariance between these two variables Temperature No of customers 97 14 86 11 89 9 84 9 94 15 74 7 Solution approach Let's do the covariance computation in excel Go to Data -> Data Analysis and Select \"Covariance\" Provide the input range and the output range and click \"OK\" Excel does the computation and shows the covariance between temperature and the number of customers which is 18.72 18.72 . A positive covariance suggests that these two variables are positively associated. But, there is one problem. We don't know to how extent these variables are related or in other words, how strong is their relationship? This is where Correlation comes into the picture. Correlation # Correlation determines the degree or the extent to which the two variables are associated. The correlation coefficient ranges from -1 \\space to +1 -1 \\space to +1 Interpretation A coefficient greater than +0.5 +0.5 indicates a positive correlation. A coefficient lesser than -0.5 -0.5 indicates a negative correlation. Formula r = \\frac{Cov(x,y)}{\\sigma_x * \\sigma_y} r = \\frac{Cov(x,y)}{\\sigma_x * \\sigma_y} Where r r is the correlation coefficient \\sigma_x \\sigma_x is the standard deviation of x x \\sigma_y \\sigma_y is the standard deviation of y y Solution approach Let's take the same data and compute the correlation coefficient. Go to Data -> Data analysis -> Correlation and Click \"OK\" Provide the input range and the output range and click \"OK\" We get the correlation coefficient as 0.88 This means the variables temperature and number of customers are highly correlated. Thus correlation provides the extent of correlation between the two variables. Correlation types Positive correlation is a relationship between two variables in which both variables move in the same direction. i.e as x increases, y also increases. E.g. housing prices over time Negative correlation is a relationship between two variables in which both variables move in the opposite direction. i.e. as x increases, y decreases. E.g Health over time When there is no correlation between variables, the graph would look like below Central Limit Theorem (CLT) # The central limit theorem states that if you have a population with mean \u03bc and standard deviation \u03c3 and take sufficiently large random samples ( n \\ge 30 n \\ge 30 ) from the population, then the distribution of the sample means will be approximately normally distributed. The following three properties hold in the central limit theorem \\mu_{\\bar{x}} = \\mu \\mu_{\\bar{x}} = \\mu \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} z = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} z = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} Summary # In this post, we saw what is statistics and how the data is described using descriptive statistics. We have also seen the measures of central tendency, and the measures of variation. We touched upon the coefficient of variation and how to measure the association of two variables using covariance and correlation. We finally concluded with Central Limit Theorem which is crucial when we proceed with inferential statistics. Next post Inferential statistics","title":"Descriptive Statistics"},{"location":"statistics/descriptive_stats_basics/#descriptive_statistics","text":"","title":"Descriptive Statistics"},{"location":"statistics/descriptive_stats_basics/#statistics_basics","text":"","title":"Statistics Basics"},{"location":"statistics/descriptive_stats_basics/#what_is_statistics","text":"A branch of mathematics taking and transforming numbers into useful information for decision-makers. It originated from the word Stata which means state - i.e. understanding about the state of data. It is a way to get information from data. Complex data can be easily translated into meaningful information using statistics.","title":"What is statistics?"},{"location":"statistics/descriptive_stats_basics/#example","text":"A college in the US has students from the following countries. Which country is in the majority? US Canada China US India England Mexico Japan China Germany China China Germany US Japan India US China India Japan US Japan India US England China Canada US India China Sweden Mexico India China India Mexico Pakistan Japan China US China US Japan China Japan US India Germany China Japan It is very difficult to get that information from this data when presented this way. However, when the same data is converted into meaningful information, conclusions can arrive very easily. Translating the above table into the below table gives us the answer much faster. This is the use of statistics. Count Frequency Canada 2 China 12 England 2 Germany 3 India 8 Japan 8 Mexico 3 Pakistan 1 Sweden 1 US 10","title":"Example"},{"location":"statistics/descriptive_stats_basics/#example_2","text":"Data can be misleading. A parent changes the school of their Son who is studying in 11^{th} 11^{th} standard since his academic results are not good in 10^{th} 10^{th} Standard in his current School. They change Student A from ABC school to XYZ school The result was different Ranked 15^{th} 15^{th} in ABC school Ranked 2^{nd} 2^{nd} in XYZ school What's the conclusion? Has the student improved? Well, it depends on the number of students studying school. It seems that in the XYZ school, only 2 students were studying and hence he came 2^{nd} 2^{nd} . Such clarity of information can be obtained via statistics. The knowledge of statistics allows us to make better sense of the use of numbers.","title":"Example 2"},{"location":"statistics/descriptive_stats_basics/#statistics_classification","text":"Statistics can be classified into two types","title":"Statistics classification"},{"location":"statistics/descriptive_stats_basics/#types_of_variables","text":"Variables are the fancy name of data in statistics. Variables can be classified as Qualitative or Quantitative","title":"Types of variables"},{"location":"statistics/descriptive_stats_basics/#check_your_understanding","text":"","title":"Check your understanding"},{"location":"statistics/descriptive_stats_basics/#characteristics_of_frequency_distribution","text":"There are four characteristics of a frequency distribution. They are Modality Symmetry Central tendency Variability","title":"Characteristics of frequency distribution"},{"location":"statistics/descriptive_stats_basics/#modality","text":"The modality is determined by the number of peaks. A unimodal distribution has one peak, a bimodal distribution has two peaks.","title":"Modality"},{"location":"statistics/descriptive_stats_basics/#symmetry","text":"A distribution can be symmetric or asymmetric .","title":"Symmetry"},{"location":"statistics/descriptive_stats_basics/#measure_of_central_tendency","text":"A measure of Central Tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data . In other words, the Central Tendency computes the \"center\" around which the data is distributed. There are three measures of central tendency Median Mode Mean","title":"Measure of central tendency"},{"location":"statistics/descriptive_stats_basics/#measures_of_dispersion","text":"Measures of Dispersion describe the data spread or how far the measurements are from the center. The measures of dispersion are Range Variance Standard deviation","title":"Measures of dispersion"},{"location":"statistics/descriptive_stats_basics/#percentile_quartile","text":"","title":"Percentile / Quartile"},{"location":"statistics/descriptive_stats_basics/#percentile","text":"A percentile (or a centile) is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations falls. For example, the 20 th percentile is the value (or score) below which 20% of the observations may be found. Nth percentile states that there are at least N% of values less than or equal to this value and (100-N) values are greater or equal to this value The formula for finding the number in the N'th percentile is i = \\frac{N}{100}* n i = \\frac{N}{100}* n where N N is the percentile we are interested in and n n is the number of values Key points If i is decimal then round off to next value If i is an integer then take the average of i and i+1 value","title":"Percentile"},{"location":"statistics/descriptive_stats_basics/#quartile","text":"Quartile means dividing data % into 4 parts Q1 - First Quartile - 25 th percentile Q2 - Second Quartile - 50 th percentile (Median) Q3 - Third Quartile - 75 th percentile","title":"Quartile"},{"location":"statistics/descriptive_stats_basics/#coefficient_of_variation","text":"The coefficient of variation shows the extent of variability of data in a sample about the mean of the population. The lesser the coefficient of variation, the better. It is the ratio of the standard deviation to the mean. The formula for computing the coefficient of variation (CV) is CV= \\frac{\\sigma}{\\mu} CV= \\frac{\\sigma}{\\mu} where: \\sigma \\sigma = standard deviation \\mu \\mu = mean\u200b","title":"Coefficient of Variation"},{"location":"statistics/descriptive_stats_basics/#example_1","text":"Let's take an example and see why the coefficient of variation plays an important part.","title":"Example"},{"location":"statistics/descriptive_stats_basics/#correlation_and_covariance","text":"What if we need to measure the association between two variables? So far, we have seen only one variable. We have two measures that will help us determine the association between two variables Covariance Correlation coefficient","title":"Correlation and Covariance"},{"location":"statistics/descriptive_stats_basics/#covariance","text":"Covariance is a measure of how much two random variables vary together. It's similar to variance, but where variance tells you how a single variable varies , covariance tells you how two variables vary together . here is no meaning of covariance numerical value only sign is useful. Since covariance is the variance for two variables, the formula is Cov(X,Y)= \\frac{\\sum{(X_i - \\bar{X})*(Y_i - \\bar{Y})}}{n} Cov(X,Y)= \\frac{\\sum{(X_i - \\bar{X})*(Y_i - \\bar{Y})}}{n} X_i X_i = Observation point of variable \\bar{X} \\bar{X} = Mean of all observations (X) Y_i Y_i = Observation point of variable Y \\bar{Y} \\bar{Y} = Mean of all observations (Y) n n = Number of observations","title":"Covariance"},{"location":"statistics/descriptive_stats_basics/#example_3","text":"","title":"Example"},{"location":"statistics/descriptive_stats_basics/#correlation","text":"Correlation determines the degree or the extent to which the two variables are associated. The correlation coefficient ranges from -1 \\space to +1 -1 \\space to +1","title":"Correlation"},{"location":"statistics/descriptive_stats_basics/#central_limit_theorem_clt","text":"The central limit theorem states that if you have a population with mean \u03bc and standard deviation \u03c3 and take sufficiently large random samples ( n \\ge 30 n \\ge 30 ) from the population, then the distribution of the sample means will be approximately normally distributed. The following three properties hold in the central limit theorem \\mu_{\\bar{x}} = \\mu \\mu_{\\bar{x}} = \\mu \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt n} z = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}} z = \\frac{\\bar{x} - \\mu}{\\sigma_{\\bar{x}}} = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt n}}","title":"Central Limit Theorem (CLT)"},{"location":"statistics/descriptive_stats_basics/#summary","text":"In this post, we saw what is statistics and how the data is described using descriptive statistics. We have also seen the measures of central tendency, and the measures of variation. We touched upon the coefficient of variation and how to measure the association of two variables using covariance and correlation. We finally concluded with Central Limit Theorem which is crucial when we proceed with inferential statistics.","title":"Summary"},{"location":"statistics/hypothesis_testing_01/","text":"Hypothesis Testing # What is Hypothesis Testing? # First let us understand what does the word hypothesis mean. Let us break the word into two parts hypo + thesis . What does thesis mean? thesis means something that has already been proven to be true. e.g Atleast 60% of the adult human body is made up of water. Sounds fine! So what does hypothesis mean? Hypothesis is something that is not yet been proven to be true. Let's come back to the original question! What is hypothesis testing? Can we say that it is the testing of hypothesis or more precisely the process of determining whether a given hypothesis is true or not To sum up, we take the hypothesis, we perform some statistical computations and prove if the hypothesis holds true or not. Null Hypothesis & Alternate Hypothesis # Null hypothesis is the assertion or belief that we hold as true unless we have sufficient evidence to prove otherwise. In statistical terms, we say it as the belief we hold about the value of a population parameter Remember Parameter is for the population and Statistic is for the sample Let's take an example and see what is the null hypothesis and how it is written. We believe that the mean of the population is 500 . Unless we obtain sufficient evidence that it is not 500 , our belief holds true i.e we accept that the mean is 500 So we can write it as null hypothesis: mean = 500 To write it more compactly, we can represent the same thing as \\mathbf{H}_\\mathbf{0} : \\mu = 500 \\mathbf{H}_\\mathbf{0} : \\mu = 500 where the symbol \\space\\mathbf{H}_\\mathbf{0} \\space\\mathbf{H}_\\mathbf{0} denotes Null hypothesis. The opposite of Null hypothesis is called as Alternate Hypothesis . That is, the negation of the null hypothesis. If there is a way to represent the null hypothesis, then there should be a way to represent the alternate hypothesis. Agreed? Ah, I see I have quoted a null hypothesis there! \\mathbf{H}_\\mathbf{1}: \\mu \\ne 500 \\mathbf{H}_\\mathbf{1}: \\mu \\ne 500 where the symbol \\space\\mathbf{H}_\\mathbf{1} \\space\\mathbf{H}_\\mathbf{1} denotes alternate hypothesis Note Since the null hypothesis and the alternate hypothesis are exactly opposite statements, only one can be true. Rejecting one is accepting the other. Let's check our understanding with a few examples Example 1 My broadband company claims that the minimum internet speed they are providing is 150 Mbps. However, I suspect that they are not providing the promised internet speed. Let's write the null and alternate hypothesis for this! \\mathbf{H}_\\mathbf{0}: speed \\ge 150 \\space Mbps \\\\ \\mathbf{H}_\\mathbf{1}: speed < 150 \\space Mbps \\mathbf{H}_\\mathbf{0}: speed \\ge 150 \\space Mbps \\\\ \\mathbf{H}_\\mathbf{1}: speed < 150 \\space Mbps Example 2 The project manager claims that the software defects raised are resolved within 5 business days. I being the quality department manager think it is taking longer to fix the bugs. Can we write the null and alternate hypothesis for this? \\mathbf{H}_\\mathbf{0}: Resolution\\space period \\le 5 \\space days \\\\ \\mathbf{H}_\\mathbf{1}: Resolution\\space period > 5 \\space days \\mathbf{H}_\\mathbf{0}: Resolution\\space period \\le 5 \\space days \\\\ \\mathbf{H}_\\mathbf{1}: Resolution\\space period > 5 \\space days Let's see how we can prove the alternate hypothesis Example 1 To check the internet speed, I visit Speed Test website from my home computer and measure the speed. I do this n n number of times and find that the mean speed is around 100 Mbps. So we reject the null hypothesis and accept the alternate hypothesis - which is, the internet speed is less than 150 Mbps Example 2 To verify the claims of the project manager, I take all of the 5 projects managed by him. He is relatively new to the project manager role and has only managed 5 projects so far. I get the bug details from the bug tracking system - bugzilla / jira for example. I compute the mean of all the bug resolution time by doing some computations ( calculate the difference between the bug start date and bug end date and so on). I find that the resolution time is indeed less than 5 days. So we accept the null hypothesis here i.e. resolution time is less than or equal to 5 business days. Wow, Hypothesis testing is very easy! Level of significance # Life is not easy always, Isn't it? In the above two cases, taking the samples was easy and our agreement / rejection of the null hypothesis was indeed accurate. In real life scenarios, we need to estimate a population parameter based on the sample and things might go wrong. That is to say, we might reject the null hypothesis when it is actually true or we might accept the null hypothesis when it is otherwise. This depends on the sample we take and if we don't have enough samples (or worse picked up wrong samples), things might go wrong. Say for example, the null hypothesis state that the mean of the population is greater than or equal to 500 i.e. \\mathbf{H}_\\mathbf{0}: \\mu \\ge 500 \\mathbf{H}_\\mathbf{0}: \\mu \\ge 500 and the alternate hypothesis is mean less than 500 \\mathbf{H}_\\mathbf{1}: \\mu < 500 \\mathbf{H}_\\mathbf{1}: \\mu < 500 Let's say, we took a sample size of 30 i.e n=30 and found the mean is 499 . Ah! now comes the dilemma - whether we need to accept or reject the null hypothesis? The sample mean is just falling short by 1 from the population mean. We go into self-doubt if we have taken the correct sample size or what happens if the sample size is increased or worst case, should I have to repeat the experiment once again and my manager fires me for wasting the time and effort? So, what we are contemplating here is the probability of the evidence (samples picked) being unfavorable to the null hypothesis. This probablility is called as the p-value If we say the p_value or probability is 2% , it means that our sample has 2% chances of going wrong in rejecting the null hypothesis. If we say the p_value or probability is 30% , it means that our sample has 30% chances of going wrong in rejecting the null hypothesis and our chances of getting a promotion will be adversely impacted. But, we are humans and we need to have a leeway (a threshold) for making some mistakes / error with the samples. This threshold is called as the level of significance In other words, level of significance is the maximum level of risk (maximum acceptable probability - in statistical terms) that we may take in rejecting the null hypothesis while the null hypothesis is actually true. Level of significance is denoted by the letter \\alpha \\alpha (alpha) alpha is normally represented as 1% or 5% or 10% etc. Let's consolidate our understanding! if the p-value is less than the level of significance , we reject the null hypothesis. if the p-value is greater than the level of significance , we accept the null hypothesis. Confused? Yeah, it happened with me. If not, I salute you. Proof for the 1 st statement Let's say the p-value is 2% and the level of significance is 5% . What does this mean? It means the probability of getting our samples wrong is 2% but the maximum risk that we can take is 5% . Correct? Remember our quest is always to prove the null hypothesis is wrong . In this case, our samples are within the permissible limits of going wrong and hence we reject the null hypothesis. Proof for the 2 nd statement Let's say the p-value is 10% and the level of significance is 5% . What does this mean? It means the probability of getting our samples wrong is 10% but the maximum risk that we can take is 5% . Correct? In this case, our samples are above the permissible limits of going wrong and hence we cannot reject the null hypothesis and have to accept the null hypothesis. Confidence level # When the null hypothesis is rejected with a level of significance of 5% , we may be questioned of how confident we are in rejecting the null hypothesis In other words, it is to say that what is our confidence level in rejecting the null hypothesis. The relationship between confidence level and the level of significance is given by the relation confidence \\space level = (1 - \\alpha) \\\\ confidence \\space level = (1 - \\alpha) \\\\ which in this case is confidence \\space level = (1 - 0.05) \\space = 0.95 confidence \\space level = (1 - 0.05) \\space = 0.95 which means, we need to have atleast 95% confidence level to reject the null hypothesis. Summary # To summarize, we have seen what is hypothesis, what is a null hypothesis, what is an alternate hypothesis and when hypothesis testing can go wrong. In the class, we have not seen the level of significance in detail and what important role it has in hypothesis testing. The last part of the article was included to make this grey area more clearer. Next post Hypothesis Test Process","title":"Hypothesis Testing"},{"location":"statistics/hypothesis_testing_01/#hypothesis_testing","text":"","title":"Hypothesis Testing"},{"location":"statistics/hypothesis_testing_01/#what_is_hypothesis_testing","text":"First let us understand what does the word hypothesis mean. Let us break the word into two parts hypo + thesis . What does thesis mean? thesis means something that has already been proven to be true. e.g Atleast 60% of the adult human body is made up of water. Sounds fine! So what does hypothesis mean? Hypothesis is something that is not yet been proven to be true. Let's come back to the original question! What is hypothesis testing? Can we say that it is the testing of hypothesis or more precisely the process of determining whether a given hypothesis is true or not To sum up, we take the hypothesis, we perform some statistical computations and prove if the hypothesis holds true or not.","title":"What is Hypothesis Testing?"},{"location":"statistics/hypothesis_testing_01/#null_hypothesis_alternate_hypothesis","text":"Null hypothesis is the assertion or belief that we hold as true unless we have sufficient evidence to prove otherwise. In statistical terms, we say it as the belief we hold about the value of a population parameter Remember Parameter is for the population and Statistic is for the sample Let's take an example and see what is the null hypothesis and how it is written. We believe that the mean of the population is 500 . Unless we obtain sufficient evidence that it is not 500 , our belief holds true i.e we accept that the mean is 500 So we can write it as null hypothesis: mean = 500 To write it more compactly, we can represent the same thing as \\mathbf{H}_\\mathbf{0} : \\mu = 500 \\mathbf{H}_\\mathbf{0} : \\mu = 500 where the symbol \\space\\mathbf{H}_\\mathbf{0} \\space\\mathbf{H}_\\mathbf{0} denotes Null hypothesis. The opposite of Null hypothesis is called as Alternate Hypothesis . That is, the negation of the null hypothesis. If there is a way to represent the null hypothesis, then there should be a way to represent the alternate hypothesis. Agreed? Ah, I see I have quoted a null hypothesis there! \\mathbf{H}_\\mathbf{1}: \\mu \\ne 500 \\mathbf{H}_\\mathbf{1}: \\mu \\ne 500 where the symbol \\space\\mathbf{H}_\\mathbf{1} \\space\\mathbf{H}_\\mathbf{1} denotes alternate hypothesis Note Since the null hypothesis and the alternate hypothesis are exactly opposite statements, only one can be true. Rejecting one is accepting the other.","title":"Null Hypothesis &amp; Alternate Hypothesis"},{"location":"statistics/hypothesis_testing_01/#level_of_significance","text":"Life is not easy always, Isn't it? In the above two cases, taking the samples was easy and our agreement / rejection of the null hypothesis was indeed accurate. In real life scenarios, we need to estimate a population parameter based on the sample and things might go wrong. That is to say, we might reject the null hypothesis when it is actually true or we might accept the null hypothesis when it is otherwise. This depends on the sample we take and if we don't have enough samples (or worse picked up wrong samples), things might go wrong. Say for example, the null hypothesis state that the mean of the population is greater than or equal to 500 i.e. \\mathbf{H}_\\mathbf{0}: \\mu \\ge 500 \\mathbf{H}_\\mathbf{0}: \\mu \\ge 500 and the alternate hypothesis is mean less than 500 \\mathbf{H}_\\mathbf{1}: \\mu < 500 \\mathbf{H}_\\mathbf{1}: \\mu < 500 Let's say, we took a sample size of 30 i.e n=30 and found the mean is 499 . Ah! now comes the dilemma - whether we need to accept or reject the null hypothesis? The sample mean is just falling short by 1 from the population mean. We go into self-doubt if we have taken the correct sample size or what happens if the sample size is increased or worst case, should I have to repeat the experiment once again and my manager fires me for wasting the time and effort? So, what we are contemplating here is the probability of the evidence (samples picked) being unfavorable to the null hypothesis. This probablility is called as the p-value If we say the p_value or probability is 2% , it means that our sample has 2% chances of going wrong in rejecting the null hypothesis. If we say the p_value or probability is 30% , it means that our sample has 30% chances of going wrong in rejecting the null hypothesis and our chances of getting a promotion will be adversely impacted. But, we are humans and we need to have a leeway (a threshold) for making some mistakes / error with the samples. This threshold is called as the level of significance In other words, level of significance is the maximum level of risk (maximum acceptable probability - in statistical terms) that we may take in rejecting the null hypothesis while the null hypothesis is actually true. Level of significance is denoted by the letter \\alpha \\alpha (alpha) alpha is normally represented as 1% or 5% or 10% etc.","title":"Level of significance"},{"location":"statistics/hypothesis_testing_01/#confidence_level","text":"When the null hypothesis is rejected with a level of significance of 5% , we may be questioned of how confident we are in rejecting the null hypothesis In other words, it is to say that what is our confidence level in rejecting the null hypothesis. The relationship between confidence level and the level of significance is given by the relation confidence \\space level = (1 - \\alpha) \\\\ confidence \\space level = (1 - \\alpha) \\\\ which in this case is confidence \\space level = (1 - 0.05) \\space = 0.95 confidence \\space level = (1 - 0.05) \\space = 0.95 which means, we need to have atleast 95% confidence level to reject the null hypothesis.","title":"Confidence level"},{"location":"statistics/hypothesis_testing_01/#summary","text":"To summarize, we have seen what is hypothesis, what is a null hypothesis, what is an alternate hypothesis and when hypothesis testing can go wrong. In the class, we have not seen the level of significance in detail and what important role it has in hypothesis testing. The last part of the article was included to make this grey area more clearer.","title":"Summary"},{"location":"statistics/hypothesis_testing_02/","text":"Hypothesis Tests # In the previous article , we got introduced to the concept of hypothesis - null hypothesis and the alternate hypothesis. We concluded with how the p-value is compared with the level of significance to either accept or reject the null hypothesis. We will keep p-value aside at the moment and will later see how it is calculated; less we know - less we are confused. Hypothesis test steps # Just to recap, hypothesis testing is the process of determining whether a given hypothesis is true or not The highlight is on the word process . If it is called a process, then there have to be some steps associated with it. The steps for testing the hypothesis are mentioned below State the null hypothesis \\mathbf{H}_\\mathbf{0} \\mathbf{H}_\\mathbf{0} and the alternate hypothesis \\mathbf{H}_\\mathbf{1} \\mathbf{H}_\\mathbf{1} Choose the level of significance Find critical values Find test statistic Draw your conclusion Ok. Sounds good! Let us take stock of what knowledge we have already about the above-mentioned steps. We know what the null and the alternate hypothesis are, level of significance, and probably presume what draw your conclusion means. What we don't know about at this point is Step 4 - Find critical values and Step 5 - Find the test statistic. Good! Let's see what a critical value is! To understand the critical value and critical region, we need to first understand what one-tailed and two-tailed tests are. One-Tailed and Two-Tailed Tests # We have already seen tails in statistics - when there is a long tail to the right of a frequency distribution, the data is positively skewed and where is a long tail to the left, the data is negatively skewed. So it has to do something with the distribution curve? Yes, you are right! Let's dive into detail! What are tails? # In the day-to-day context, we know a tail is that extra portion that is attached to the body of an animal. Similarly, in the context of statistics, tails are that portion that is attached to the side of distribution - see figure below - the grey area Note Distribution does not have to have two tails always. There can be only one tail as well. Now that we have seen the tails visually, we understand things better. But, there are two tails here - do we have names that distinguish between these tails? Yes! It's upper tail and lower tail. The following image shows the lower tail and upper tail respectively. Upper tail The upper tail is towards the upper side i.e. the positive side of the graph (see image above). Remember positive values lie on the right of the graph (1, 2, 3, etc). So it is called as \"right-tail\" also. Lower tail The lower tail is towards the lower side i.e the negative side of the graph. Hence it is referred to as \"left-tail\" as well. So if the distribution has just one tail, it can be either an upper-tail or lower-tail and if the distribution has two-tails, it will have both upper-tail and lower-tail . Critical Region / Rejection Region The region that is shaded in grey i.e. the tail area is called as the Critical Region or Rejection Region The following image shows the rejection region for the lower-tailed graph Similarly, the image below depicts the rejection region for the upper tailed graph The rejection region for the two-tailed graph is shown below So far, so good!? Now in real-life problems, we will not be given the graph to understand if the tail is towards the left or right or it is two-tailed. The problem statement will be provided. We need to figure out the hypothesis and then decide if its right-tailed or left-tailed or both! One-Tailed tests # One-tailed tests can be either an upper tailed test or a lower tailed test. How can we spot or understand which tailed-tests the problem we have at hand belong to? Upper tail or lower tail? Remember in the previous article , we had stated the hypothesis for two examples. Let's revisit those examples. Example 1 My broadband company claims that the minimum internet speed they are providing is 150 Mbps. However, I suspect that they are not providing the promised internet speed. Let's write the null and alternate hypotheses for this! \\mathbf{H}_\\mathbf{0}: speed \\ge \\space 150 Mbps \\\\ \\mathbf{H}_\\mathbf{1}: speed < \\space 150 Mbps \\mathbf{H}_\\mathbf{0}: speed \\ge \\space 150 Mbps \\\\ \\mathbf{H}_\\mathbf{1}: speed < \\space 150 Mbps What we are trying to prove is our alternate hypothesis with evidence is that the speed is less than 150 Mbps. So we see the symbol of alternate hypothesis i.e - if it is less than < 150 Mbps, it is called a lower tail test. Example 2 The project manager claims that the software defects raised are resolved within 5 business days. I being the quality department manager think it is taking longer to fix the bugs. The null and alternate hypothesis is mentioned below \\mathbf{H}_\\mathbf{0}: Resolution\\space period \\le 5 \\space days \\\\ \\mathbf{H}_\\mathbf{1}: Resolution\\space period > 5 \\space days \\mathbf{H}_\\mathbf{0}: Resolution\\space period \\le 5 \\space days \\\\ \\mathbf{H}_\\mathbf{1}: Resolution\\space period > 5 \\space days We see that the alternate hypothesis is greater than > 5 days. Hence, it is an upper-tail test. Important The upper-tailed test or lower-tailed test is determined based on the alternative hypothesis only and NOT the null hypothesis. To conclude, one-tailed tests can be either an upper-tail test or a lower-tail test and the test is determined by the symbol of the alternate hypothesis \\mathbf{H}_\\mathbf{1} \\mathbf{H}_\\mathbf{1} Two-Tailed tests # So how does the hypothesis of a two-tailed test look like? A residential school claims that the average time the students of the school get to do extra-curricular activities is 200 hours per year but, the parents doubt that the students spend so much time. The null and the alternate hypothesis looks like the below \\mathbf{H}_\\mathbf{0}: \\mu = 200 \\space hours \\\\ \\mathbf{H}_\\mathbf{1}: \\mu \\ne 200 \\space hours \\mathbf{H}_\\mathbf{0}: \\mu = 200 \\space hours \\\\ \\mathbf{H}_\\mathbf{1}: \\mu \\ne 200 \\space hours Here we see the symbol of the alternate hypothesis is \\ne \\ne which means the test is a two-tailed test. We don't check for increase or decrease i.e. \\ge \\ge or \\le \\le but, we check for a change in the parameter. So the critical region / tails are split over both the ends. Both the ends contain \\alpha/2 \\alpha/2 , making a total of \\alpha \\alpha - which is the level of significance. Refer to the previous article for level of significance. Milestone reached: So from the hypothesis statement, we are understanding if the test is either a one-tailed (upper or lower tail) or two-tailed. We are still in step 3 - Finding the critical value Let's understand what a critical value is and then we will see how to compute the critical value. Critical value # We know what a critical region or a rejection region is! So critical value should be near or in that critical region. To take an analogy, the critical values of water - i.e. the boiling point is 100 deg Celcius and the freezing point is 0 deg Celcius. It is an important measure that helps us make important decisions. Similarly, critical value in statistics helps us to take important decisions. Definition of critical value A critical value is a line on a graph that splits the graph into sections. If our test value falls into that region, then you reject the null hypothesis - which means the samples and the evidence we had taken supports the alternate hypothesis Critical value depicted in the graph below - for lower-tailed graph Similarly, the critical value for the upper-tailed graph is below Critical values for a two-tailed graph is below In the case of a two-tailed graph, the value that we are computing should be either below or above the critical value for rejecting the null hypothesis. Ok - now we have visualized what a critical value is. This critical value is nothing but the 'Z' score We already know some of the common 'Z' scores. Example of a 'Z' score Let's say we have a sample normal distribution. We know that as per the empirical split (or rule), 68% lie within the 1 standard deviation from the mean, 95% lie within 2 standard deviation from the mean and 99.7% of the values lie within 3 standard deviations from the mean. We know that 'Z' score for +2.0 standard deviation is 1.96 and for -2.0 standard deviation is -1.96 . We have used this in several computations in the class. We will see how to compute 'Z' scores in another post. Let us see how the level of significance ( \\alpha \\alpha ) is used to find the critical value (Z score). Level of significance is covered in this post Critical value and level of significance # We use the level of significance ( \\alpha \\alpha ) to determine the critical value. How? Example: Determine the critical value at 5% level of significance. Presume its a right-tailed / upper-tailed test . We know \\alpha \\alpha = 5% (see image below to find where level of significance \\alpha \\alpha of 5% falls) and 1 - \\alpha \\alpha is 95% We know that 95% of the values fall within the 2 standard deviation mark and the corresponding the 'Z' score is 1.96 . So the critical value ( Z ) is 1.96 . So how will we determine if we have to accept or reject the null hypothesis? I am quoting a new word test statistic here. We will see what it is and how it is computed in the next post. At the moment, just think of test statistic as a number computed from our samples. If the test statistic ( test Z ) is less than the critical value ( Z ), we will accept the null hypothesis. In other words, we have not stepped into the rejection region and hence will accept. test\\space Z \\le Z : \\space accept \\space \\mathbf{H}_\\mathbf{0} \\\\ test\\space Z > Z : \\space reject \\space \\mathbf{H}_\\mathbf{0} test\\space Z \\le Z : \\space accept \\space \\mathbf{H}_\\mathbf{0} \\\\ test\\space Z > Z : \\space reject \\space \\mathbf{H}_\\mathbf{0} Let's visualize the above scenario with an example graph! The below are the numbers Critical value Z = 1.96 Test statistic ( test Z ) = 0.5 Since the test statistic is less than the critical value, we will accept the null hypothesis \\mathbf{H}_\\mathbf{0} \\mathbf{H}_\\mathbf{0} Summary # In this article, we focussed on the 3 rd step of the hypothesis testing process - Find the critical value We started with what are tails, upper tail and lower tail, one-tailed tests and two-tailed tests, critical region / rejection region, critical values and when to accept or reject the null hypothesis. In the next post, we will explore more on finding the test statistic which is the 4 th step of the hypothesis testing process. Next post Test statistic - Z and t","title":"Hypothesis Test Process"},{"location":"statistics/hypothesis_testing_02/#hypothesis_tests","text":"In the previous article , we got introduced to the concept of hypothesis - null hypothesis and the alternate hypothesis. We concluded with how the p-value is compared with the level of significance to either accept or reject the null hypothesis. We will keep p-value aside at the moment and will later see how it is calculated; less we know - less we are confused.","title":"Hypothesis Tests"},{"location":"statistics/hypothesis_testing_02/#hypothesis_test_steps","text":"Just to recap, hypothesis testing is the process of determining whether a given hypothesis is true or not The highlight is on the word process . If it is called a process, then there have to be some steps associated with it. The steps for testing the hypothesis are mentioned below State the null hypothesis \\mathbf{H}_\\mathbf{0} \\mathbf{H}_\\mathbf{0} and the alternate hypothesis \\mathbf{H}_\\mathbf{1} \\mathbf{H}_\\mathbf{1} Choose the level of significance Find critical values Find test statistic Draw your conclusion Ok. Sounds good! Let us take stock of what knowledge we have already about the above-mentioned steps. We know what the null and the alternate hypothesis are, level of significance, and probably presume what draw your conclusion means. What we don't know about at this point is Step 4 - Find critical values and Step 5 - Find the test statistic. Good! Let's see what a critical value is! To understand the critical value and critical region, we need to first understand what one-tailed and two-tailed tests are.","title":"Hypothesis test steps"},{"location":"statistics/hypothesis_testing_02/#one-tailed_and_two-tailed_tests","text":"We have already seen tails in statistics - when there is a long tail to the right of a frequency distribution, the data is positively skewed and where is a long tail to the left, the data is negatively skewed. So it has to do something with the distribution curve? Yes, you are right! Let's dive into detail!","title":"One-Tailed and Two-Tailed Tests"},{"location":"statistics/hypothesis_testing_02/#what_are_tails","text":"In the day-to-day context, we know a tail is that extra portion that is attached to the body of an animal. Similarly, in the context of statistics, tails are that portion that is attached to the side of distribution - see figure below - the grey area Note Distribution does not have to have two tails always. There can be only one tail as well. Now that we have seen the tails visually, we understand things better. But, there are two tails here - do we have names that distinguish between these tails? Yes! It's upper tail and lower tail. The following image shows the lower tail and upper tail respectively.","title":"What are tails?"},{"location":"statistics/hypothesis_testing_02/#one-tailed_tests","text":"One-tailed tests can be either an upper tailed test or a lower tailed test. How can we spot or understand which tailed-tests the problem we have at hand belong to? Upper tail or lower tail? Remember in the previous article , we had stated the hypothesis for two examples. Let's revisit those examples. Example 1 My broadband company claims that the minimum internet speed they are providing is 150 Mbps. However, I suspect that they are not providing the promised internet speed. Let's write the null and alternate hypotheses for this! \\mathbf{H}_\\mathbf{0}: speed \\ge \\space 150 Mbps \\\\ \\mathbf{H}_\\mathbf{1}: speed < \\space 150 Mbps \\mathbf{H}_\\mathbf{0}: speed \\ge \\space 150 Mbps \\\\ \\mathbf{H}_\\mathbf{1}: speed < \\space 150 Mbps What we are trying to prove is our alternate hypothesis with evidence is that the speed is less than 150 Mbps. So we see the symbol of alternate hypothesis i.e - if it is less than < 150 Mbps, it is called a lower tail test. Example 2 The project manager claims that the software defects raised are resolved within 5 business days. I being the quality department manager think it is taking longer to fix the bugs. The null and alternate hypothesis is mentioned below \\mathbf{H}_\\mathbf{0}: Resolution\\space period \\le 5 \\space days \\\\ \\mathbf{H}_\\mathbf{1}: Resolution\\space period > 5 \\space days \\mathbf{H}_\\mathbf{0}: Resolution\\space period \\le 5 \\space days \\\\ \\mathbf{H}_\\mathbf{1}: Resolution\\space period > 5 \\space days We see that the alternate hypothesis is greater than > 5 days. Hence, it is an upper-tail test. Important The upper-tailed test or lower-tailed test is determined based on the alternative hypothesis only and NOT the null hypothesis. To conclude, one-tailed tests can be either an upper-tail test or a lower-tail test and the test is determined by the symbol of the alternate hypothesis \\mathbf{H}_\\mathbf{1} \\mathbf{H}_\\mathbf{1}","title":"One-Tailed tests"},{"location":"statistics/hypothesis_testing_02/#two-tailed_tests","text":"","title":"Two-Tailed tests"},{"location":"statistics/hypothesis_testing_02/#critical_value","text":"We know what a critical region or a rejection region is! So critical value should be near or in that critical region. To take an analogy, the critical values of water - i.e. the boiling point is 100 deg Celcius and the freezing point is 0 deg Celcius. It is an important measure that helps us make important decisions. Similarly, critical value in statistics helps us to take important decisions.","title":"Critical value"},{"location":"statistics/hypothesis_testing_02/#critical_value_and_level_of_significance","text":"We use the level of significance ( \\alpha \\alpha ) to determine the critical value. How? Example: Determine the critical value at 5% level of significance. Presume its a right-tailed / upper-tailed test . We know \\alpha \\alpha = 5% (see image below to find where level of significance \\alpha \\alpha of 5% falls) and 1 - \\alpha \\alpha is 95% We know that 95% of the values fall within the 2 standard deviation mark and the corresponding the 'Z' score is 1.96 . So the critical value ( Z ) is 1.96 .","title":"Critical value and level of significance"},{"location":"statistics/hypothesis_testing_02/#summary","text":"In this article, we focussed on the 3 rd step of the hypothesis testing process - Find the critical value We started with what are tails, upper tail and lower tail, one-tailed tests and two-tailed tests, critical region / rejection region, critical values and when to accept or reject the null hypothesis. In the next post, we will explore more on finding the test statistic which is the 4 th step of the hypothesis testing process.","title":"Summary"},{"location":"statistics/hypothesis_testing_03/","text":"Test statistic - Z & t # In the previous article , we used test statistic as a value to compare against the critical value. This helps us to accept or reject our null hypothesis. In this article, we will see what test statistic is and how to compute it. Note We are in the fourth step of the hypothesis testing process Find the test statistic What is a test statistic? # A test statistic is used in a hypothesis test to decide to support or reject a null hypothesis. A test statistic is a number that is calculated from a sample and is compared with the null hypothesis. So what does this mean? We know that for the population we have parameters and for sample, we have statistics. i.e. any value that represents the population is called as parameter and any value that represents sample is called as a statistic We use this sample and come up with a value (statistic). This is called the test statistic. Methods of finding the test statistic # There are various methods/tests for finding the test statistic. Usually, the following four methods are used. Z test t test Chi-squared test ( \\chi^2 \\chi^2 ) F test In this article, we will cover the Z test and t test. We have these tests - how are they different? When the null hypothesis is about the mean of the population, the Z test or the t test is used. When the null hypothesis is about the variance of the population, the \\chi^2 \\chi^2 (chi-square) test and the F test is used. Let us see the Z test first. Z test # Assumptions of a Z test # Z test is used when the following conditions are met. The sample size n should be greater than 30 i.e. n > 30 n > 30 The population standard deviation \\sigma \\sigma should be known The variable should be continuous (remember, this is a continuous sampling distribution) Formula for Z test # The test statistic for Z test is represented as test Z The formula for finding the test statistic in a Z test is \\textrm {test}\\space Z = \\frac{\\bar{x} - \\mu} {\\sigma / \\sqrt n} \\textrm {test}\\space Z = \\frac{\\bar{x} - \\mu} {\\sigma / \\sqrt n} where \\bar{x} \\bar{x} is the sample mean, \\mu \\mu is the population mean, \\sigma \\sigma is the population standard deviation and n n is the number of samples. Example # Let's say the null and the alternate hypothesis are \\mathbf{H}_\\mathbf{0} : \\mu \\ge 1000 \\\\ \\mathbf{H}_\\mathbf{1} : \\mu < 1000 \\mathbf{H}_\\mathbf{0} : \\mu \\ge 1000 \\\\ \\mathbf{H}_\\mathbf{1} : \\mu < 1000 and the population standard deviation \\sigma \\sigma is known and a random sample of size n n = 30 is taken, then the test statistic of Z Z would be test\\space Z = \\frac{\\bar{x} - 1000}{\\sigma / \\sqrt 30} test\\space Z = \\frac{\\bar{x} - 1000}{\\sigma / \\sqrt 30} Let's check our understanding with a detailed example Let's highlight the keywords along with the problem statement An automatic bottling machine fills cola into 2-liter ( 2,000 - cubic cm) bottles. A consumer advocate wants to test the null hypothesis that the average amount filled by the machine into a bottle is at least 2,000 cubic cm . A random sample of 40 bottles coming out of the machine was selected and the exact contents of the selected bottles are recorded. The sample mean was 1,999.6 cubic cm . The population standard deviation is known from past experience to be 1.30 cubic cm. Test the null hypothesis at an \\alpha \\alpha of 5%. Let's write the values that have been provided \\mu = 2000; \\space n = 40; \\space \\bar{x} = 1999.6;\\space\\sigma=1.30; \\space \\alpha = 5% \\mu = 2000; \\space n = 40; \\space \\bar{x} = 1999.6;\\space\\sigma=1.30; \\space \\alpha = 5% Let's try to follow the hypothesis test process State the null and the alternate hypothesis Choose the level of significance Find the critical value Find the test statistic Draw the conclusion 1. Stating the null and the alternate hypothesis \\mathbf{H}_\\mathbf{0}: \\mu \\ge 2000 \\\\ \\mathbf{H}_\\mathbf{1}: \\mu < 2000 \\mathbf{H}_\\mathbf{0}: \\mu \\ge 2000 \\\\ \\mathbf{H}_\\mathbf{1}: \\mu < 2000 We see the sign of the alternate hypothesis and it is < (less than symbol) which means the test is a one-tailed lower-tail test. 2. Choose the level of significance The level of significance provided is \\alpha = 5 \\alpha = 5 %. So the confidence level is , 1 - \\alpha\\space = 95 \\alpha\\space = 95 % 3. Find the critical value The corresponding Z score (critical value) is z_c = -1.64 z_c = -1.64 Here the - (negative) sign implies that it is a lower tail test. Note The Z score for 95% is 1.64 but, since this is a lower-tailed test, we take the Z score as -1.64 The rejection region is highlighted in the graph below 4. Find the test statistic Here the sample is n = 40 n = 40 which means it confirms to the first condition of the Z Z test mentioned above Second, the population standard deviation is known - which means we can use the Z Z test to find the test statistic The formula for computing the Z Z score is \\textrm {test}\\space Z = \\frac{\\bar{x} - \\mu} {\\sigma / \\sqrt n} \\textrm {test}\\space Z = \\frac{\\bar{x} - \\mu} {\\sigma / \\sqrt n} When we substitute the values, we get \\textrm {test}\\space Z = \\frac{1999.6 - 2000} {1.30 / \\sqrt 40} \\textrm {test}\\space Z = \\frac{1999.6 - 2000} {1.30 / \\sqrt 40} which yeilds the result \\textrm {test} \\space z \\textrm {test} \\space z = -1.95 When we plot it in the graph, the graph would look like the below image The test statistic \\textrm{test} \\space z \\textrm{test} \\space z falls within the rejection range. Hence the null hypothesis will be rejected. 5. Drawing the conclusion The hypothesis that the average amount filled by the cola machine into a cola bottle is less than 2,000 cubic cm and hence, it is rejected. To sum up, the Z test is used to calculate the test statistic when the null hypothesis is about the means of the population and it satisfies the assumption mentioned above. 't' test or Student 't' test # We saw that the 'Z' test is applicable when the sample size is greater than 30 i.e. n>30 . There arises a logical question as to what test is to be done when the sample size is less than 30 i.e. n<30 . We use the 't' test for computing the test statistic for null hypothesis testing. Assumptions of a 't' test # t t test is used when the following conditions are met. The sample size n should be less than 30 i.e. n < 30 n < 30 The population standard deviation is not known irrespective of the sample size The population is normally distributed Formula for 't' test # t \\space statistic \\space (or \\space t\\space score), t= \\frac{\\bar{x} - \\mu}{s /\\sqrt n} t \\space statistic \\space (or \\space t\\space score), t= \\frac{\\bar{x} - \\mu}{s /\\sqrt n} where \\bar{x} \\bar{x} is the sample mean, \\mu \\mu is the population mean, s s is the sample standard deviation and n n is the number of samples. Since we don't know the population standard deviation in a 't' test, we use the sample standard deviation Characteristics of the 't' distribution It has degrees-of-freedom parameter df df It is symmetric and bell-shaped Has wider tails than the Z distribution Extra_research Why does a t t distribution has wider tails than the Z distribution? In a t t distribution, the population standard deviation ( \\sigma \\sigma ) is not known and only the sample standard deviation ( s s ) is known. We know that by using the sample standard deviation, we cannot accurately project the population variance. Hence, to accommodate the level of uncertainity in computing the population variance, it has wider tails. As the degree of freedom ( df df ) increases, the t-distribution approaches the 'Z' distribution The 't' distribution with infinite degrees-of-freedom is called as the standard normal distribution. Confidence interval to estimate the population mean # As the population standard deviation, \\sigma \\sigma is not known in a t t , we will not able to estimate the population mean \\mu \\mu - but, we can find the range (confidence interval) of the population mean \\mu \\mu The formula for computing the confidence interval to estimate \\mu \\mu is \\bar{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\le \\mu \\le \\bar{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\bar{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\le \\mu \\le \\bar{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} where \\bar{x} \\bar{x} is the sample mean, t t is the test statistic of the t t distribution, n-1 n-1 is the degree of freedom, \\alpha \\alpha is the level of significance, S S is the sample standard deviation, n n is the number of samples. Why \\frac{\\alpha}{2} \\frac{\\alpha}{2} ? In a confidence interval, the area is symmetrically distributed between the two tails. Example # A stock market analyst wants to estimate the average return on a certain stock. A random sample of 15 days yields an average (annualized) return of \\bar{x} \\bar{x} 10.37% and a standard deviation of s = = 3.5%. Assuming a normal population of returns, give a 95% confidence interval for the average return on this stock. What we know here? Sample of 15 days i.e. s = 15 s = 15 (since sample is less than 30, t test is to be used). We need to use t distribution with n-1 n-1 degrees of freedom. So, df = 14 df = 14 . Sample standard deviation s = 3.5 s = 3.5 Sample mean \\bar{x} = 10.37 \\bar{x} = 10.37 Confidence level is 95%. So the level of significance deduced is 5%. Remember the formula confidence level = 1 - \\alpha \\alpha which means \\alpha \\alpha = 100 - confidence level Solution We know the formula for the confidence interval of the t distribution is \\bar{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\le \\mu \\le \\bar{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\bar{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\le \\mu \\le \\bar{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} If we substitute the values, we get 10.37 - t_{14,0.025} \\frac{3.5}{\\sqrt 15} \\le \\mu \\le 10.37 + t_{14,0.025} \\frac{3.5}{\\sqrt 15} 10.37 - t_{14,0.025} \\frac{3.5}{\\sqrt 15} \\le \\mu \\le 10.37 + t_{14,0.025} \\frac{3.5}{\\sqrt 15} From the t tables, we can find the t statistic value for df = 14 df = 14 and \\alpha /2 = 0.025 \\alpha /2 = 0.025 is t_{14,0.025} = 2.145 t_{14,0.025} = 2.145 See below image for finding the t value for df = 14 df = 14 and \\alpha = 0.025 \\alpha = 0.025 Substituting in the formula, we get 10.37 - 2.145 \\frac{3.5}{\\sqrt 15} \\le \\mu \\le 10.37 + 2.145 \\frac{3.5}{\\sqrt 15} 10.37 - 2.145 \\frac{3.5}{\\sqrt 15} \\le \\mu \\le 10.37 + 2.145 \\frac{3.5}{\\sqrt 15} the confidence interval range is [8.43, 12.31] [8.43, 12.31] . This is represented as CI(0.05) = [8.43, 12.31] CI(0.05) = [8.43, 12.31] Thus, the analyst may be 95% sure that the average annualized return on the stock is anywhere from 8.43% to 12.31%. Degree of Freedom # Degrees of freedom refers to the maximum number of logically independent values in a data sample which have the freedom to vary within. What does that mean? We will take an example and try to understand. Example Let us consider that we have a sample of 3 values {5, x , 15} {5, x , 15} and the mean of all the values is 10. Note x x is unknown here It is easy to deduce that the value of x x would be 10 as the mean of these 3 values has to equate to 10. But, let's say 2 values from this sample are not known, {5, x, y} {5, x, y} Note ( x, y x, y are unknown) with the same mean 10, then we cannot be sure about the exact values of x x and y y . It could be any values of these values - (10, 15), (15, 10), (5, 20), (20, 5) or even (1, 24). So we cannot determine the exact value of these variables x x and y y . These 2 values have the freedom to vary. So, the degree of freedom ( df df ) of this sample data of size 3 is 2. Formula df = n \u2013 1 df = n \u2013 1 where df df is the degree of freedom and n n is the sample size Application of t-test # Your company wants to improve sales. Past sales data indicate that the average sale was $100 per transaction. After training your sales force, recent sales data (taken from a sample of 25 salesmen) indicates an average sale of $130, with a standard deviation of $15. Did the training work? Test your hypothesis at a 5% alpha level. What is given? From the problem, first, we need to understand if the given problem belongs to which category of tests - Z or t. The average sale is provided \\mu = 100 \\mu = 100 . Sample is n = 25 n = 25 . Recent sales data (sample) is given - ie. average sale \\bar{x} = 130 \\bar{x} = 130 . Sample standard deviation s = 15 s = 15 . Level of significance = 5% or 0.05 Since the sample size is less than 30 and we don't know the population standard deviation, we will go with the t test Solution Approach Did the training work? In other words, is there an increase in sales after the training is provided. Hypothesis testing steps We know the hypothesis testing steps State the null and the alternate hypothesis \\mathbf{H}_\\mathbf{0}: \\mu = 100 \\\\ \\mathbf{H}_\\mathbf{1}: \\mu > 100 \\mathbf{H}_\\mathbf{0}: \\mu = 100 \\\\ \\mathbf{H}_\\mathbf{1}: \\mu > 100 Since the symbol of the alternate hypothesis is greater than, it is a right-tailed test. Find the level of significance. Here the level of significance is provided - which is \\alpha = 0.05 \\alpha = 0.05 Find the critical value Since this is a t test, we need to use the t-table to find the critical value. The t-table has the degree of freedom and corresponding level of significance to provide the critical value. Here the degree of freedom df = n - 1 df = n - 1 which is 25 - 1 = 24 25 - 1 = 24 Looking into the critical table for df = 24 df = 24 and \\alpha = 0.05 \\alpha = 0.05 , we get the critical value t_c = 1.711 t_c = 1.711 (see figure below) Find the test statistic We know the formula for the t test is t_{score} = \\frac{\\bar{x} - \\mu}{s / \\sqrt n } t_{score} = \\frac{\\bar{x} - \\mu}{s / \\sqrt n } Substituting the above values, we get t_{score} = \\frac{130 - 100}{15 / \\sqrt 25} t_{score} = \\frac{130 - 100}{15 / \\sqrt 25} which gives us the t_{score} = 10 t_{score} = 10 Drawing the conclusions We see that the t_{score} t_{score} falls beyond the rejection region and hence the null hypothesis is rejected. This means to say that indeed the average sales increased after the training. Summary # In this article, we saw what a test statistic is!, the different types of tests, when to use Z test and t test and their respective formulas. In the end, we saw what is the degree of freedom along with an application of t-test using an example. We also got more familiar with the hypothesis testing process and following it would help us accept or reject the null hypothesis in a logical manner. Next post Test statistic - chi-square and F","title":"Test statistic - Z & t"},{"location":"statistics/hypothesis_testing_03/#test_statistic_-_z_t","text":"In the previous article , we used test statistic as a value to compare against the critical value. This helps us to accept or reject our null hypothesis. In this article, we will see what test statistic is and how to compute it. Note We are in the fourth step of the hypothesis testing process Find the test statistic","title":"Test statistic - Z &amp; t"},{"location":"statistics/hypothesis_testing_03/#what_is_a_test_statistic","text":"A test statistic is used in a hypothesis test to decide to support or reject a null hypothesis. A test statistic is a number that is calculated from a sample and is compared with the null hypothesis. So what does this mean? We know that for the population we have parameters and for sample, we have statistics. i.e. any value that represents the population is called as parameter and any value that represents sample is called as a statistic We use this sample and come up with a value (statistic). This is called the test statistic.","title":"What is a test statistic?"},{"location":"statistics/hypothesis_testing_03/#methods_of_finding_the_test_statistic","text":"There are various methods/tests for finding the test statistic. Usually, the following four methods are used. Z test t test Chi-squared test ( \\chi^2 \\chi^2 ) F test In this article, we will cover the Z test and t test. We have these tests - how are they different? When the null hypothesis is about the mean of the population, the Z test or the t test is used. When the null hypothesis is about the variance of the population, the \\chi^2 \\chi^2 (chi-square) test and the F test is used. Let us see the Z test first.","title":"Methods of finding the test statistic"},{"location":"statistics/hypothesis_testing_03/#z_test","text":"","title":"Z test"},{"location":"statistics/hypothesis_testing_03/#assumptions_of_a_z_test","text":"Z test is used when the following conditions are met. The sample size n should be greater than 30 i.e. n > 30 n > 30 The population standard deviation \\sigma \\sigma should be known The variable should be continuous (remember, this is a continuous sampling distribution)","title":"Assumptions of a Z test"},{"location":"statistics/hypothesis_testing_03/#formula_for_z_test","text":"The test statistic for Z test is represented as test Z The formula for finding the test statistic in a Z test is \\textrm {test}\\space Z = \\frac{\\bar{x} - \\mu} {\\sigma / \\sqrt n} \\textrm {test}\\space Z = \\frac{\\bar{x} - \\mu} {\\sigma / \\sqrt n} where \\bar{x} \\bar{x} is the sample mean, \\mu \\mu is the population mean, \\sigma \\sigma is the population standard deviation and n n is the number of samples.","title":"Formula for Z test"},{"location":"statistics/hypothesis_testing_03/#example","text":"Let's say the null and the alternate hypothesis are \\mathbf{H}_\\mathbf{0} : \\mu \\ge 1000 \\\\ \\mathbf{H}_\\mathbf{1} : \\mu < 1000 \\mathbf{H}_\\mathbf{0} : \\mu \\ge 1000 \\\\ \\mathbf{H}_\\mathbf{1} : \\mu < 1000 and the population standard deviation \\sigma \\sigma is known and a random sample of size n n = 30 is taken, then the test statistic of Z Z would be test\\space Z = \\frac{\\bar{x} - 1000}{\\sigma / \\sqrt 30} test\\space Z = \\frac{\\bar{x} - 1000}{\\sigma / \\sqrt 30}","title":"Example"},{"location":"statistics/hypothesis_testing_03/#t_test_or_student_t_test","text":"We saw that the 'Z' test is applicable when the sample size is greater than 30 i.e. n>30 . There arises a logical question as to what test is to be done when the sample size is less than 30 i.e. n<30 . We use the 't' test for computing the test statistic for null hypothesis testing.","title":"'t' test or Student 't' test"},{"location":"statistics/hypothesis_testing_03/#assumptions_of_a_t_test","text":"t t test is used when the following conditions are met. The sample size n should be less than 30 i.e. n < 30 n < 30 The population standard deviation is not known irrespective of the sample size The population is normally distributed","title":"Assumptions of a 't' test"},{"location":"statistics/hypothesis_testing_03/#formula_for_t_test","text":"t \\space statistic \\space (or \\space t\\space score), t= \\frac{\\bar{x} - \\mu}{s /\\sqrt n} t \\space statistic \\space (or \\space t\\space score), t= \\frac{\\bar{x} - \\mu}{s /\\sqrt n} where \\bar{x} \\bar{x} is the sample mean, \\mu \\mu is the population mean, s s is the sample standard deviation and n n is the number of samples. Since we don't know the population standard deviation in a 't' test, we use the sample standard deviation","title":"Formula for 't' test"},{"location":"statistics/hypothesis_testing_03/#confidence_interval_to_estimate_the_population_mean","text":"As the population standard deviation, \\sigma \\sigma is not known in a t t , we will not able to estimate the population mean \\mu \\mu - but, we can find the range (confidence interval) of the population mean \\mu \\mu The formula for computing the confidence interval to estimate \\mu \\mu is \\bar{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\le \\mu \\le \\bar{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\bar{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} \\le \\mu \\le \\bar{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{s}{\\sqrt n} where \\bar{x} \\bar{x} is the sample mean, t t is the test statistic of the t t distribution, n-1 n-1 is the degree of freedom, \\alpha \\alpha is the level of significance, S S is the sample standard deviation, n n is the number of samples.","title":"Confidence interval to estimate the population mean"},{"location":"statistics/hypothesis_testing_03/#example_1","text":"A stock market analyst wants to estimate the average return on a certain stock. A random sample of 15 days yields an average (annualized) return of \\bar{x} \\bar{x} 10.37% and a standard deviation of s = = 3.5%. Assuming a normal population of returns, give a 95% confidence interval for the average return on this stock.","title":"Example"},{"location":"statistics/hypothesis_testing_03/#degree_of_freedom","text":"Degrees of freedom refers to the maximum number of logically independent values in a data sample which have the freedom to vary within. What does that mean? We will take an example and try to understand.","title":"Degree of Freedom"},{"location":"statistics/hypothesis_testing_03/#application_of_t-test","text":"Your company wants to improve sales. Past sales data indicate that the average sale was $100 per transaction. After training your sales force, recent sales data (taken from a sample of 25 salesmen) indicates an average sale of $130, with a standard deviation of $15. Did the training work? Test your hypothesis at a 5% alpha level.","title":"Application of t-test"},{"location":"statistics/hypothesis_testing_03/#summary","text":"In this article, we saw what a test statistic is!, the different types of tests, when to use Z test and t test and their respective formulas. In the end, we saw what is the degree of freedom along with an application of t-test using an example. We also got more familiar with the hypothesis testing process and following it would help us accept or reject the null hypothesis in a logical manner.","title":"Summary"},{"location":"statistics/hypothesis_testing_04/","text":"Test statistic - \\chi^2 \\chi^2 & F # In the previous article , we saw what a Z test and t-test are! Z test and t-test are used when the hypothesis test is about the means of the population. In this article, let us see two tests - the chi-squared ( \\chi^2 \\chi^2 ) and F test which tests the hypothesis about the variance of the population. Recall The sample estimate of the population variance is given by $$ s^2 = \\frac{\\sum(x_i-\\bar{x})^2}{n-1} $$ where s s is the sample variance, \\bar{x} \\bar{x} is the sample mean, and n n is the number of samples. Chi-squared statistic # \\chi^2 \\chi^2 is used to test the hypothesis about a single population variance. Formula # The formula for computing \\chi ^2 \\chi ^2 is \\chi ^2 = \\frac{(n-1)s ^2}{\\sigma ^2} \\chi ^2 = \\frac{(n-1)s ^2}{\\sigma ^2} where n - 1 n - 1 is the df df (degree of freedom), s s is the sample standard deviation and \\sigma \\sigma is the population standard deviation and n n is the number of samples. Interpretation of \\chi ^2 \\chi ^2 If the chi-square value is more than the critical value, then there is a significant difference in the variance between the sample and the population. Tip The Chi-square statistic can only be used on numbers . They can\u2019t be used for percentages, proportions, means or similar statistical value. For example, if you have 20 percent of 100 people, you would need to convert that to a number (20) before you can run a test statistic. Application of Chi-square # A manufacturing company produces bearings of 2.65 cm in diameter. A major customer requires that the variance in diameter be no more than 0.001 cm ^2 cm ^2 . The manufacturer tests 20 bearings using a precise instrument and gets the below values. Assuming the diameters are normally distributed, can the population of these bearings be rejected due to high variance at 1% significance level. Data 2.69, 2.66, 2.64, 2.59, 2.62, 2.63, 2.69, 2.66, 2.63, 2.65, 2.57, 2.63, 2.70, 2.71, 2.64, 2.65, 2.59, 2.66, 2.62, 2.57 Solution approach # The problem talks about a single population variance. Hence, a \\chi ^2 \\chi ^2 test can be used. Known parameters sample \\space size \\space n = 20 sample \\space size \\space n = 20 level \\space of \\space significance \\space \\alpha = 1\\% \\space or \\space 0.01 level \\space of \\space significance \\space \\alpha = 1\\% \\space or \\space 0.01 Degree \\space of \\space freedom \\space = n - 1 \\space which \\space is \\space 20 - 1 \\space = 19 Degree \\space of \\space freedom \\space = n - 1 \\space which \\space is \\space 20 - 1 \\space = 19 population \\space variance = \\sigma ^2 = 0.001 population \\space variance = \\sigma ^2 = 0.001 Let's follow the hypothesis testing process State the null and the alternate hypothesis. The variance in diameter to be no more than 0.001 cm ^2 cm ^2 . So the null hypothesis is \\mathbf{H}_\\mathbf{0}: diameter \\le 0.001 \\mathbf{H}_\\mathbf{0}: diameter \\le 0.001 and hence the alternate hypothesis is \\mathbf{H}_\\mathbf{1} : diameter > 0.001 \\mathbf{H}_\\mathbf{1} : diameter > 0.001 Since the alternate hypothesis has the greater than symbol > > , it is a chi-square right-tailed test. Find the level of significance The level of significance is already provided \\alpha = 0.01 \\alpha = 0.01 Tip If the level of significance is not provided, take the default \\alpha = 0.05 \\alpha = 0.05 Find the critical value The critical value is found out in the chi-squared table for df = 19 df = 19 and \\alpha = 0.01 \\alpha = 0.01 . The critical value is 36.191. i.e \\chi ^2_c = 36.191 \\chi ^2_c = 36.191 Find the test statistic To find the test statistic, we will use the formula. \\chi ^2 = \\frac{(n-1)s ^2}{\\sigma ^2} \\chi ^2 = \\frac{(n-1)s ^2}{\\sigma ^2} We don't know the sample variance s ^2 s ^2 but, it can be computed by using the data provided. Copy the data in excel and use the formula VAR.S and get the value which in turn is 0.001621 0.001621 . Substituting the values, we get \\chi ^2 = \\frac{19 *0.001621}{0.001} \\chi ^2 = \\frac{19 *0.001621}{0.001} which gives \\chi ^2 = 30.8 \\chi ^2 = 30.8 Draw the conclusion - to accept/reject the null hypothesis Since the \\chi ^2 < \\chi^2_{critical} \\chi ^2 < \\chi^2_{critical} i.e. 30.8 < 36.191 30.8 < 36.191 , we accept the null hypothesis. The bearings produced are within the specified limits required by the customer. To conclude, a \\chi ^2 \\chi ^2 test (chi-squared) is used to test the hypothesis about a single population variance. F distribution # \\chi ^2 \\chi ^2 is useful when testing hypothesis about a single population. What if we want to test the hypothesis about the difference in variances of two populations? Example Do parts manufactured on 2 different machines have the same variance or not? Formula # Since F-test is a comparison of variances of two different populations using samples collected from each population, we can say that it is the ratio of two sample variances i.e. F = \\frac{s_1 ^2}{s_2 ^2} = \\frac{est.\\sigma ^2_1}{est.\\sigma ^2_2} F = \\frac{s_1 ^2}{s_2 ^2} = \\frac{est.\\sigma ^2_1}{est.\\sigma ^2_2} What does this formula mean? We know that s_1 s_1 is the standard deviation of sample 1 and s_2 s_2 is the standard deviation of sample 2. Since the F test is a comparison between two variances , we need to square the standard deviation. (Remember: variance = standard deviation ^2 ^2 ) Interpretation Ideally, this F ratio should be about 1 if the 2 samples come from the same population or the 2 samples come from a different population with the same variance So if we compute the F ratio and see if the value is near to 1, it means two samples have the same variance thereby the population variance is also the same. Bigger the F ratio - bigger the variance (or the two population is not related to each other) Important Since the F test/distribution deals with two samples, there will be two degrees of freedom - one for sample 1 and one for sample 2. Facts The curve is not symmetrical but skewed to the right. There is a different curve for each set of df . The F statistic is greater than or equal to zero. As the degrees of freedom for the numerator and the denominator get larger, the curve approximates the normal. Application of the 'F' test # A machine produces metal sheets with 22mm thickness. There is a variability in thickness due to machines, operators, manufacturing environment, raw material, etc. The company wants to know the consistency of two machines and randomly samples 10 sheets from machine 1 and 12 sheets from machine 2. Thickness measurements are taken. Assume sheet thickness is normally distributed in the population. The company wants to know if the variance for each sample comes from the same population variance (i.e. population variances are equal) or from different population variances (population variances are unequal). Data provided Machine 1 Machine 2 22.3 22.0 21.8 22.1 22.3 21.8 21.6 21.9 21.8 22.2 21.9 22.0 22.4 21.7 22.5 21.9 22.2 22.0 21.6 22.1 21.9 22.1 Solution Approach # Understanding what type of test it is Here the problem statement is about knowing the consistency of two machines in terms of variability . For variances test, we have two tests - \\chi ^2 \\chi ^2 and F test. But since we have to compare two variances, we have to go with the F test. Known parameters Samples \\space from \\space machine \\space one \\space n_1 = 10 \\\\ Samples \\space from \\space machine \\space two \\space n_2 = 12 Samples \\space from \\space machine \\space one \\space n_1 = 10 \\\\ Samples \\space from \\space machine \\space two \\space n_2 = 12 We have the data for machine 1 and machine 2 - from which we can find the variance - from excel Compute the rest of the parameters We compute the variance and the count of samples. The degree of freedom for machine 1 is 9 ( df_1 = n_1 - 1 df_1 = n_1 - 1 ). The degree of freedom for machine 1 is 11 ( df_2 = n_2 - 1 df_2 = n_2 - 1 ). Following the hypothesis testing process There are 5 steps to the hypothesis testing process. Let's follow that one by one. State the null and the alternate hypothesis Since this problem talks about variance in the two machines, the null hypothesis will be that there is no variance. \\mathbf{H}_\\mathbf{0}: \\sigma_1 ^2 = \\sigma_2 ^2 \\mathbf{H}_\\mathbf{0}: \\sigma_1 ^2 = \\sigma_2 ^2 The alternate hypothesis will be \\mathbf{H}_\\mathbf{0}: \\sigma_1 ^2 \\ne \\sigma_2 ^2 \\mathbf{H}_\\mathbf{0}: \\sigma_1 ^2 \\ne \\sigma_2 ^2 So based on the symbol of the alternate hypothesis which is \\ne \\ne , we conclude that this is a two-tailed test. Tip If there is any difficulty in stating the null hypothesis, start with the alternate hypothesis and then draft the null hypothesis. Find the level of significance The level of significance \\alpha \\alpha is not given and hence a \\alpha \\alpha of 5% or 0.05 is presumed. Since this is a two-tailed test, we have to divide \\alpha \\alpha by 2 which gives the value as 0.025. Find the critical value Similar to tables for other tests, F tests also have a corresponding table called the F-table. The F-table has a degree of freedom one ( df_1 df_1 ) on the 'x' axis and degree of freedom two ( df_2 df_2 ) on the 'y' axis. Let's see the F table for df_1 = 9 df_1 = 9 and df_2 = 11 df_2 = 11 to find the critical value for F test. The critical value is F_{0.025,9,11} = 3.5879 F_{0.025,9,11} = 3.5879 . This number is for the right-tail. Remember, we have a two-tails for this hypothesis testing. We also need to compute the left-tail critical value which can be either computed like the F-table above with a level of significance \\alpha = 0.975 (1-0.025 = 0.975) \\alpha = 0.975 (1-0.025 = 0.975) and the df_1 = 9 df_1 = 9 and df_2 = 11 df_2 = 11 . The other method of doing this is to divide 1 by F_{0.025,9,11} F_{0.025,9,11} F_{0.975,9,11} = \\frac{1}{F_{0.025,9,11}} F_{0.975,9,11} = \\frac{1}{F_{0.025,9,11}} F_{0.975,9,11} = \\frac{1}{3.5879} = 0.2787 F_{0.975,9,11} = \\frac{1}{3.5879} = 0.2787 Find the test statistic The test statistic is found by the F distribution formula - which is F = \\frac{s_1 ^2}{s_2 ^2} F = \\frac{s_1 ^2}{s_2 ^2} So the F score is 5.62 $$ F_{score} = \\frac{0.11378}{0.02023} = 5.62 $$ Draw the conclusion From the above steps, we know that the F_{critical} = 3.5879 F_{critical} = 3.5879 and the F_{score} = 5.62 F_{score} = 5.62 Since F_{score} F_{score} > F_{critical} F_{critical} or in other words, falls into the rejection region, we reject the null hypothesis. That is, the variance in Machine 1 and Machine 2 are not equal. Machine 1 has a higher variance and hence needs to be inspected for issues. The graph below illustrates the F score and the F critical value and why the null hypothesis is rejected. ANOVA # ANOVA - Analysis of Variance ANOVA is used to analyze the means of the samples. In the \\chi^2 \\chi^2 test, we saw how to compare variance within a single population and in the F test, we saw how to compare the variance between two samples of a single population / two population. What if there are three or more samples? Can we find if they are from the same population? ANOVA test is used for this purpose. When performing ANOVA test, we try to determine if the difference between the averages reflects a real difference between the groups, or is due to the random noise inside each group. The groups here mean samples - say out of a Population p, we are taking three groups of samples - n_1, n_2, n_3 n_1, n_2, n_3 . Assumptions in the ANOVA test # The samples taken are independent; (taking samples in one group does not affect the probability of the samples taken in other groups) The population must be normally distributed. Null hypothesis and computation # Since we are comparing three groups or more, the null hypothesis of ANOVA would look like below \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C We compute the F value and compare it with the critical value determined by the degrees of freedom. Here, the degrees of freedom are to be calculated for the groups and the number of items in each group. Example # Let's say we have three groups - A, B and C which have the below samples picked. Group A Group B Group C 37 62 50 60 27 63 52 69 58 43 64 54 40 43 49 52 54 52 55 44 53 39 31 43 39 49 65 23 57 43 These three groups have an equal number of samples. The degree of freedom of the groups = df_g = 3 - 1 = 2 df_g = 3 - 1 = 2 . The degree of freedom for the samples within each group is df_s = 9 + 9 + 9 = 27 df_s = 9 + 9 + 9 = 27 i.e. n-1 n-1 for each group which is 10 - 1 = 9 10 - 1 = 9 Let us calculate the mean of each of the groups and the total mean (which is total of all the means of the three groups divided by 3) Group A Group B Group C 37 62 50 60 27 63 52 69 58 43 64 54 40 43 49 52 54 52 55 44 53 39 31 43 39 49 65 23 57 43 Mean 44 50 53 Mean Total 49 Important ANOVA considers two types of variance Between groups How far each group mean vary from the total mean (i.e. in this case, how 44, 50, 53 44, 50, 53 vary from the total mean 49 49 Within groups How far individual values vary from their respective group mean. Note We will compute the F score both manually as well as using excel. If you want to move on with excel computation only, you may skip the below section and can proceed with computation with excel (using data-analysis addin). Formula # We compute F value for the groups which is the ratio between the two variances - i.e. variance between groups and variance within groups F = \\frac{Variance\\space Between\\space Groups}{Variance\\space Within\\space Groups} F = \\frac{Variance\\space Between\\space Groups}{Variance\\space Within\\space Groups} Recall We know that the formula for variance is $$ s ^2 = \\frac{\\sum(x - \\bar{x}) ^2}{n -1} = \\frac{SS}{df} $$ where \\sum(x - \\bar{x}) ^2 \\sum(x - \\bar{x}) ^2 is the sum of squares SS SS and the n -1 n -1 is the degree of freedom . So the formula for the f value becomes F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} where SSG SSG = Sum of squares groups, df_{groups} = df_{groups} = degrees of freedom (groups) and SSE SSE = Sum of squares error and df_{error} df_{error} = degrees of freedom (error) Solution approach # As with any hypothesis testing, let us perform the calculation using the hypothesis testing steps. State the null and the alternate hypothesis For ANOVA, the hypothesis will always be - the means across different groups will be equal. In this case, the means of the groups will be equal is the null hypothesis. \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C The alternate hypothesis will be \\mathbf{H}_\\mathbf{1}: \\mu_A \\ne \\mu_B \\ne \\mu_C \\mathbf{H}_\\mathbf{1}: \\mu_A \\ne \\mu_B \\ne \\mu_C Important In ANOVA, there will be NO two-tailed test. ANOVA will ALWAYS be one-tailed and that will be upper-tailed only . Why? We know that in the formula, we are dividing the sum of squares between groups and within groups which would always yield a positive value and hence it will always be upper tailed . Find the level of significance Here level of significance is not provided; we will take the default \\alpha = 0.05 \\alpha = 0.05 Find the critical value We know that the two degrees of freedom - for the groups and within the groups are 2 and 27 respectively. So, df_1 = 2 df_1 = 2 and df_2 = 27 df_2 = 27 Looking into the F table (for ANOVA) at 0.05 significance level, we get the F_{critical} F_{critical} value as 3.35 3.35 (see image below) Find the test statistic To compute it manually, we will be using excel. The following screenshot shows how it is done. We know the formula is F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} Computing the sum of squares within groups and between groups manually. substituting in the formula, we get F_{test} = \\frac{\\frac{420}{2}}{\\frac{3300}{27}} = 1.718 F_{test} = \\frac{\\frac{420}{2}}{\\frac{3300}{27}} = 1.718 Computing the sum of squares within groups and between groups via excel data-analysis addin. Enter the data in excel Go to Data -> Data Analysis and select \"Anova - Single factor\" and click \"OK\" Give the input range, check \"labels in first row\" as we have the group headers in the first row and input the level of significance. Click \"OK\" We get the analysis in a new sheet. Draw your conclusion Here we see that the F_{score} F_{score} is less than the F_{critical} F_{critical} ( 1.718 < 3.354 1.718 < 3.354 ) and hence we will accept the null hypothesis - which means that there is no difference between the means of any group. Summary # To summarize, we have seen two tests - the chi-square and the F test to test the variances in the population. We have also seen ANOVA - which is used to test the hypothesis when more than two groups of samples are picked from the population.","title":"Test statistic - Chi-square & F"},{"location":"statistics/hypothesis_testing_04/#test_statistic_-_chi2chi2_f","text":"In the previous article , we saw what a Z test and t-test are! Z test and t-test are used when the hypothesis test is about the means of the population. In this article, let us see two tests - the chi-squared ( \\chi^2 \\chi^2 ) and F test which tests the hypothesis about the variance of the population. Recall The sample estimate of the population variance is given by $$ s^2 = \\frac{\\sum(x_i-\\bar{x})^2}{n-1} $$ where s s is the sample variance, \\bar{x} \\bar{x} is the sample mean, and n n is the number of samples.","title":"Test statistic - \\chi^2\\chi^2 &amp; F"},{"location":"statistics/hypothesis_testing_04/#chi-squared_statistic","text":"\\chi^2 \\chi^2 is used to test the hypothesis about a single population variance.","title":"Chi-squared statistic"},{"location":"statistics/hypothesis_testing_04/#formula","text":"The formula for computing \\chi ^2 \\chi ^2 is \\chi ^2 = \\frac{(n-1)s ^2}{\\sigma ^2} \\chi ^2 = \\frac{(n-1)s ^2}{\\sigma ^2} where n - 1 n - 1 is the df df (degree of freedom), s s is the sample standard deviation and \\sigma \\sigma is the population standard deviation and n n is the number of samples.","title":"Formula"},{"location":"statistics/hypothesis_testing_04/#application_of_chi-square","text":"A manufacturing company produces bearings of 2.65 cm in diameter. A major customer requires that the variance in diameter be no more than 0.001 cm ^2 cm ^2 . The manufacturer tests 20 bearings using a precise instrument and gets the below values. Assuming the diameters are normally distributed, can the population of these bearings be rejected due to high variance at 1% significance level.","title":"Application of Chi-square"},{"location":"statistics/hypothesis_testing_04/#solution_approach","text":"The problem talks about a single population variance. Hence, a \\chi ^2 \\chi ^2 test can be used.","title":"Solution approach"},{"location":"statistics/hypothesis_testing_04/#f_distribution","text":"\\chi ^2 \\chi ^2 is useful when testing hypothesis about a single population. What if we want to test the hypothesis about the difference in variances of two populations? Example Do parts manufactured on 2 different machines have the same variance or not?","title":"F distribution"},{"location":"statistics/hypothesis_testing_04/#formula_1","text":"Since F-test is a comparison of variances of two different populations using samples collected from each population, we can say that it is the ratio of two sample variances i.e. F = \\frac{s_1 ^2}{s_2 ^2} = \\frac{est.\\sigma ^2_1}{est.\\sigma ^2_2} F = \\frac{s_1 ^2}{s_2 ^2} = \\frac{est.\\sigma ^2_1}{est.\\sigma ^2_2} What does this formula mean? We know that s_1 s_1 is the standard deviation of sample 1 and s_2 s_2 is the standard deviation of sample 2. Since the F test is a comparison between two variances , we need to square the standard deviation. (Remember: variance = standard deviation ^2 ^2 )","title":"Formula"},{"location":"statistics/hypothesis_testing_04/#application_of_the_f_test","text":"A machine produces metal sheets with 22mm thickness. There is a variability in thickness due to machines, operators, manufacturing environment, raw material, etc. The company wants to know the consistency of two machines and randomly samples 10 sheets from machine 1 and 12 sheets from machine 2. Thickness measurements are taken. Assume sheet thickness is normally distributed in the population. The company wants to know if the variance for each sample comes from the same population variance (i.e. population variances are equal) or from different population variances (population variances are unequal).","title":"Application of the 'F' test"},{"location":"statistics/hypothesis_testing_04/#solution_approach_1","text":"","title":"Solution Approach"},{"location":"statistics/hypothesis_testing_04/#anova","text":"ANOVA - Analysis of Variance ANOVA is used to analyze the means of the samples. In the \\chi^2 \\chi^2 test, we saw how to compare variance within a single population and in the F test, we saw how to compare the variance between two samples of a single population / two population. What if there are three or more samples? Can we find if they are from the same population? ANOVA test is used for this purpose. When performing ANOVA test, we try to determine if the difference between the averages reflects a real difference between the groups, or is due to the random noise inside each group. The groups here mean samples - say out of a Population p, we are taking three groups of samples - n_1, n_2, n_3 n_1, n_2, n_3 .","title":"ANOVA"},{"location":"statistics/hypothesis_testing_04/#assumptions_in_the_anova_test","text":"The samples taken are independent; (taking samples in one group does not affect the probability of the samples taken in other groups) The population must be normally distributed.","title":"Assumptions in the ANOVA test"},{"location":"statistics/hypothesis_testing_04/#null_hypothesis_and_computation","text":"Since we are comparing three groups or more, the null hypothesis of ANOVA would look like below \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C We compute the F value and compare it with the critical value determined by the degrees of freedom. Here, the degrees of freedom are to be calculated for the groups and the number of items in each group.","title":"Null hypothesis and computation"},{"location":"statistics/hypothesis_testing_04/#example","text":"Let's say we have three groups - A, B and C which have the below samples picked. Group A Group B Group C 37 62 50 60 27 63 52 69 58 43 64 54 40 43 49 52 54 52 55 44 53 39 31 43 39 49 65 23 57 43 These three groups have an equal number of samples. The degree of freedom of the groups = df_g = 3 - 1 = 2 df_g = 3 - 1 = 2 . The degree of freedom for the samples within each group is df_s = 9 + 9 + 9 = 27 df_s = 9 + 9 + 9 = 27 i.e. n-1 n-1 for each group which is 10 - 1 = 9 10 - 1 = 9 Let us calculate the mean of each of the groups and the total mean (which is total of all the means of the three groups divided by 3) Group A Group B Group C 37 62 50 60 27 63 52 69 58 43 64 54 40 43 49 52 54 52 55 44 53 39 31 43 39 49 65 23 57 43 Mean 44 50 53 Mean Total 49 Important ANOVA considers two types of variance Between groups How far each group mean vary from the total mean (i.e. in this case, how 44, 50, 53 44, 50, 53 vary from the total mean 49 49 Within groups How far individual values vary from their respective group mean. Note We will compute the F score both manually as well as using excel. If you want to move on with excel computation only, you may skip the below section and can proceed with computation with excel (using data-analysis addin).","title":"Example"},{"location":"statistics/hypothesis_testing_04/#formula_2","text":"We compute F value for the groups which is the ratio between the two variances - i.e. variance between groups and variance within groups F = \\frac{Variance\\space Between\\space Groups}{Variance\\space Within\\space Groups} F = \\frac{Variance\\space Between\\space Groups}{Variance\\space Within\\space Groups} Recall We know that the formula for variance is $$ s ^2 = \\frac{\\sum(x - \\bar{x}) ^2}{n -1} = \\frac{SS}{df} $$ where \\sum(x - \\bar{x}) ^2 \\sum(x - \\bar{x}) ^2 is the sum of squares SS SS and the n -1 n -1 is the degree of freedom . So the formula for the f value becomes F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} where SSG SSG = Sum of squares groups, df_{groups} = df_{groups} = degrees of freedom (groups) and SSE SSE = Sum of squares error and df_{error} df_{error} = degrees of freedom (error)","title":"Formula"},{"location":"statistics/hypothesis_testing_04/#solution_approach_2","text":"As with any hypothesis testing, let us perform the calculation using the hypothesis testing steps. State the null and the alternate hypothesis For ANOVA, the hypothesis will always be - the means across different groups will be equal. In this case, the means of the groups will be equal is the null hypothesis. \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C \\mathbf{H}_\\mathbf{0}: \\mu_A = \\mu_B = \\mu_C The alternate hypothesis will be \\mathbf{H}_\\mathbf{1}: \\mu_A \\ne \\mu_B \\ne \\mu_C \\mathbf{H}_\\mathbf{1}: \\mu_A \\ne \\mu_B \\ne \\mu_C Important In ANOVA, there will be NO two-tailed test. ANOVA will ALWAYS be one-tailed and that will be upper-tailed only . Why? We know that in the formula, we are dividing the sum of squares between groups and within groups which would always yield a positive value and hence it will always be upper tailed . Find the level of significance Here level of significance is not provided; we will take the default \\alpha = 0.05 \\alpha = 0.05 Find the critical value We know that the two degrees of freedom - for the groups and within the groups are 2 and 27 respectively. So, df_1 = 2 df_1 = 2 and df_2 = 27 df_2 = 27 Looking into the F table (for ANOVA) at 0.05 significance level, we get the F_{critical} F_{critical} value as 3.35 3.35 (see image below) Find the test statistic To compute it manually, we will be using excel. The following screenshot shows how it is done. We know the formula is F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}} F = \\frac{VarianceBetweenGroups}{VarianceWithinGroups} = \\frac{\\frac{SSG}{df_{groups}}}{\\frac{SSE}{df_{error}}}","title":"Solution approach"},{"location":"statistics/hypothesis_testing_04/#summary","text":"To summarize, we have seen two tests - the chi-square and the F test to test the variances in the population. We have also seen ANOVA - which is used to test the hypothesis when more than two groups of samples are picked from the population.","title":"Summary"}]}